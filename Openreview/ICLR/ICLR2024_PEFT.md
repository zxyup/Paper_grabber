# ICLR2024_PEFT
## [Med-Tuning: Parameter-Efficient Transfer Learning with Fine-Grained Feature Enhancement for Medical Volumetric Segmentation](https://openreview.net/forum?id=zlcktbqv6b) ------> [PDF](https://openreview.net/pdf/cc47e98139312571591554001cc52cdf670ec38a.pdf)
#### Keywords:  Parameter-Efficient Transfer Learning   Medical Volumetric Segmentation   Brain Tumor   Kidney Tumor   Intra-stage enhancement   Inter-stage Interaction
  Abs:  Deep learning-based medical volumetric segmentation methods either train the model from scratch or follow the standard ``pre-training then finetuning" paradigm. Although finetuning a pre-trained model on downstream tasks can harness its representation power, the standard full finetuning is costly in terms of computation and memory footprint. In this paper, we present the study on parameter-efficient transfer learning for medical volumetric segmentation and propose a new framework named Med-Tuning based on intra-stage feature enhancement and inter-stage feature interaction. Additionally, aiming at exploiting the intrinsic global properties of Fourier Transform for parameter-efficient transfer learning, a new adapter block namely Med-Adapter with a well-designed Fourier Transform branch is proposed for effectively and efficiently modeling the crucial global context for medical volumetric segmentation. Given a large-scale pre-trained model on 2D natural images, our method can exploit both the crucial spatial multi-scale feature and temporal correlations along slices for accurate segmentation. Extensive experiments on three benchmark datasets (including CT and MRI) show that our method can achieve better results than previous parameter-efficient transfer learning methods for segmentation task, with much less tuned parameter costs. Compared to full finetuning, our method reduces the finetuned model parameters by up to 4x, with even better segmentation performance.
## [Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape](https://openreview.net/forum?id=zNzVhX00h4) ------> [PDF](https://openreview.net/pdf/935fe78ab5a7e467682aebae7c1effc337847979.pdf)
#### Keywords:  overparameterization   loss landscape   jacobian   full rank   activation region   activation pattern
  Abs:  We study the loss landscape of two-layer mildly overparameterized ReLU neural networks on a generic finite input dataset for the squared error loss. Our approach involves bounding the dimension of the sets of local and global minima using the rank of the Jacobian of the parametrization map. Using results on random binary matrices, we show most activation patterns correspond to parameter regions with no bad differentiable local minima. Furthermore, for one-dimensional input data, we show most activation regions realizable by the network contain a high dimensional set of global minima and no bad local minima. We experimentally confirm these results by finding a phase transition from most regions having full rank to many regions having deficient rank depending on the amount of overparameterization.
## [H-Rockmate: Hierarchical Approach for Efficient Re-materialization of Large Neural Networks](https://openreview.net/forum?id=zLwCT9srfo) ------> [PDF](https://openreview.net/pdf/81e7578663a3ecac8a153fbf4e007cfb931967d1.pdf)
#### Keywords:  Rematerialization   Neural Networks   Memory-Efficient Training   PyTorch   Integer Linear Programming   Training   Checkpointing
  Abs:  Training modern neural networks poses a significant memory challenge, as storing intermediate results during the forward and backward passes demands substantial memory resources. To address this issue while maintaining model accuracy, re-materialization techniques have been introduced to recompute selected intermediate results rather than storing them, thereby adhering to peak memory constraints. The main algorithmic problem is to compute a re-materialization schedule that minimizes the computational overhead within a given memory budget. Our H-Rockmate framework builds upon an existing Rockmate solution and overcomes its limitation to work with sequential block structures by proposing a hierarchical approach. The framework performs an automatic decomposition of the data-flow graph into a hierarchy of small-scale subgraphs, and finds a re-materialization schedule for the whole graph by recursively solving optimization problems for each subgraph. H-Rockmate allows users to transform their PyTorch models into nn.Modules that execute forward and backward passes efficiently within the specified memory budget. This framework can handle neural networks with diverse data-flow graph structures, including U-Nets and encoder-decoder Transformers. H-Rockmate outperforms existing re-materialization approaches in terms of average training iteration time and peak memory trade-offs, demonstrating superior memory efficiency in training modern neural networks.
## [In defense of parameter sharing for model-compression](https://openreview.net/forum?id=ypAT2ixD4X) ------> [PDF](https://openreview.net/pdf/fc352569090a158df0418b74068e451d0c72e099.pdf)
#### Keywords:  parameter sharing   model compression   pruning
  Abs:  When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at
start of training. In this paper, we comprehensively assess the trade-off between
memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior
technique in terms of memory-efficient representation when compared to pruning
for linear models. This paper argues in favor of paradigm shift towards RPS based
models. During our rigorous evaluation of RPS, we identified issues in the state-
of-the-art RPS technique ROAST, specifically regarding stability (ROAST’s sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST’s inability to recover the accuracy of the original model at zero
compression). We provably address both of these issues. We refer to the modified
RPS, which incorporates our improvements, as STABLE-RPS
## [Time- and Label-efficient Active Learning by Diversity and Uncertainty of Probabilities](https://openreview.net/forum?id=yZBpnKpBCw) ------> [PDF](https://openreview.net/pdf/e80d989ad48d4e0c7f14c9cd55f9c963d2e952fb.pdf)
#### Keywords:  Active Learning   Deep Active Learning   Fast   Label-Efficient
  Abs:  We propose FALCUN, a novel deep batch active learning method that is label- and time-efficient. Our proposed acquisition uses a natural, self-adjusting balance of uncertainty and diversity: It slowly transitions from emphasizing uncertain instances at the decision boundary to emphasizing batch diversity.
In contrast, established deep active learning methods often have a fixed weighting of uncertainty and diversity. Moreover, most methods demand intensive search through a deep neural network's high-dimensional latent embedding space. This leads to high acquisition times during which experts are idle as they wait for the next batch to label. 
We overcome this structural problem by exclusively operating on the low-dimensional probability space, yielding much faster acquisition times. 
In extensive experiments, we show FALCUNs suitability for diverse use cases, including image and tabular data. 
Compared to state-of-the-art methods like BADGE, CLUE, and AlfaMix, FALCUN consistently excels in quality and speed: while FALCUN is among the fastest methods, it has the highest average label efficiency.
## [TransNeXt: Aggregating Diverse Attentions in One Vision Model](https://openreview.net/forum?id=yMwJiJoadt) ------> [PDF](https://openreview.net/pdf/622b1ac909d01033efaf26474e1d7775858ff97b.pdf)
#### Keywords:  Vision Transformer   Efficient Transformer   Self-Attention   Convolution   Visual Backbone   Image Classification   Object Detection   Image Segmentation
  Abs:  In the design of previous Vision Transformers (ViTs), different token mixers were often alternately stacked to balance the visual model’s aggregation of global and local information, or to combine the characteristics of convolution with attention mechanism. In this paper, we propose Aggregated Attention, which is a biomimetic design-based token mixer enabling each token to have fine-grained attention to its nearest neighbor features and coarse-grained attention to global features in terms of spatial information aggregation. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. All of these improvements can be achieved within a single attention layer, eliminating the need for alternately stacking different token mixers.
Additionally, we propose Convolutional GLU, a channel mixer that bridges the gap between GLU and SE mechanism, which empowers each token to have channel attention based on its nearest neighbor image features, enhancing local modeling capability and model robustness. We combine aggregated attention and convolutional GLU to create a new visual backbone called TransNeXt. Extensive experiments demonstrate that our TransNeXt achieves state-of-the-art performance across multiple model sizes. At a resolution of $224^2$, TransNeXt-Tiny attains an ImageNet accuracy of 84.0\%, surpassing ConvNeXt-B with 69\% fewer parameters. Our TransNeXt-Base achieves an ImageNet accuracy of 86.2\% and an ImageNet-A accuracy of 61.6\% at a resolution of $384^2$, a COCO object detection mAP of 57.1, and an ADE20K semantic segmentation mIoU of 54.7.
## [Enhancing Sample Efficiency in Black-box Combinatorial Optimization via Symmetric Replay Training](https://openreview.net/forum?id=xzRnzHUVE9) ------> [PDF](https://openreview.net/pdf/5f94b778a4caf45422feca2eca3c91f04f770a6b.pdf)
#### Keywords:  Black-box combinatorial optimization   sample efficiency   symmetries   drug discovery   hardware design   deep reinforcement learning   imitation learning
  Abs:  Black-box combinatorial optimization (black-box CO) is frequently encountered in various industrial fields, such as drug discovery or hardware design. Despite its widespread relevance, solving black-box CO problems is highly challenging due to the vast combinatorial solution space and resource-intensive nature of black-box function evaluations. These inherent complexities induce significant constraints on the efficacy of existing deep reinforcement learning (DRL) methods when applied to practical problem settings. For efficient exploration with the limited availability of function evaluations, this paper introduces a new generic method to enhance sample efficiency. We propose symmetric replay training that leverages the high-reward samples and their under-explored regions in the symmetric space. In replay training, the policy is trained to imitate the symmetric trajectories of these high-rewarded samples. The proposed method is beneficial for the exploration of highly rewarded regions without the necessity for additional online interactions - free. The experimental results show that our method consistently improves the sample efficiency of various DRL methods on real-world tasks, including molecular optimization and hardware design. Our source code is available at https://anonymous.4open.science/r/sym_replay.
## [LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](https://openreview.net/forum?id=xw29VvOMmU) ------> [PDF](https://openreview.net/pdf/e01f42229254b8c5d9a81a9651874d93ae5f2b32.pdf)
#### Keywords:  Low-rank plus Quantized Matrix Decomposition   Efficient Language Model Finetuning
  Abs:  We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component, which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget.  We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Across standard NLP benchmarks, our low-rank plus quantized matrix decomposition approach (LQ-LoRA) is found to perform well against strong baselines.
## [BiXT: Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers](https://openreview.net/forum?id=xvhjRjoFCN) ------> [PDF](https://openreview.net/pdf/fe81bfe873899ece7dcbe3f6199a3c82fe1cf733.pdf)
#### Keywords:  Transformers   representation learning   efficiency   efficient attention   neural architectures
  Abs:  We present a novel bi-directional Transformer architecture (BiXT) for which computational cost and memory consumption scale linearly with input size, but without suffering the drop in performance or limitation to only one input modality seen with other efficient Transformer-based approaches. BiXT is inspired by the Perceiver architectures but replaces iterative attention with an efficient bi-directional cross-attention module in which input tokens and latent variables attend to each other simultaneously, leveraging a naturally emerging attention-symmetry between the two. This approach unlocks a key bottleneck experienced by Perceiver-like architectures and enables the processing and interpretation of both semantics (‘what’) and location (‘where’) to develop alongside each other over multiple layers – allowing its direct application to dense and instance-based tasks alike. By combiningefficiency with the generality and performance of a full Transformer architecture, BiXT can processes longer sequences like point clouds or images at higher feature resolutions. Our model achieves accuracies up to 82.0% for classification on ImageNet1K with tiny models and no modality-specific internal components,
and performs competitively on semantic image segmentation (ADE20K) and point cloud part segmentation (ShapeNetPart) even against modality-specific methods.
## [Score Regularized Policy Optimization through Diffusion Behavior](https://openreview.net/forum?id=xCRr9DrolJ) ------> [PDF](https://openreview.net/pdf/6fc8565b7127457110f8afcbafc2ea8af3a291c4.pdf)
#### Keywords:  offline reinforcement learning   generative models   diffusion models   behavior modeling   computational efficiency
  Abs:  Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior
distribution’s score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.
## [STABLE ESTIMATION OF SURVIVAL CAUSAL EFFECTS](https://openreview.net/forum?id=wsn1lPgDvU) ------> [PDF](https://openreview.net/pdf/f5f3cae96152929c378569e56dbfc06e044168b3.pdf)
#### Keywords:  survival analysis   causal inference   covariate balancing   semi-parametric efficiency   double machine learning
  Abs:  We study the problem of estimating survival causal effects, where the aim is to characterize the impact of an intervention on survival times, i.e., how long it takes for an event to occur. Applications include determining if a drug reduces the time to ICU discharge or if an advertising campaign increases customer dwell time. Historically, the most popular estimates have been based on parametric or semiparametric (e.g. proportional hazards) models; however, these methods suffer from problematic levels of bias. Recently debiased machine learning approaches are becoming increasingly popular, especially in applications to large datasets. However, despite their appealing theoretical properties, these estimators tend to be unstable because the debiasing step involves the use of the inverses of small estimated probabilities---small errors in the estimated probabilities can result in huge changes in their inverses and therefore the resulting estimator. This problem is exacerbated in survival settings where probabilities are a product of treatment assignment and censoring probabilities. We propose a covariate balancing approach to estimating these inverses directly, sidestepping this problem. The result is an estimator that is stable in practice and enjoys many of the same theoretical properties. In particular, under overlap and asymptotic equicontinuity conditions, our estimator is asymptotically normal with negligible bias and optimal variance. Our experiments on synthetic and semi-synthetic data demonstrate that our method has competitive bias and smaller variance than debiased machine learning approaches.
## [Are We in (A)Sync?: Guidance for Efficient Federated Learning](https://openreview.net/forum?id=wilJbPvRYv) ------> [PDF](https://openreview.net/pdf/2cf61a8bd2afbdd100a0ad9f2674e56a9e3b7b51.pdf)
#### Keywords:  federated learning   synchronous federated learning   asynchronous federated learning   time-to-accuracy   resource efficiency
  Abs:  Federated Learning (FL) methods have widely adopted synchronous FL (syncFL), where a server distributes and aggregates the model weights with clients in coordinated rounds. As syncFL suffers from low resource utilization on clients with heterogeneous computing power, asynchronous FL (asyncFL), which allows the server to exchange models with available clients continuously, has been proposed. Despite numerous studies on syncFL and asyncFL, how they differ in training time and resource efficiency is still unclear. Given the training and communication speed of participating clients, we present a formulation of time and resource usage on syncFL and asyncFL. Our formulation weights asyncFL against its inefficiencies stemming from stale model updates, enabling more accurate comparison to syncFL in achieving the same objectives. Unlike previous findings, the formulation reveals that no single approach always works better than the other regarding time and resource usage. Our experiments across five datasets show that the formulation predicts relative time and resource usage of syncFL and asyncFL with up to 5.5$\times$ smaller root-mean-square error (RMSE) compared to the baseline methods. We envision our formulation to guide FL practitioners in making informed decisions between syncFL and asyncFL, depending on their resource constraints.
## [Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation](https://openreview.net/forum?id=wfzXa8e783) ------> [PDF](https://openreview.net/pdf/7d8ef9b75e4571bb57fe86bdf18d0af30d2e307a.pdf)
#### Keywords:  Text-to-Image   Model Customization   Parameter-Efficient Fine-Tuning   Model Evaluation
  Abs:  Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.
## [Faster and Accurate Neural Networks with Semantic Inference](https://openreview.net/forum?id=wZXwP3H5t6) ------> [PDF](https://openreview.net/pdf/a49a6933e68d6fcd93d392b3d466077570ea7946.pdf)
#### Keywords:  Semantic Inference   Semantic Pruning   Deep Learning   Efficient Neural Networks
  Abs:  Deep neural networks (DNNs) usually come with a significant computational and data labeling burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur in drastic accuracy loss. Conversely from prior work, in this paper we leverage the intrinsic redundancy in latent representations to drastically reduce the computational load with very limited loss in performance. Specifically, we show that semantically similar inputs share a significant number of filter activations, especially in the earlier layers. As such, semantically similar classes can be “clustered” so as to create cluster-specific subgraphs. These may be “turned on” when an input belonging to a semantic cluster is being presented to the DNN, while the rest of the DNN can be “turned off”. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier; and then (ii) executes the subgraph extracted from the base DNN related to that semantic cluster to perform the inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that effectively finds the subgraph with the capability to discriminate among the members of a specific semantic cluster. Importantly, DCS is independent from SINF, as it is a general-purpose quantity that can be applied to any DNN. We benchmark the performance of DCS on the VGG16, VGG19, and ResNet50 DNNs trained on the CIFAR100 dataset against 6 state-of-the-art pruning approaches. Our results show that (i) SINF reduces the inference time of VGG19, VGG16, and ResNet50 respectively by up to 35%, 29% and 15% with only 0.17%, 3.75%, and 6.75% accuracy loss; (ii) DCS achieves respectively up to 3.65%, 4.25%, and 2.36% better accuracy with VGG16, VGG19, and ResNet50 with respect to existing discriminative scores; (iii) when used as a pruning criterion, DCS achieves up to 8.13% accuracy gain with 5.82% less parameters than the existing state of the art work published at ICLR 2023; (iv) when considering per-cluster accuracy, SINF performs on average 5.73%, 8.38% and 6.36% better than the base VGG16, VGG19, and ResNet50. We share our code for reproducibility.
## [GIFF: Generalized Inference Friendly Forward-Forward Algorithm](https://openreview.net/forum?id=wUKVia7J10) ------> [PDF](https://openreview.net/pdf/ef7b486f102e4ff34d9bd2f101d735555965f69a.pdf)
#### Keywords:  Forward-Forward algorithm   on-device learning   tinyML   edge computing   machine learning   memory efficient
  Abs:  The Forward-Forward (FF) algorithm has recently been proposed to enable neural network training using only forward passes, inspired by the human cortex excitation and inhibition mechanisms. In contrast to Backpropagation (BP), which uses a global loss function, FF utilizes local loss functions at each layer, reducing peak memory requirements. The local weight update scope allows alternative optimizers, non-differentiable neural network layers, and aggressive quantization. Despite its promise for on-device training, the original FF technique has three major limitations that hinder its potential: the label embedding problem, lack of support for convolutional layers, and inefficient inference passes. These issues hamper its performance even on basic datasets like CIFAR10 and restrict its applicability. This paper presents the Generalized Inference Friendly Forward-Forward (GIFF) algorithm to address the limitations of the FF algorithm. We demonstrate GIFF on three representative tinyML benchmarks where FF cannot function. GIFF performs as well as BP on all three tasks, using up to 43% less memory. Furthermore, GIFF requires significantly fewer computations than FF for inference. Thus, GIFF unlocks the potential benefits of the FF algorithm for efficient on-device learning.
## [BarLeRIa: An Efficient Tuning Framework for Referring Image Segmentation](https://openreview.net/forum?id=wHLDHRkmEu) ------> [PDF](https://openreview.net/pdf/97c21bb7ede98aa1bddbc801d39bb57fbdb89b76.pdf)
#### Keywords:  referring image segmentation; parameter efficient tuning
  Abs:  Pre-training followed by full fine-tuning has gradually been substituted by Parameter-Efficient Tuning (PET) in the field of computer vision tasks. PET has gained popularity, especially in the context of large-scale models, due to its ability to reduce transfer learning costs and conserve hardware resources. However, existing PET approaches primarily focus on recognition tasks and typically support uni-modal optimization, neglecting dense prediction tasks and vision language interactions. To address this limitation, we propose a novel PET framework called Bi-directional Intertwined Vision Language Efficient Tuning for Referring Image Segmentation (BarLeRIa), which leverages bi-directional intertwined vision language adapters to fully exploit the frozen pre-trained models' potential in cross-modal dense prediction tasks. In BarLeRIa, two different tuning modules are employed for efficient global and local attention, as well as an intertwined vision language tuning algorithm for efficient modal fusion. Extensive experiments conducted on challenging RefCOCO-related benchmarks demonstrating the superiority of BarLeRIa over prior PET methods with a significant margin, \emph{i.e.}, achieving an average improvement of 5.6\%. Remarkably, without requiring additional training datasets, BarLeRIa even surpasses SOTA full fine-tuning approaches.
## [Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment](https://openreview.net/forum?id=w9tc699w3Z) ------> [PDF](https://openreview.net/pdf/2d158bb8cf0ab75af0b02a80feb233aac8425c4e.pdf)
#### Keywords:  remote sensing   vision-language models   zero-shot   foundation models   label-efficiency
  Abs:  We introduce a method to train vision-language models for remote-sensing images without using any textual annotations. Our key insight is to use co-located internet imagery taken on the ground as an intermediary for connecting remote-sensing images and language.  Specifically, we train an image encoder for remote sensing images to align with the image encoder of CLIP using a large amount of paired internet and satellite images.  Our unsupervised approach enables the training of a first-of-its-kind large scale VLM for remote sensing images at two different resolutions. We show that these VLMs enable zero-shot, open-vocabulary image classification, retrieval, segmentation and visual question answering for satellite images. On each of these tasks, our VLM trained without textual annotations outperforms existing VLMs trained with supervision, with gains of up to 20\% for classification and 80\% for segmentation.
## [LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition](https://openreview.net/forum?id=w8eCnnq57m) ------> [PDF](https://openreview.net/pdf/84743d43731da0d0084effcb44751f6b1654246a.pdf)
#### Keywords:  Parameter Efficient Tuning   LoRA   Cross-Task Generalization
  Abs:  Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a simple framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a new task, LoraHub can fluidly combine multiple LoRA modules, eliminating the need for human expertise and assumptions.  Notably, the composition requires neither additional model parameters nor gradients.  Our empirical results, derived from the Big-Bench Hard benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input.
## [Sapling: $\underline{S}$uccessive $\underline{A}$daptation and Com$\underline{p}$ression with $\underline{L}$ayer Dropp$\underline{ing}$ for LLMs](https://openreview.net/forum?id=vzvCaYFTLq) ------> [PDF](https://openreview.net/pdf/879da421a703e561a83471931eece0ac9bc5a014.pdf)
#### Keywords:  Efficient Deep Learning   Layer Dropping   LLM Fine-tuning   Specialized LLMs
  Abs:  Specializing Large language models (LLMs) for local deployment and domain-specific use can deliver state-of-the-art performance while meeting latency and privacy requirements. However, conventional task-specific adaptation does not show both memory saving and inference speedup at deployment time. Practical compression techniques like quantization and pruning require hardware support or system optimization to achieve measured inference speedup. We propose Sapling, which can retain LLMs' capacity in a specific knowledge domain and achieve inference speedup on any hardware and deep learning systems by reducing the model depth. Sapling is based on the knowledge localization phenomenon we empirically observed and verified on LLMs, and achieves model compression via successive layer dropping. We evaluated Sapling on LLaMA-7B. At inference time, the models adapted on medical, legal, and financial datasets have all demonstrated reliable performance, comparable memory saving, $1.2$ to $8.5\times$ inference speedup on consumer-level hardware compared to state-of-the-art quantization algorithms, depending on how well the algorithms are supported by efficient accelerator kernels.
## [The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis](https://openreview.net/forum?id=vtMrbs8Zwd) ------> [PDF](https://openreview.net/pdf/3c3ed3203063aad296c0637f59af53976a90dd0d.pdf)
#### Keywords:  sharpness-aware minimization   overparameterization
  Abs:  Training an overparameterized neural network can yield minimizers of the same level of training loss and yet different generalization capabilities. With evidence that indicates a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions.
This sharpness-aware minimization (SAM) strategy, however, has not been studied much yet as to how overparameterization can actually affect its behavior.
In this work, we analyze SAM under varying degrees of overparameterization and present both empirical and theoretical results that suggest a critical influence of overparameterization on SAM.
Specifically, we first use standard techniques in optimization to prove that SAM can achieve a linear convergence rate under overparameterization in a stochastic setting.
We also show based on a stability analysis that the solutions found by SAM are indeed flatter and have more uniformly distributed Hessian moments compared to those of SGD.
These results are corroborated with our experiments that reveal a consistent trend that the generalization improvement made by SAM continues to increase as the model becomes more overparameterized.
We further present that sparsity can open up an avenue for effective overparameterization in practice.
## [Communication Efficient Federated Representation Learning](https://openreview.net/forum?id=vstaHBy5N4) ------> [PDF](https://openreview.net/pdf/3612011bb55acbb1d533de1730656895ff889296.pdf)
#### Keywords:  Federated learning   Communication efficiency   Distributed machine learning
  Abs:  The Federated Averaging (FedAvg) algorithm is a widely utilized technique in Federated Learning. It follows a recursive pattern where nodes perform a few local stochastic gradient descents (SGD), and then the central server updates the model by taking an average.
The primary purpose of conducting model averaging is to mitigate the consensus error that arises between models across different nodes. 
In our empirical examination, it becomes evident that in non-iid data distribution setting, the consensus error in the initial layers of deep neural network is considerably smaller than that observed in the later layers. This observation hints at the feasibility of applying a less intensive averaging approach for the initial layers. Typically, these layers are designed to extract meaningful representations from the neural network's input. To delve deeper into this phenomenon, we formally analyze it within the context of linear representation.
We illustrate that increasing the number of local SGD iterations or reducing the frequency of averaging for the representation extractor leads to enhanced generalizability in the learned model produced by FedAvg's output.
The paper is followed with experimental results showing the effectiveness of this method.
## [S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks](https://openreview.net/forum?id=vlQ56aWJhl) ------> [PDF](https://openreview.net/pdf/27da03e6d467a190a67091ea27f279f85f4a9d3c.pdf)
#### Keywords:  Local learning   Spiking Neural Networks   Memory-efficient learning   STDP
  Abs:  Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for deploying energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses significant challenges due to the necessity for precise temporal and spatial credit assignments. Back-propagation through time (BPTT) algorithm, whilst the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training deep SNNs on event-based learning tasks. Furthermore, S-TLLR is designed to have low memory and time complexities, which are independent of the number of time steps, rendering it suitable for online learning on low-power edge devices. To demonstrate the scalability of our proposed method, we have conducted extensive evaluations on event-based datasets spanning a wide range of applications, such as image and gesture recognition, audio classification, and optical flow estimation. In all the experiments, S-TLLR achieved high accuracy, comparable to BPTT, with reduction in the number of computations between $1.1-10\times$.
## [Clip21: Error Feedback for Gradient Clipping](https://openreview.net/forum?id=viC3cpWFTN) ------> [PDF](https://openreview.net/pdf/9d0598c99a24181e8582d24d2f4919a97fbd4565.pdf)
#### Keywords:  clipping   error feedback   gradient-based methods   communication-efficient learning   optimization
  Abs:  Motivated by the increasing importance of deep neural network training, we study distributed gradient methods with gradient clipping, i.e., clipping applied to the gradients computed from local information at the nodes. While gradient clipping enforces the convergence of gradient-based methods that minimize rapidly growing functions, it also induces bias which causes serious convergence issues specific to the distributed setting. Inspired by  recent progress in the error-feedback literature which is focused on taming the bias/error introduced by communication compression operators such as Top-$k$ (Richtárik et al, 2021), and  mathematical similarities between the clipping operator and contractive compression operators, we design Clip21 -- the first provably effective and practically useful error feedback mechanism for distributed methods with gradient clipping. We prove that our method converges at the same $\mathcal{O}({1}/{K})$ rate as distributed gradient descent in the smooth nonconvex regime, which improves the previous best $\mathcal{O}({1}/{\sqrt{K}})$ rate which was obtained under significantly stronger assumptions.
Our method converges significantly faster in practice than competing methods.
## [Efficient VideoMAE via Temporal Progressive  Training](https://openreview.net/forum?id=vex1yNHNFL) ------> [PDF](https://openreview.net/pdf/3d9894b620ba928d5ccd4020c29bfc9b0dacd888.pdf)
#### Keywords:  Masked autoencoder   Video transformer   Training Efficiency
  Abs:  Masked autoencoders (MAE) have recently been adapted for video recognition, setting new performance benchmarks. Nonetheless, the computational overhead of training VideoMAE remains a prominent challenge, often demanding extensive GPU resources and days of training. To improve the training efficiency of VideoMAE, this paper presents Temporal Progressive Training (TPT), a simple way to strategically introduce longer video clips along the training process. Specifically, TPT decomposes the intricate task of long-clip reconstruction into a series of step-by-step sub-tasks, progressively transitioning from short video clips to long video clips. Our experiments extensively verify the efficacy and efficiency of TPT. For example, TPT can impressively reduce training costs by factors of $2\times$ on Kinetics-400 and $3\times$ on Something-Something V2, while still matching the performance of VideoMAE. Additionally, TPT consistently shows superior performance than VideoMAE when trained with the same budget.
## [Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios](https://openreview.net/forum?id=vRyp2dhEQp) ------> [PDF](https://openreview.net/pdf/f3470b042d32c22afc40c9240f882bd6230628d1.pdf)
#### Keywords:  Deep Neural Networks   Backdoor Attacks   Poisoning Efficiency.
  Abs:  Recent deep neural networks (DNNs) have come to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as $\textbf{data-constrained backdoor attacks}$. In such cases, previous attack methods suffer from severe efficiency degradation due to the $\textbf{entanglement}$ between benign and poisoning features during the backdoor injection process. To tackle this problem, we introduce three CLIP-based technologies from two distinct streams: $\textit{Clean Feature Suppression}$ and $\textit{Poisoning Feature Augmentation}$. The results demonstrate remarkable improvements, with some settings achieving over 100\% improvement compared to existing attacks in data-constrained scenarios. Our research contributes to addressing the limitations of existing methods and provides a practical and effective solution for data-constrained backdoor attacks.
## [Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning](https://openreview.net/forum?id=vNiI3aGcE6) ------> [PDF](https://openreview.net/pdf/6ddb14dd4863096944f06b71673a4aa2b41c0acf.pdf)
#### Keywords:  model-free RL   self-play   memory efficiency   Q-learning   Nash equilibrium   Markov policy
  Abs:  The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. We prove that ME-Nash-QL can output an $\varepsilon$-approximate Nash policy with remarkable space complexity $O(SABH)$, sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, and computational complexity $O(T\mathrm{poly}(AB))$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for the two players, $H$ is the horizon length, and $T$ is the number of samples. Notably, our approach outperforms in terms of space complexity compared to existing algorithms for tabular cases. It achieves the lowest computational complexity while preserving Markov policies, setting a new standard. Furthermore, our algorithm outputs a Nash policy and achieves the best sample complexity compared with the existing guarantee for long horizons, i.e. when $\min \\{ A, B \\} \ll H^2$. Our algorithm also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms need at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.
## [Skip-Attention: Improving Vision Transformers by Paying Less Attention](https://openreview.net/forum?id=vI95kcLAoU) ------> [PDF](https://openreview.net/pdf/987ca002a45640904eb1d5052f9f090bc48fa0bb.pdf)
#### Keywords:  vision transformers   efficiency   redundacy in attention maps   improved throughput
  Abs:  This work aims to improve the efficiency of vision transformers (ViTs).  While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers --  a key redundancy that causes unnecessary computations. Based on this observation, we propose SkipAT a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer's performance while running computationally faster. We show that SkipAT is agnostic to transformer architecture and is effective in image classification,  semantic segmentation on ADE20K, image denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.
## [Accelerating Federated Learning with Quick Distributed Mean Estimation](https://openreview.net/forum?id=v8eWha27jw) ------> [PDF](https://openreview.net/pdf/526b7495fd614089f893a8bc59c75514d9450c04.pdf)
#### Keywords:  Distributed Mean Estimation   Federate Learning   Unbiased Quantization   Communication Efficient   Bandwidth Reduction   Compression
  Abs:  Distributed Mean Estimation (DME), in which clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization.
## [Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs](https://openreview.net/forum?id=uNrFpDPMyo) ------> [PDF](https://openreview.net/pdf/bed6a0aff8359d475400a0927bf8e9b81db61928.pdf)
#### Keywords:  Large Language Model   Efficient Inference   Generative Inference   Key-Value Cache
  Abs:  In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.
## [The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model](https://openreview.net/forum?id=u3dHl287oB) ------> [PDF](https://openreview.net/pdf/780fa86f1bbb8716116a2dcfdf4aea82be0ee944.pdf)
#### Keywords:  deep learning   continual learning   overparameterization   task similarity   catastrophic forgetting   theory
  Abs:  In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. For example, previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting - and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity leads to the most forgetting.  However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.
## [LIFT: Efficient Layer-wise Fine-tuning for Large Model Models](https://openreview.net/forum?id=u0INlprg3U) ------> [PDF](https://openreview.net/pdf/775fdfca08ab5d44202787c2e449ac1f51b0f62a.pdf)
#### Keywords:  efficient fine-tuning; large languag emodels
  Abs:  Fine-tuning is widely applied in language language processing to adapt the model
for downstream tasks. However, as model sizes grow rapidly, fine-tuning the
full model is computationally expensive. Conventional studies mainly focused
on parameter-efficiency, but reducing the number of trainable parameters does
not translate to less backward computation and fine-tuning speedup. Parameter-
efficient fine-tuning still needs to perform complete backward pass to the foremost
layer to calculate required gradients. For example, Adapter reduces the trainable
parameters by 275× but the fine-tuning throughput is only 1.25× better. To achieve
real training throughput improvement, we propose LIFT: a Layer-wise fine-tuning
strategy that only learns one layer of the Transformer architecture at a time. This
approach not only reduces the number of trainable parameters but also improves
the finetuning throughput. We thoroughly evaluated the effectiveness of LIFT on
BERT, GPT, and LLaMA models. LIFT saves the fine-tuning memory by 3.7x and improves the throughput by 2.1x to 2.6x compared to full fine-tuning, while
maintaining the quality.
## [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How](https://openreview.net/forum?id=tqh1zdXIra) ------> [PDF](https://openreview.net/pdf/94931896351a08902d66e0987466a06f0b96c4e6.pdf)
#### Keywords:  Finetuning   pretrained model hubs   transfer learning   hyperparameter optimization   meta-learning
  Abs:  With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a gray-box performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
## [Learning Communication-Efficient Optimizers](https://openreview.net/forum?id=tfYPgTXjzH) ------> [PDF](https://openreview.net/pdf/c55b6c5d3f2690b3f4cc4828ff62807d95a9275b.pdf)
#### Keywords:  learned optimization   local sgd   communication-efficient distributed learning   meta-learning
  Abs:  Communication-efficient variants of SGD, specifically local SGD, have received a great deal of interest in recent years. These approaches compute multiple gradient steps locally, that is on each worker, before averaging model parameters, helping relieve the critical communication bottleneck in distributed deep learning training. Although many variants of these approaches have been proposed, they can sometimes lag behind state-of-the-art optimizers for deep learning. In this work, we incorporate local optimizers that compute multiple updates into a learned optimization framework, allowing to meta-learn potentially more efficient local SGD algorithms. Our results demonstrate that local learned optimizers can substantially outperform local SGD and its sophisticated variants while maintaining their communication efficiency. We show that the learned optimizers can generalize to new datasets and architectures, demonstrating the potential of learned optimizers for improving communication-efficient distributed learning.
## [Efficient Hyperparameter Optimization with Adaptive Fidelity Identification](https://openreview.net/forum?id=tQoGDHn2XO) ------> [PDF](https://openreview.net/pdf/06f2a69378e39feb7ee881f9f9b24d447f496150.pdf)
#### Keywords:  hyperparameter optimization   multi-fidelity   computational efficiency
  Abs:  Hyperparameter optimization is powerful in automatically tuning hyperparameters, with Bayesian Optimization (BO) being a mainstream method for this task. Extending BO into the multi-fidelity setting has been an emerging research topic in this field, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which excels in adaptively deciding the fidelity for each configuration and providing strong performance while ensuring efficient resource usage. These advantages are achieved through our proposed techniques based on the concepts of efficient point and saturation point for each configuration, which can be obtained from the empirical learning curve of the configuration, estimated from early observations. Extensive experiments demonstrate FastBO's superior anytime performance and efficiency in identifying high-quality configurations. We also show that our method provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting the wide applicability of our approach.
## [Liteformer: Lightweight Evoformer for Protein Structure Prediction](https://openreview.net/forum?id=t0m0DdCCQ2) ------> [PDF](https://openreview.net/pdf/4a85632cd6aaecbb2e72215e68f0fd3f8056b48f.pdf)
#### Keywords:  protein structure prediction   efficient transformer
  Abs:  AlphaFold2 has achieved seminal success in predicting structures from amino acid sequences with remarkable atomic accuracy. However, its Evoformer module faces a critical challenge in terms of high memory consumption, particularly concerning the computational complexity associated with sequence length $L$ and the number of Multiple Sequence Alignments (MSA), denoted as $s$. This challenge arises from the attention mechanism involving third-order MSA and pair-wise tensors, leading to a complexity of $\mathcal{O}(L^3+sL^2)$.
This memory bottleneck poses difficulties when working with lengthy protein sequences. To tackle this problem, we introduce a novel and lightweight variant of Evoformer named Liteformer. Liteformer employs an innovative attention linearization mechanism, reducing complexity to $\mathcal{O}(L^2+sL)$ through the implementation of a bias-aware flow attention mechanism, which seamlessly integrates MSA sequences and pair-wise information. Our extensive experiments, conducted on both monomeric and multimeric benchmark datasets, showcase the efficiency gains of our framework.  Specifically, compared with Evoformer, Liteformer achieves up to a 44\% reduction in memory usage and a 23\% acceleration in training speed, all while maintaining competitive accuracy in protein structure prediction.
## [Real-Fake: Effective Training Data Synthesis Through Distribution Matching](https://openreview.net/forum?id=svIdLLZpsA) ------> [PDF](https://openreview.net/pdf/17f9425dffd501a7b9a65394acd5504be5a6ef5e.pdf)
#### Keywords:  Training Data Synthesis   Distribution Matching   Data Efficiency
  Abs:  Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits challenging tasks such as out-of-distribution generalization and privacy preservation.
## [Self-Tuning Self-Supervised Anomaly Detection](https://openreview.net/forum?id=saj54kqrBj) ------> [PDF](https://openreview.net/pdf/f788015d547a914750e2acea068edab81555ccc3.pdf)
#### Keywords:  Anomaly detection   Self-supervised learning   Data augmentation   Hyperparameter optimization
  Abs:  Self-supervised learning (SSL) has emerged as a promising paradigm that presents supervisory signals to real-world problems, bypassing the extensive cost of manual labeling. Consequently, self-supervised anomaly detection (SSAD) has seen a recent surge of interest, since SSL is especially attractive for unsupervised tasks. However, recent works have reported that the choice of a data augmentation function has significant impact on the accuracy of SSAD, posing augmentation search as an essential but nontrivial problem with the lack of labeled validation data. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach for rigorous augmentation tuning on SSAD. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. Second, we present new differentiable augmentation functions, allowing data augmentation hyperparameter(s) to be tuned end-to-end via our proposed validation loss. Experiments on two testbeds with semantic class anomalies and subtle industrial defects show that a systematic tuning of augmentation gives significant performance gains over current practices. All our code and testbeds are available at https://anonymous.4open.science/r/ST-SSAD.
## [Less is More: Selective Layer Finetuning with SubTuning](https://openreview.net/forum?id=sOHVDPqoUJ) ------> [PDF](https://openreview.net/pdf/f4aa66cefcdac634fc4b2a472d1ea9c676593072.pdf)
#### Keywords:  Parameter Efficient Transfer Learning   Multi-task Learning   Understanding Transfer Learning
  Abs:  Finetuning a pretrained model has become the standard approach for training neural networks on novel tasks, leading to rapid convergence and enhanced performance. In this work, we present a parameter-efficient finetuning method, wherein we selectively train a carefully chosen subset of layers while keeping the remaining weights frozen at their initial (pre-trained) values. We observe that not all layers are created equal: different layers across the network contribute variably to the overall performance, and the optimal choice of layers is contingent upon the downstream task and the underlying data distribution. We demonstrate that our proposed method, termed *subset finetuning* (or SubTuning), offers several advantages over conventional finetuning. We show that SubTuning outperforms both finetuning and linear probing in scenarios with scarce or corrupted data, achieving state-of-the-art results compared to competing methods for finetuning on small datasets. When data is abundant, SubTuning often attains performance comparable to finetuning while simultaneously enabling efficient inference in a multi-task setting when deployed alongside other models. We showcase the efficacy of SubTuning across various tasks, diverse network architectures and pre-training methods.
## [T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching](https://openreview.net/forum?id=rnHqwPH4TZ) ------> [PDF](https://openreview.net/pdf/c2834263fe24662570afcfbe457d000aa6319255.pdf)
#### Keywords:  diffusion model   model stitching   efficient sampling
  Abs:  Diffusion probabilistic models (DPMs) achieve great success in generating high-quality data such as images and videos. However, sampling from DPMs at inference time is often expensive for high-quality generation and typically requires hundreds of steps with a large network model. In this paper, we introduce sampling Trajectory Stitching (T-Stitch), a simple yet efficient technique to improve the generation efficiency with little or no loss in the generation quality. Instead of solely using a large DPM for the entire sampling trajectory, T-Stitch first leverages a smaller DPM in the initial steps as a cheap drop-in replacement of the larger DPM and switches to the larger DPM at a later stage. The key reason why T-Stitch works is that different diffusion models learn similar encodings under the same training data distribution. While smaller models are not as effective in refining high-frequency details in later denoising steps, they are still capable of generating good global structures in the early steps. Thus, smaller models can be used in early steps to reduce the computational cost. Notably, T-Stitch does not need any further training and uses only pretrained models. Thus, it can be easily combined with other fast sampling techniques to obtain further efficiency gains across different architectures and samplers. On DiT-XL, for example, 40% of the early timesteps can be safely replaced with a 10x faster DiT-S without performance drop on class-conditional ImageNet generation. By allocating different fractions of small and large DPMs along the sampling trajectory, we can achieve flexible speed and quality trade-offs. We further show that our method can also be used as a drop-in technique to not only accelerate the popular pretrained stable diffusion (SD) models but also improve the prompt alignment of stylized SD models from the public model zoo.
## [Reinforcement Learning with Elastic Time Steps](https://openreview.net/forum?id=riQmzq5FaQ) ------> [PDF](https://openreview.net/pdf/09c8a8534efe929875d3fac5359869d4558a3a0c.pdf)
#### Keywords:  Reinforcement Learning; Elastic Time Steps; Energy efficiency; Data Efficiency; Off-Policy Optimisation.
  Abs:  Reinforcement Learning (RL) is usually modelled as a Markov Decision Process (MDP), where an agent goes through time in discrete time steps. When applied outside of simulation, virtually all existing RL-based control systems maintain the MDP assumptions and use a constant rate control strategy, with a time step that is empirically chosen according to the specific application environment. Controlling dynamic systems with learned policies at the highest, worst-case frequency to guarantee stability can require high computational and energy resources, which can be hard to achieve with on-board hardware. Following the principles of reactive programming, we posit that applying control actions $only$ $when$ $necessary$, can allow the use of simpler hardware and reduce energy consumption. To implement this reactive policy, we break the fixed frequency assumption and propose $RL$ $with$ $elastic$ $time$ $steps$, where the policy determines the next action as well as the duration of the next time step. We also derive a Soft Elastic Actor-Critic (SEAC) algorithm to compute the optimal policy in our new setting. We demonstrate the effectiveness of SEAC both theoretically and experimentally driving an agent in a simulation of a simple world with Newtonian kinematics. Our experiments show higher average returns, shorter task completion times, and reduced energy consumption.
## [SELF-TAILORING PROMPTS FOR PARAMETER EFFICIENT TUNING SPEECH RECOGNITION](https://openreview.net/forum?id=rgjmqqP923) ------> [PDF](https://openreview.net/pdf/6e509f7f91a263b37ff8888118b9e0284ea94564.pdf)
#### Keywords:  Parameter Efficient Fine-tuning   Soft Prompt Tuning   Automatic Speech Recognition
  Abs:  Soft-prompt tuning is an emerging topic for speech recognition despite its success in many natural language processing tasks. Although it appears to be a promising approach for efficiently fine-tuning large speech models, it can suffer from subpar prompt generalization and a lack of instance-specific guidance due to its "one-size-fits-all" template. To address these limitations, we propose a self-tailoring prompting mechanism that adaptively modifies the prompt tokens to incorporate relevant speech utterance-specific information. Self-tailoring mechanism includes simple yet effective prompt masking regularization techniques and a redundancy reduction loss to improve the quality of soft prompt tokens. Extensive experiments demonstrate that our method achieves better generalization capability and consistently achieves improved performance on the speech recognition task under a wide range of acoustic scenarios, including both clean and noisy speech environments. Self-tailoring prompt tuning outperforms the full fine-tuning model with as few as 0.7% of its trainable weights.
## [Octavius: Mitigating Task Interference in MLLMs via MoE](https://openreview.net/forum?id=rTDyN8yajn) ------> [PDF](https://openreview.net/pdf/845e1fce97b62c9768aa508a80e7d313a4da4c7f.pdf)
#### Keywords:  Large Language Model (LLM)   Multi-task learning   Multi-modal learning   Mixture-of-Experts (MoE)   PEFT
  Abs:  Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, to mitigate the interference, we combine the concept of Mixture-of-Experts (MoE) with LoRA and design a multimodal LoRA-MoE decoder for task- and modality-specific learning. The experimental results (about 20% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available
soon.
## [Hierarchical Side-Tuning for Vision Transformers](https://openreview.net/forum?id=r7cYRi7NxT) ------> [PDF](https://openreview.net/pdf/017797ac0385bb60bf3b0051e4abb8e7fce6e5b0.pdf)
#### Keywords:  Deep Learning;Vision Transformer; Parameter-Efficient Transfer Learning;
  Abs:  Fine-tuning pre-trained Vision Transformers (ViT) has consistently demonstrated promising performance in the realm of visual recognition. However, adapting large pre-trained models to various tasks poses a significant challenge. This challenge arises from the need for each model to undergo an independent and comprehensive fine-tuning process, leading to substantial computational and memory demands. 
While recent advancements in Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to achieve superior performance compared to full fine-tuning with a smaller subset of parameter updates, they tend to overlook dense prediction tasks such as object detection and segmentation. In this paper, we introduce Hierarchical Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various downstream tasks effectively. Diverging from existing methods that exclusively fine-tune parameters within input spaces or certain modules connected to the backbone, we tune a lightweight and hierarchical side network (HSN) that leverages intermediate activations extracted from the backbone and generates multi-scale features to make predictions. To validate HST, we conducted extensive experiments encompassing diverse visual tasks, including classification, object detection, instance segmentation, and semantic segmentation. Notably, our method achieves state-of-the-art average Top-1 accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters. When applied to object detection tasks on COCO testdev benchmark, HST even surpasses full fine-tuning and obtains better performance with 49.7 box AP and 43.2 mask AP using Cascade Mask R-CNN.
## [Reshape and Adapt for Output Quantization (RAOQ): Quantization-aware Training for In-memory Computing Systems](https://openreview.net/forum?id=r5sikTJ94y) ------> [PDF](https://openreview.net/pdf/d1eb54c9fbf19517a54c255f5901df09b4d4db0f.pdf)
#### Keywords:  Quantization aware training   efficient deep learning inference   in-memory computing
  Abs:  In-memory computing (IMC) has emerged as a promising solution to address both the computation and data-movement challenges posed by modern AI models. IMC takes advantage of the intrinsic parallelism of memory hardware and performs computation on data in-place directly in the memory array. To do this, IMC typically relies on analog operation, which enables high energy and area efficiency. However, analog operation makes analog-to-digital converters (ADCs) necessary, for converting results back to the digital domain. This introduces an important new source of quantization error, impacting inference accuracy. This paper proposes a Reshape and Adapt for Output Quantization (RAOQ) approach to overcome this issue, which comprises two classes of mechanisms motivated by the fundamental impact and constraints of ADC quantization, including: 1) mitigating ADC quantization error by adjusting the statistics of activations and weights, through an activation-shifting approach (A-shift) and a weight reshaping technique (W-reshape); 2) adapting AI models to better tolerate ADC quantization, through a bit augmentation method (BitAug) to aid SGD-based optimization. RAOQ demonstrates consistently high performance across different scales of neural network models for image classification, object detection, and natural language processing (NLP) tasks at various ADC bit precisions, achieving state-of-the-art accuracy with practical IMC implementations.
## [Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling](https://openreview.net/forum?id=qmXedvwrT1) ------> [PDF](https://openreview.net/pdf/9012a47e004aec55f663c2e8a55289c404ecd40b.pdf)
#### Keywords:  Efficient diffusion models   short-span attention   local-feature enrichment   global-content orchestration   multi-scale generation
  Abs:  Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution image across all bricks. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.
## [BiDST: Dynamic Sparse Training is a Bi-Level Optimization Problem](https://openreview.net/forum?id=qbw861vueP) ------> [PDF](https://openreview.net/pdf/d2616a14c92221e99afdca1c7f9be8a696c1550a.pdf)
#### Keywords:  sparse training   efficient learning algorithm
  Abs:  Dynamic Sparse Training (DST) is an effective approach for addressing the substantial training resource requirements posed by the ever-increasing size of the Deep Neural Networks (DNNs). Characterized by its dynamic ``train-prune-grow'' schedule during training, DST implicitly develops a bi-level structure for training the weights while discovering a subnetwork topology. However, such a structure is consistently overlooked by the current DST algorithms for further optimization opportunities, and these algorithms, on the other hand, solely optimize the weights while determining masks heuristically. In this paper, we extensively study DST algorithms and argue that the training scheme of DST naturally forms a bi-level problem in which the updating of weight and mask is interdependent. Based on this observation, we introduce a novel efficient training framework called BiDST, which for the first time, introduces bi-level optimization methodology into dynamic sparse training domain. Unlike traditional partial-heuristic DST schemes, which suffer from sub-optimal search efficiency for masks and miss the opportunity to fully explore the topological space of neural networks, BiDST excels at discovering excellent sparse patterns by optimizing mask and weight simultaneously, resulting in maximum $2.62$% higher accuracy, $2.1\times$ faster execution speed, and $25\times$ reduced overhead. Code will be released.
## [CAST: Clustering self-Attention using Surrogate Tokens for efficient transformers](https://openreview.net/forum?id=qYwdyvvvqQ) ------> [PDF](https://openreview.net/pdf/16bb05cdf2c892a5ce801d91ed421b725c321d62.pdf)
#### Keywords:  Self-attention mechanism   Clustering self-attention mechanism   Complexity   Efficient transformers   LRA benchmark
  Abs:  The Transformer architecture has shown  to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\alpha N)$ where $N$ is the sequence length, and $\alpha$ is constant according to the number of clusters and samples per cluster. 
We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.
## [Large Language Models as Tool Makers](https://openreview.net/forum?id=qV83K9d5WB) ------> [PDF](https://openreview.net/pdf/8b9acb57dfb1a985a1a29c2d7ee7b83784f8397f.pdf)
#### Keywords:  large language models   tool making   tool using   serving efficiency
  Abs:  Recent research has highlighted the potential of large language models (LLMs)
to improve their problem-solving capabilities with the aid of suitable external
tools. In our work, we further advance this concept by introducing a closed-
loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs
create their own reusable tools for problem-solving. Our approach consists of two
phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set
of tasks, where a tool is implemented as a Python utility function. 2) tool using:
another LLM acts as the tool user, which applies the tool built by the tool maker
for problem-solving. The tool user can be either the same or a different LLM
from the tool maker. On the problem-solving server side, tool-making enables
continual tool generation and caching as new requests emerge. This framework
enables subsequent requests to access cached tools via their corresponding APIs,
enhancing the efficiency of task resolution. Beyond enabling LLMs to create their
own tools, our framework also uncovers intriguing opportunities to optimize the
serving cost of LLMs: Recognizing that tool-making requires more sophisticated
capabilities, we assign this task to a powerful, albeit resource-intensive, model.
Conversely, the simpler tool-using phase is delegated to a lightweight model. This
strategic division of labor allows the once-off cost of tool-making to be spread
over multiple instances of tool-using, significantly reducing average costs while
maintaining strong performance. Furthermore, our method offers a functional
cache through the caching and reuse of tools, which stores the functionality of
a class of requests instead of the natural language responses from LLMs, thus
extending the applicability of the conventional cache mechanism. We evaluate
our approach across various complex reasoning tasks, including Big-Bench tasks.
With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates
performance equivalent to using GPT-4 for both roles, but with a significantly
reduced inference cost.
## [Efficient Integrators for Diffusion Generative Models](https://openreview.net/forum?id=qA4foxO5Gf) ------> [PDF](https://openreview.net/pdf/b723203680f0b71bde5582f8400e22e6f2cb5933.pdf)
#### Keywords:  Diffusion models   Deep Generative Models   Efficient Sampling
  Abs:  Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey \& Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints will be made publicly available.
## [Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling](https://openreview.net/forum?id=q8mZ8AVl5d) ------> [PDF](https://openreview.net/pdf/bfdda991f702e5ecd791ede2db46ebde45aa87d5.pdf)
#### Keywords:  multimodal   parameter-efficient fine-tuning   LLM   NLP
  Abs:  Large language models (LLMs) and Vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^12) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).
## [SqueezeLLM: Dense and Sparse Quantization](https://openreview.net/forum?id=pZhdz4oyzo) ------> [PDF](https://openreview.net/pdf/3403f28169bf43687b6dbc81203ddada5cf09872.pdf)
#### Keywords:  Quantization   model compression   efficient LLM   efficient inference
  Abs:  Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1× as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3× speedup compared to the baseline.
## [Generative Modeling of Individual Behavior at Scale](https://openreview.net/forum?id=pTqmVbBa8R) ------> [PDF](https://openreview.net/pdf/fdc70e70045c6d2aff348290258debac92dd0c7a.pdf)
#### Keywords:  parameter efficient finetuning   chess   play style   stylometry   interpretation of learned representations
  Abs:  Recent years have seen a growing interest in using AI to model human behavior, particularly in domains where humans learn from or collaborate with this AI. While most existing work attempts to model human behavior at an aggregate level, our goal is to model behavior at the individual level. Recent work in the domain of chess has shown that behavioral stylometry, or the task of identifying a person from their actions alone, can be achieved with high accuracy among a pool of a few thousand players. We provide a new perspective on behavioral stylomery by connecting it to the vast literature of transfer learning in NLP. Specifically, by casting the stylometry problem as a multi-task learning problem---where each task is a distinct person---we show that parameter efficient fine-tuning (PEFT) methods can be adapted to perform stylometry at an unprecedented scale (47,864 players), while enabling few-shot learning for unseen players. Our approach leverages recent modular PEFT methods to learn a set of skill parameters that can be combined in different ways using style vectors. Style vectors enable two important capabilities. First, they make our approach generative, in that we can generate actions in the style of a player by simply indexing into that player's style vector. Second, they induce a latent style space that we can interpreted and manipulated algorithmically. This allows us to compare different player styles, as well as synthesize new (human-like) styles, e.g., merging the styles of two players or interpolating between their styles.
## [Maximizing Benefits under Harm Constraints: A Generalized Linear Contextual Bandit Approach](https://openreview.net/forum?id=pCbCcXLzSz) ------> [PDF](https://openreview.net/pdf/81487a250833e62bcd88ae75aa395846cfa44f9b.pdf)
#### Keywords:  Contextual multi-armed bandit   online learning   generalized linear models   varying coefficient models
  Abs:  In many contextual sequential decision-making scenarios, such as dose-finding clinical trials for new drugs or personalized news article recommendation systems in social media, each action can simultaneously carry both benefits and potential harm. This could manifest as efficacy versus side effects in clinical trials, or increased user engagement versus the risk of radicalization and psychological distress in news recommendation.
These multifaceted situations can be modeled using the multi-armed bandit (MAB) framework. Given the intricate balance of positive and negative outcomes in these contexts, there is a compelling need to develop methods which can maximize benefits while limiting harm within the MAB framework. This paper aims to address this gap. The primary contributions of this paper are two-fold:
(i) We propose a novel contextual MAB model with the objective of optimizing reward potential while maintaining certain harm constraints. In this model both rewards and harm are governed by a generalized linear model with coefficients that vary based on the contextual variables. This flexibility allows the model to be broadly applicable for a wide range of scenarios.
(ii) Building on our proposed generalized linear contextual MAB model, we develop an $\epsilon$-greedy-based policy. This policy is designed to strike an effective balance between the dual objectives of exploration-exploitation to achieve the desired trade-off between benefit and harm. We demonstrate that this policy achieves a sublinear $\mathcal{O}(\sqrt{T\log T})$ regret.
Extensive experimental results are presented to support our theoretical analyses and validate the effectiveness of our proposed model and policy.
## [Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks](https://openreview.net/forum?id=pAVJKp3Dvn) ------> [PDF](https://openreview.net/pdf/882188e930201dabb0aa14a40c59c4d6806e0bee.pdf)
#### Keywords:  Structured Matrix   Block Low Rank   Low Rank   Efficient Neural Network   Transformer   Fourier   Dirichlet Kernel   FFT   Boxcar   Pruning   Compression
  Abs:  This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. Finally, we introduce an effective initialization method for the proposed scheme. Our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.
## [AdaLomo: Low-memory Optimization with Adaptive Learning Rate](https://openreview.net/forum?id=p4B7rl1UFA) ------> [PDF](https://openreview.net/pdf/1e5b4b84e3882020c30e71cfffa59daaca2abb0e.pdf)
#### Keywords:  Memory-efficient   Optimization   Large language models
  Abs:  Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.
## [Efficient Offline Reinforcement Learning: The Critic is Critical](https://openreview.net/forum?id=oWKPZ1Hcsm) ------> [PDF](https://openreview.net/pdf/1bbdaf423b60fe4e31b136237ba8bb70629517ac.pdf)
#### Keywords:  Offline reinforcement learning   reinforcement learning   efficiency   temporal-difference learning   value error   Bellman error
  Abs:  Recent work has demonstrated both benefits and limitations from using supervised approaches (without temporal-difference learning) for offline reinforcement learning. While off-policy reinforcement learning provides a promising approach for improving performance beyond supervised approaches, we observe that training is often inefficient and unstable due to temporal difference bootstrapping. In this paper we propose a best-of-both approach by first learning the behaviour policy and critic with supervised learning, before improving with off-policy reinforcement learning. Crucially, we demonstrate that the critic can be learned by pre-training with a supervised Monte-Carlo value-error, making use of commonly neglected downstream information from the provided offline trajectories. This provides consistent initial values for efficient improvement with temporal difference learning. We further generalise our approach to entropy-regularised reinforcement learning and apply our proposed pre-training to state-of-the-art hard and soft off-policy algorithms. We find that we are able to more than halve the training time of the considered offline algorithms on standard benchmarks, and surprisingly also achieve greater stability. We further build on our insight into the importance of having consistent policy and value functions to propose novel hybrid algorithms that regularise both the actor and the critic towards the behaviour policy. This maintains the benefits of pre-training when learning from limited human demonstrations.
## [SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models](https://openreview.net/forum?id=oMRrVXrHxW) ------> [PDF](https://openreview.net/pdf/0991e90500bfa7d6a08f40ff670124302e435142.pdf)
#### Keywords:  vision-language models   efficient training   generative models
  Abs:  In this paper, we propose ``SimVLG'', a streamlined framework for the pre-training of computationally intensive vision-language generative models, leveraging frozen pre-trained large language models (LLMs). The prevailing paradigm in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, aimed at extracting and consolidating pertinent visual features, followed by a subsequent phase focusing on end-to-end alignment between visual and linguistic modalities. Our one-stage, single-loss framework circumvents the aforementioned computationally demanding first stage of training by gradually merging similar visual tokens during training. This gradual merging process effectively compacts the visual information while preserving the richness of semantic content, leading to fast convergence without sacrificing performance. Our experiments show that our approach can speed up the training of vision-language models by a factor $\times 5$ without noticeable impact on the overall performance. Additionally, we show that our models can achieve comparable performance to current vision-language models with only $1/10$ of the data. Finally, we demonstrate how our image-text models can be easily adapted to video-language generative tasks through a novel soft attentive temporal token merging modules.
## [Transformer-VQ: Linear-Time Transformers via Vector Quantization](https://openreview.net/forum?id=oDdzXQzP2F) ------> [PDF](https://openreview.net/pdf/eb58722b10b58ef3f7983340d60242c5dfd627c4.pdf)
#### Keywords:  Transformer   Transformer Decoder   Decoder-Only Transformer   Natural Language Processing   NLP   Vector Quantization   VQ   K-Means   Clustering   Causal Attention   Autoregressive Attention   Efficient Attention   Linear-Time Attention   Autoregressive Modeling   Generative Modeling   Gated Attention   Compressive Attention   Kernelized Attention   Kernelizable Attention   Hierarchical Attention   Segment-Level Recurrent Attention   Long-Context Modeling   Long-Range Modeling   Long-Range Dependencies   Long-Term Dependencies   Cached Attention   Shift-Equivariant Attention
  Abs:  We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time.  Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. 
In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
## [Structured Pruning of CNNs at Initialization](https://openreview.net/forum?id=nuSbbzqULg) ------> [PDF](https://openreview.net/pdf/83afb69b29e249e68c11c6675813797bdbbc4f17.pdf)
#### Keywords:  Pruning   Pruning-at-Initialization   Structured Pruning   Efficient Deep Learning   Efficient Model   Acceleration   Deep Learning Acceleration   Convolutional Neural Network
  Abs:  Pruning-at-initialization (PAI) methods prune the individual weights of a convolutional neural network (CNN) before training, thus avoiding expensive fine-tuning or retraining of the pruned model. While PAI shows promising results in reducing model size, the pruned model still requires unstructured sparse matrix computation, making it difficult to achieve a real speedup. In this work, we show both theoretically and empirically that the accuracy of CNN models pruned by a PAI method is independent of the granularity of pruning when layer-wise density (i.e., the fraction of the remaining parameters in each layer) remains the same. We formulate PAI as a convex optimization problem based on an expectation-based proxy for model accuracy, which can instantly produce the optimal allocation of the layer-wise densities with respect to the proxy model. Using our formulation, we further propose a structured and hardware-friendly PAI method, named PreCrop, to prune or reconfigure CNNs in the channel dimension. Our empirical results show that PreCrop achieves a higher accuracy than existing PAI methods on several popular CNN architectures, including ResNet, MobileNetV2, and EfficientNet, on both CIFAR-10 and ImageNet. Notably, PreCrop achieves an accuracy improvement of up to 1.9% over a state-of-the-art PAI algorithm when pruning MobileNetV2 on ImageNet.
## [Provably Efficient Learning in Partially Observable Contextual Bandit](https://openreview.net/forum?id=nsvgVuaWXK) ------> [PDF](https://openreview.net/pdf/99be8e29bc117097bd55dbbc6f7e968b985da03f.pdf)
#### Keywords:  partially observable contextual bandit   causal bound   provably efficient
  Abs:  In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods.
## [Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=nhub8Pjp7y) ------> [PDF](https://openreview.net/pdf/7bc0e8a08657c0d9cbb4f42cec855090c975e11c.pdf)
#### Keywords:  Trojan attacks   Parameter-efficient fine-tuning   Pre-trained language models
  Abs:  Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empirically provide possible explanations for PETA's efficacy: the bilevel optimization inherently 'orthogonalizes' the backdoor and PEFT modules, thereby retaining the backdoor throughout PEFT. Based on this insight, we explore a simple defense that omits PEFT in selected layers of the backdoored PLM and unfreezes a subset of these layers' parameters, which is shown to effectively neutralize PETA.
## [REDUCR: Robust Data Downsampling Using Class Priority Reweighting](https://openreview.net/forum?id=nKYTiJhhAu) ------> [PDF](https://openreview.net/pdf/5dafad923f675c7e976323a5a22b04d2a80b57ce.pdf)
#### Keywords:  Class Robustness   Online Batch Selection   Robust Machine Learning   Training Efficiency   Data Downsampling   Class Imbalance
  Abs:  Modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion. To reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints. However, these techniques can suffer from poor worst-class generalization performance due to class imbalance and distributional shifts. This work introduces REDUCR, a robust and efficient data downsampling method that uses class priority reweighting. REDUCR reduces the training data while preserving worst-class generalization performance. REDUCR assigns priority weights to datapoints in a class-aware manner using an online learning algorithm. We demonstrate the data efficiency and robust performance of REDUCR on vision and text classification tasks. On web-scraped datasets with imbalanced class distributions, REDUCR achieves significant test accuracy boosts for the worst-performing class (but also on average), surpassing state-of-the-art methods by around 15\%.
## [Learning to Select Camera Views: Efficient Multiview Understanding at Few Glances](https://openreview.net/forum?id=mzWQ2hOKNX) ------> [PDF](https://openreview.net/pdf/eb3fed336afa268c273ceb0b0c4e274a571e07a6.pdf)
#### Keywords:  Multi-view classification; multi-view detection; efficient algorithm; reinforcement learning
  Abs:  Multiview camera setups have proven useful in many computer vision applications for reducing ambiguities, mitigating occlusions, and increasing field-of-view coverage. However, the high computational cost associated with multiple views creates a significant challenge for end devices with limited computational resources. To address this issue, we propose a view selection approach that analyzes the target object or scenario from given views and selects the next-best-view for recognition or detection. Our approach features a reinforcement learning based camera selection module, MVSelect, that not only selects views but also facilitates joint training with the task network. Experimental results on multiview classification and detection tasks show that our approach achieves promising performance while using only 2 or 3 out of N available views, significantly reducing computational costs. Furthermore, analysis on the selected views reveals that certain cameras can be shut off with minimal performance impact, shedding light on future camera layout optimization for multiview systems.
## [Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding](https://openreview.net/forum?id=mqVgBbNCm9) ------> [PDF](https://openreview.net/pdf/9e1e39c03454cf85ccad319bd35b19bbcb016a4b.pdf)
#### Keywords:  large language model   efficient inference   data-centric optimization   parallel decoding   prompt engineering
  Abs:  This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose Skeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-ups across 12 LLMs, but it can also potentially improve the answer quality on several question categories. SoT is an initial attempt at data-centric optimization for inference efficiency, and further underscores the potential of pushing LLMs to think more like a human for answer quality.
## [NanoLM: An Affordable LLM Study Benchmark via Accurate Loss Prediction Across Scales](https://openreview.net/forum?id=mao3y822aM) ------> [PDF](https://openreview.net/pdf/53e6a601e28adf0fb1e5cc86bcb05e1cf355fbd3.pdf)
#### Keywords:  Large Language Model   Scaling Law   Hyperparameter Transfer   Hyperparameter Tuning
  Abs:  High computational cost, data collection, and difficulty in distributed training are the three significant barriers in pre-training large language models (LLMs) for many researchers. 
In this paper, we try to solve the question ''Under constrained computational resources, what type of model design(eg. model size, model architecture) should I train in order to to achieve the best possible performance?" To answer this question, based on Scaling Laws for LLM, we introduce nanoLM: an affordable LLM Study Benchmark via Accurate Loss Prediction across scales. This benchmark unlocks a new LLM study paradigm without direct training. Under the loss basin area, the training loss and model size can be accurately fitted as a power law. This allows us to extrapolate LM from small- to large-scale. For example, with just 13.1%, 14.2% of the total pretraining cost, we can accurately forecast the loss for models sized 26B and 52B. To ensure compatibility with mainstream Transformer architectures, nanoLM offers support for decoder-only structures (eg., GPT), encoder-only structures (eg., BERT), and encoder-decoder structures (eg., T5). Considering that excessive model parameters might lead to GPU memory overflow, nanoLM also supports for data parallelism strategies. Our goal with nanoLM is to empower researchers to make cheap and meaningful comparisons of varying model designs at large scales. We also aspire for our benchmark to serve as a bridge between the academic community and the industry.
## [The Expressive Power of Low-Rank Adaptation](https://openreview.net/forum?id=likXVjmh3E) ------> [PDF](https://openreview.net/pdf/94dab51bcb53056c946d865636dec72eaeed0e61.pdf)
#### Keywords:  LoRA   expressive power   parameter-efficient fine-tuning   adaptation   neural networks   transformer
  Abs:  *Low-Rank Adaptation* (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models.
Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. 
This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. 
We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\bar{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\bar{f}}{\text{depth of }f}$, under a mild assumption. 
We also quantify the approximation error when the LoRA-rank is lower than the threshold. 
For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
All our theoretical insights are validated by numerical experiments.
## [Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling](https://openreview.net/forum?id=lOwkOIUJtx) ------> [PDF](https://openreview.net/pdf/d0270e52c5d7cb778439b2da382a9fe9d994ae03.pdf)
#### Keywords:  Biological inspired high performance energy efficient vision system   data efficient training   energy saving sensoring   learned saccade   reinforcement learning   foveated visual sampling   continuous scene reconstruction.
  Abs:  High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.
## [Parameter Estimation of Long Memory Stochastic Processes with Deep Neural Networks](https://openreview.net/forum?id=lLhEQWQYtb) ------> [PDF](https://openreview.net/pdf/72771de84ed45e0dd594c155a8a2a5a887afad97.pdf)
#### Keywords:  Hurst parameter   Fractional Brownian motion   ARFIMA   Fractional Ornstein-Uhlenbeck processes   1D convolutional neural networks   LSTM
  Abs:  We present a pure deep neural network-based approach for estimating long memory parameters of time series models that incorporate the phenomenon of long range dependence. Long memory parameters such as the Hurst exponent are critical in characterizing the long-range dependence, roughness, and self-similarity of stochastic processes. The accurate and fast estimation of these parameters is of paramount importance in various scientific fields, including finance, physics, and engineering. We harnessed efficient process generators to provide high-quality synthetic training data to train 1D Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models. Our neural models outperform conventional statistical methods, even if the latter have neural network extensions. Precision, speed as well as consistency and robustness of the estimators are supported by experiments with fractional Brownian motion (fBm), the Autoregressive Fractionally Integrated Moving Average (ARFIMA) process, and the fractional Ornstein-Uhlenbeck process (fOU). We believe that our work will inspire further research in the application of deep learning techniques for stochastic process modeling and parameter estimation.
## [Efficient ConvBN Blocks for Transfer Learning and Beyond](https://openreview.net/forum?id=lHZm9vNm5H) ------> [PDF](https://openreview.net/pdf/7780dda2acd86ad100be6f1c5e1f03ac05d847b5.pdf)
#### Keywords:  transfer learning   batch normalization   efficient training
  Abs:  Convolution-BatchNorm (ConvBN) blocks are integral components in various computer vision tasks and other domains. A ConvBN block can operate in three modes: Train, Eval, and Deploy. While the Train mode is indispensable for training models from scratch, the Eval mode is suitable for transfer learning and beyond, and the Deploy mode is designed for the deployment of models. This paper focuses on the trade-off between stability and efficiency in ConvBN blocks: Deploy mode is efficient but suffers from training instability; Eval mode is widely used in transfer learning but lacks efficiency. To solve the dilemma, we theoretically reveal the reason behind the diminished training stability observed in the Deploy mode. Subsequently, we propose a novel Tune mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune mode is as stable as Eval mode for transfer learning, and its computational efficiency closely matches that of the Deploy mode. Through extensive experiments in object detection, classification, and adversarial example generation across $5$ datasets and $12$ model architectures, we demonstrate that the proposed Tune mode retains the performance while significantly reducing GPU memory footprint and training time, thereby contributing efficient ConvBN blocks for transfer learning and beyond. Our method has been integrated into both PyTorch (general machine learning framework) and MMCV/MMEngine (computer vision framework). Practitioners just need one line of code to enjoy our efficient ConvBN blocks thanks to PyTorch's builtin machine learning compilers.
## [Dynamic Sparse Training with Structured Sparsity](https://openreview.net/forum?id=kOBkxFRKTA) ------> [PDF](https://openreview.net/pdf/78040ab3f547afac1bceaa0ceb1be6f061cb18f6.pdf)
#### Keywords:  Machine Learning   dynamic sparse training   structured sparsity   N:M sparsity   efficient deep learning   RigL   SRigL   constant fan-in   dynamic neuron ablation   neuron ablation   structured and fine-grained sparsity   online inference   accelerating inference
  Abs:  Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained structured N:M sparsity by imposing a constant fan-in constraint. Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures. We demonstrate reduced real-world timings on CPU for online inference — 3.6×/2× faster at 90% sparsity than equivalent dense/unstructured sparse layers, respectively
## [ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference](https://openreview.net/forum?id=kMp8zCsXNb) ------> [PDF](https://openreview.net/pdf/824bb7ecf94b2146fa367b6c747baf17c7486c96.pdf)
#### Keywords:  Implicit Neural Representation   Coordinate Network   Multi-resolution   Efficient Inference
  Abs:  Coordinate network or implicit neural representation (INR) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an INR, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate (MAC) count. This is particularly critical in use cases where inference bandwidth is greatly limited by hardware constraints. To this end, we propose the Activation-Sharing Multi-Resolution (ASMR) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations. Specifically, an ASMR model enables the sharing of activations across grids of the data. This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near $O(1)$ inference complexity irrespective of the number of layers. Experiments show that ASMR can reduce the MAC of a vanilla SIREN model by up to 350$\times$ while achieving an even higher reconstruction quality than its SIREN baseline.
## [Towards Exact Computation of Inductive Bias](https://openreview.net/forum?id=kKxvFpvV04) ------> [PDF](https://openreview.net/pdf/80185e3e04a9e9cfb373bcbef59dc95758d25ca2.pdf)
#### Keywords:  generalization   inductive bias   efficiency   information theory   complexity
  Abs:  Much research in machine learning involves finding appropriate inductive biases (e.g. convolutional neural networks, momentum-based optimizers, transformers) to promote generalization on tasks. However, quantification of the amount of inductive bias associated with these architectures and hyperparameters has been limited. We propose a novel method for efficiently computing the inductive bias required for generalization on a task with a fixed training data budget; formally, this corresponds to the amount of information required to specify well-generalizing models within a specific hypothesis space of models. Our approach involves sampling from the hypothesis space and modeling the loss distribution of hypotheses to estimate the required inductive bias for a task. Unlike prior work, our method provides a direct estimate of inductive bias without using bounds and is applicable to diverse hypothesis spaces. Moreover, we derive approximation error bounds for our estimation approach in terms of the number of sampled hypotheses. Consistent with prior results, our empirical results demonstrate that higher dimensional tasks require greater inductive bias. We show that relative to other expressive model classes, neural networks as a model class encode massive amounts of inductive bias. Furthermore, our measure quantifies the relative difference in inductive bias between different neural network architectures (e.g. with varying width and depth). Our proposed inductive bias metric provides an information-theoretic interpretation of the benefits of specific model architectures for certain tasks and provides a quantitative guide to developing tasks requiring greater inductive bias, thereby encouraging the development of more powerful inductive biases.
## [Everyone Deserves A Reward: Learning Customized Human Preferences](https://openreview.net/forum?id=jXR5pjs1rV) ------> [PDF](https://openreview.net/pdf/60ccc2f81e3f619256906e57bb3a3c56ebdc6c4d.pdf)
#### Keywords:  Human Preference Alignment   Large Language Model   Data Efficiency
  Abs:  Reward models (RMs) are essential for aligning large language models (LLMs) with human preferences to improve interaction quality. However, the real world is pluralistic, which leads to diversified human preferences with respect to different religions, politics, cultures, etc. Moreover, each individual can have their unique preferences on various topics. Neglecting the diversity of preferences, current human feedback aligning methods only consider a general reward model, which is below satisfaction for customized or personalized application scenarios. To explore customized preference learning, we collect a domain-specific preference (DSP) dataset, which consists of comprehensive user queries and corresponding responses preferred from four practical domains. Besides, from the perspective of data efficiency, we propose a three-stage customized RM learning scheme, then empirically verify its effectiveness on both general preference datasets and our DSP set. Furthermore, we test multiple training and data strategies on the three learning stages. We find several ways to better preserve the general preferring ability while training the customized RMs, especially general preference enrichment, and customized preference imitation learning.
## [Jointly-Learned Exit and Inference for a Dynamic Neural Network](https://openreview.net/forum?id=jX2DT7qDam) ------> [PDF](https://openreview.net/pdf/78cb7f07c02fcb7e48ef9571dfa5ddb1fb60f668.pdf)
#### Keywords:  early-exit dynamic networks   efficient inference
  Abs:  Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for $\textit{every}$ inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs are decoupled during training, leading to a train-test mismatch; and 2) the thresholding gating mechanism introduces a positive bias into the predictive probabilities, making it difficult to readily extract uncertainty information. We propose a novel architecture that connects these two modules. This leads to significant performance improvements on classification datasets and enables better uncertainty characterization capabilities.
## [Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models](https://openreview.net/forum?id=jVsXDLIt45) ------> [PDF](https://openreview.net/pdf/bf5a3b91221268ff5a962f7e5db9e2c5e0ac66e2.pdf)
#### Keywords:  large language models   efficient transformers   autoencoder
  Abs:  Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed “nuggets” which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
## [Aligner: One Global Token is Worth Millions of Parameters When Aligning LLMs](https://openreview.net/forum?id=jTdqzBGMsq) ------> [PDF](https://openreview.net/pdf/e999270a1e5850bd100f13de3fd468b7719c1cce.pdf)
#### Keywords:  LLM   Parameter-Efficient-Finetuning   Alignment   Human Preference   Value
  Abs:  We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method for aligning multi-billion-sized Large Language Models (LLMs). Aligner employs a unique design that constructs a globally shared set of tunable tokens that will change the attention of every layer. Remarkably with this method, even when using one token accounting for a mere 5,000 parameters, Aligner can still perform comparably well to state-of-the-art methods like LoRA that require millions of parameters. This capacity is substantiated in both instruction following and value alignment tasks. Besides the multiple order-of-magnitude improvement in parameter efficiency, the insight Aligner provides into the internal mechanisms of LLMs is also valuable. The architectural features and efficacy of our method demonstrate that an LLM separates its handling of "form" and "knowledge" internally in some orthogonal manner. This finding should give impetus to new research into LLM mechanism understanding and value alignment.
## [Privacy Preserving API Fine-tuning for LLMs](https://openreview.net/forum?id=jMJ9IRWmH9) ------> [PDF](https://openreview.net/pdf/10c60fbbe43c04f28176807deae7ffdc34d0849a.pdf)
#### Keywords:  Split Learning   Vertical Federated Learning   Federated Learning   Parameter Efficient Fine-tuning   Large Language Models
  Abs:  As deep learning models become larger and more expensive, many practitioners turn to fine-tuning APIs. 
These web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model. 
While convenient, the fine-tuning APIs raise a new concern: the data of the client is at risk of privacy breach during the training procedure.
This challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large pre-trained model.
In this study, we systematically search for a way to fine-tune models over an API  *while keeping the labels private*.
We analyze the privacy of popular algorithms for parameter-efficient fine-tuning when training over an API.
Using this analysis, we propose P$^3$EFT, a two-party split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead.
To validate our algorithm, we fine-tune DeBERTa-v2-XXLarge and Flan-T5 using LoRA adapters on a range of common NLP tasks. We find that P$^3$EFT is competitive with existing privacy-preserving methods in a two-party setup while having higher accuracy.
## [Parameter-Efficient Multi-Task Model Fusion with Partial Linearizeation](https://openreview.net/forum?id=iynRvVVAmH) ------> [PDF](https://openreview.net/pdf/7e0debc28ab733e23de3a27da1d02281b4f88436.pdf)
#### Keywords:  model fusion   parameter-efficient fine-tuning
  Abs:  Large pre-trained models have enabled significant advances in machine learning and served as foundation components.
Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. 
However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion.
In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning.
Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters.
This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently.
We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outperforming standard adapter tuning and task arithmetic alone.
Experimental results demonstrate the capabilities of our proposed partial linearization technique to effectively construct unified multi-task models via the fusion of fine-tuned task vectors. 
We evaluate performance over an increasing number of tasks and find that our approach outperforms standard parameter-efficient fine-tuning techniques. The results highlight the benefits of partial linearization for scalable and efficient multi-task model fusion.
## [SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers](https://openreview.net/forum?id=iq2FBcjYRn) ------> [PDF](https://openreview.net/pdf/985470cb3632de7b6cfaf0619d50def322d6b604.pdf)
#### Keywords:  Adversarial attack   Efficient Transformers   Energy Attack   Transformers   Universal Adversarial Patch
  Abs:  Recently, there has been a lot of progress in reducing the computation of deep models at inference time. These methods can reduce both the computational needs and power usage of deep models. Some of these approaches adaptively scale the compute based on the input instance. We show that such models can be vulnerable to a universal adversarial patch attack, where the attacker optimizes for a patch that when pasted on any image, can increase the compute and power consumption of the model. We run experiments with three different efficient vision transformer methods showing that in some cases, the attacker can increase the computation to the maximum possible level by simply pasting a patch that occupies only 8\% of the image area. We also show that a standard adversarial training defense method can reduce some of the attack's success. We believe adaptive efficient methods will be necessary for the future to lower the power usage of deep models, so we hope our paper encourages the community to study the robustness of these methods and develop better defense methods for the proposed attack.
## [Efficient Modulation for Vision Networks](https://openreview.net/forum?id=ip5LHJs6QX) ------> [PDF](https://openreview.net/pdf/f79235b2cdf61e79c075df402998ed7458906982.pdf)
#### Keywords:  EfficientMod   Efficient Networks
  Abs:  In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the abstracted modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Bene- fiting from the prominent representational ability of modulation mechanism and the efficiency of efficient modulation design, our network can accomplish better accuracy-efficiency trade-offs and set new state-of-the-art performance for efficient networks. When integrating EfficientMod block with the vanilla self-attention block, we obtain the hybrid architecture and further improve the performance without sacrificing the efficiency. We carry out comprehensive experiments to verify EfficientMod’s performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than the prior state-of-the-art approach EfficientFormerV2-s2 without any training tricks and is 25% faster on GPU. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Codes and checkpoints are available in the supplementary material.
## [Efficient OCR for Building a Diverse Digital History](https://openreview.net/forum?id=ihX0d33lk5) ------> [PDF](https://openreview.net/pdf/89ae5638b61ee1712518644213f000c00d379303.pdf)
#### Keywords:  contrastive learning   efficient computer vision   low resource settings
  Abs:  Many users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) – which jointly learns a vision and language model - is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters’ visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, it opens new avenues for community engagement in making digital history more representative of documentary history.
## [HART: Efficient Adaptation via Regularized Autoregressive Parameter Generation](https://openreview.net/forum?id=iP8ig954Uz) ------> [PDF](https://openreview.net/pdf/d4253a89ca59d8cb3c5beffebec5e33454d4dc38.pdf)
#### Keywords:  hypernetwork   weight generation   parameter-efficient fine-tuning   task adaptation   in-context learning
  Abs:  Fine-tuning is an effective approach for adapting a pre-trained language model to downstream tasks, but it incurs a high computational cost. To achieve an extremely efficient task adaptation, \citet{phang2022hypertuning} have proposed to use an auxiliary hypernetwork to generate task-specific weights without any backpropagation. A hypernetwork can generate weights for parameter-efficient fine-tuning (PEFT) modules, such as prefixes \citep{li2021prefix} and LoRAs \citep{hu2021lora}, for any unseen task based on a few task-specific demonstration examples, at the cost of a single forward pass. However, hypernetwork training is challenging. Firstly, it is sample inefficient due to the under-exploitation of the dependencies between PEFT weights across layers. Secondly, it exhibits training instability due to the high diversity of few-shot demonstration inputs. To address these limitations, we propose a novel hypernetwork training approach, named HART. It exploits layerwise dependencies by autoregressively generating weights for individual layers, and stabilizes the training by regularizing the consistency between weights generated based on different demonstrations. We train the hypernetwork on a diverse collection of tasks \citep{wang2022super,sanh2021multitask} and evaluate its performance on unseen tasks. HART notably outperforms \citet{phang2022hypertuning} on both T5-Large and T5-XL models.
## [Reliable Test-Time Adaptation via Agreement-on-the-Line](https://openreview.net/forum?id=iEFMwP5wng) ------> [PDF](https://openreview.net/pdf/9d561582323440bb1a95414f5073df610702b6ff.pdf)
#### Keywords:  Test-time adaptation;Agreement-on-the-line;Accuarcy-on-the-line;Error estimation;Hyperparameter tuning;Calibration
  Abs:  Test-time adaptation (TTA) methods aim to improve robustness to distribution shifts by adapting models using unlabeled data from the shifted test distribution. However, there remain unresolved challenges that undermine the reliability of TTA, which include difficulties in evaluating TTA performance, miscalibration after TTA, and  unreliable hyperparameter tuning for adaptation. In this work, we make a notable and surprising observation that TTAed models strongly show the agreement-on-the-line phenomenon (Baek et al., 2022) across a wide range of distribution shifts. We find such linear trends occur consistently in a wide range of models adapted with various hyperparameters, and persist in distributions where the phenomenon fails to hold in vanilla model (i.e., before adaptation). We leverage these observations to make TTA methods more reliable from three perspectives: (i) estimating OOD accuracy (without labeled data) to determine when TTA helps and when it hurts, (ii) calibrating TTAed models again without any labeled data, and (iii) reliably determining hyperparameters for TTA without any labeled validation data. Through extensive experiments, we demonstrate that various TTA methods can be precisely evaluated, both in terms of their improvements and degradations. Moreover, our proposed methods on unsupervised calibration and hyperparameters tuning for TTA achieve results close to the ones assuming access to ground-truth labels, in both OOD accuracy and calibration error.
## [Scaling Laws for Sparsely-Connected Foundation Models](https://openreview.net/forum?id=i9K2ZWkYIP) ------> [PDF](https://openreview.net/pdf/06f71737076495a0b7d14a66035676929158c06f.pdf)
#### Keywords:  sparsity   scaling   optimal sparsity   efficiency   foundational models   transformers   structured sparsity   pruning
  Abs:  We explore the impact of parameter sparsity on the scaling behavior of Transformers trained on massive datasets (i.e., "foundation models"), in both vision and language domains. In this setting, we identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data, which we validate empirically across model and data scales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the "optimal sparsity", the sparsity level which yields the best performance for a given effective model size and training budget. For a fixed number of non-zero parameters, we identify that the optimal sparsity increases with the amount of data used for training. We also extend our study to different sparsity structures (such as the hardware-friendly n:m pattern) and strategies (such as starting from a pretrained dense model). Our findings shed light on the power and limitations of weight sparsity across various parameter and computational settings, offering both theoretical understanding and practical implications for leveraging sparsity towards computational efficiency improvements.
## [NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference](https://openreview.net/forum?id=hlj6HiGJeB) ------> [PDF](https://openreview.net/pdf/77bacb4d855fba21c31716210b5d2248e699d6bb.pdf)
#### Keywords:  Neural Network   Linear Matrix Operation   Efficient Inference
  Abs:  The inherent diversity of computation types within individual deep neural network (DNN) models necessitates a corresponding variety of computation units within hardware processors, leading to a significant constraint on computation efficiency during neural network execution. In this study, we introduce NeuralMatrix, a framework that transforms the computation of entire DNNs into linear matrix operations, effectively enabling their execution with one general-purpose matrix multiplication (GEMM) accelerator. By surmounting the constraints posed by the diverse computation types required by individual network models, this approach provides both generality, allowing a wide range of DNN models to be executed using a single GEMM accelerator and application-specific acceleration levels without extra special function units, which are validated through main stream DNNs and their variant models.
## [Bi-directional Deformation for Parameterization of Neural Implicit Surfaces](https://openreview.net/forum?id=hkAzAZXbdE) ------> [PDF](https://openreview.net/pdf/b476bf2f5adbf2b4d3a2e3d2d477b19b376719b5.pdf)
#### Keywords:  neural parameterization   multi-view reconstruction   texture editing   neural implicit surfaces   neural rendering
  Abs:  The growing capabilities of neural rendering have increased the demand for new techniques that enable the intuitive editing of 3D objects, particularly when they are represented as neural implicit surfaces. In this paper, we present a novel neural algorithm to parameterize neural implicit surfaces to simple parametric domains, such as spheres, cubes or polycubes, where 3D radiance field can be represented as a 2D field, thereby facilitating visualization and various editing tasks. Technically, our method computes a bi-directional deformation between 3D objects and their chosen parametric domains, eliminating the need for any prior information. We adopt a forward mapping of points on the zero level set of the 3D object to a parametric domain, followed by a backward mapping through inverse deformation. To ensure the map is bijective, we employ a cycle loss while optimizing the smoothness of both deformations. Additionally, we leverage a Laplacian regularizer to effectively control angle distortion and offer the flexibility to choose from a range of parametric domains for managing area distortion. Designed for compatibility, our framework integrates seamlessly with existing neural rendering pipelines, taking multi-view images as input to reconstruct 3D geometry and compute the corresponding texture map. We also introduce a simple yet effective technique for intrinsic radiance decomposition, facilitating both view-independent material editing and view-dependent shading editing. Our method allows for the immediate rendering of edited textures through volume rendering, without the need for network re-training. Moreover, our approach supports the co-parameterization of multiple objects and enables texture transfer between them. We demonstrate the effectiveness of our method on images of human heads and man-made objects. We will make the source code publicly available.
## [Jorge: Approximate Preconditioning for GPU-Efficient Second-Order Optimization](https://openreview.net/forum?id=hdCDVSPQ7v) ------> [PDF](https://openreview.net/pdf/0b1f1bf98f7e3c440910f83f2ecf7214a425bbed.pdf)
#### Keywords:  second order optimizer   hardware efficiency   approximate preconditioning
  Abs:  Despite their better convergence properties compared to first-order optimizers,
second-order optimizers for deep learning have been less popular due to their
significant computational costs. The primary efficiency bottleneck in such
optimizers is matrix inverse calculations in the preconditioning step, which
are expensive to compute on GPUs.  In this paper, we introduce Jorge, a
second-order optimizer that promises the best of both worlds -- rapid
convergence benefits of second-order methods, and high computational efficiency
typical of first-order methods. We address the primary computational bottleneck
of computing matrix inverses by completely eliminating them using an
approximation of the preconditioner computation. This makes Jorge extremely
efficient on GPUs in terms of wall-clock time. Further, we describe an approach
to determine Jorge's hyperparameters directly from a well-tuned SGD baseline,
thereby significantly minimizing tuning efforts. Our empirical evaluations
demonstrate the distinct advantages of using Jorge, outperforming
state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple
deep learning models, both in terms of sample efficiency and wall-clock time.
## [Continual Learning with Orthogonal Weights and Knowledge Transfer](https://openreview.net/forum?id=hac6DzbMa7) ------> [PDF](https://openreview.net/pdf/b41b6fc5debf570071a58f733b6f2a1021588fcc.pdf)
#### Keywords:  Continual Learning   Catastrophic Forgetting   Knowledge Transfer   Orthogonal Gradients Projection   Weight/Parameter-Level Orthogonality
  Abs:  Orthogonal projection has been shown highly effective at overcoming *catastrophic forgetting* (CF) in continual learning (CL). Existing orthogonal projection methods are *all* based on *orthogonal gradients* (OG) between tasks. However, this paper shows theoretically that OG cannot guarantee CF elimination, which is a major limitation of the existing OG-based CL methods. Our theory further shows that only the *weight/parameter-level orthogonality* between tasks can guarantee CF elimination as the final classification is computed based on the network weights/parameters only. Existing OG-based methods also have two other *inherent limitations*, i.e., *over-consumption of network capacity* and *limiting knowledge transfer* (KT) across tasks. KT is also a core objective of CL. This paper then proposes a novel *weight-level orthogonal projection* method (called STIL), which ensures that each task occupies a weight subspace that is orthogonal to those of the other tasks. The method also addresses the two other limitations of the OG-based methods. Extensive evaluations show that the proposed STIL not only overcomes CF better than baselines, but also, perhaps more importantly, performs KT much better than them.
## [True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu) ------> [PDF](https://openreview.net/pdf/b07f32555a70fe9d2c2e072bd430d9b740e948d9.pdf)
#### Keywords:  Reinforcement Learning   Large Language Models   Parameter-Efficient Fine-Tuning
  Abs:  Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the
joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.
## [The Power of Minimalism in Long Sequence Time-series Forecasting](https://openreview.net/forum?id=hF8jnnexSB) ------> [PDF](https://openreview.net/pdf/0f6c8a9749dbe3de2fb85638fef5704a4ffc3e60.pdf)
#### Keywords:  Long-term time series forecasting   Transformers   Efficiency
  Abs:  Recently, transformer-based models have been widely applied to time series forecasting tasks due to their remarkable capability to capture complex interactions within sequential data. However, as the sequence length expands, Transformer-based models suffer from increased memory consumption, overfitting, and performance deterioration in capturing long-range dependencies. Recently, several studies have shown that MLP-based models can outperform advanced Transformer-based models for long-term time series forecasting (LTSF) tasks. Unfortunately, linear mappings often struggle to capture intricate dependencies when handling multivariate time series. Although modeling each channel independently can alleviate this issue, it will significantly increase the computational cost. To this end, we introduce a set of simple yet effective depthwise convolution models named LTSF-Conv to perform LTSF. Specifically, we apply unique filters to each channel to achieve channel independence, which plays a pivotal role in enhancing overall forecasting performance. Experimental results show that LTSF-Conv models outperform the state-of-the-art Transformer-based and MLP-based models across seven real-world LTSF benchmarks. Surprisingly, a two-layer non-stacked network can outperform the state-of-the-art Transformer model in 91\% of cases with a significant reduction of computing resources. In particular, LTSF-Conv models substantially decrease the average number of trainable parameters (by $\sim$ 12$\times$), maximum memory consumption (by $\sim$ 86$\times$), running time (by $\sim$ 18$\times$), and inference time (by $\sim$ 2$\times$) on the Electricity benchmark.
## [VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs](https://openreview.net/forum?id=h6Tz85BqRI) ------> [PDF](https://openreview.net/pdf/5c5216001a8155eed9887a4dcd954983d0224219.pdf)
#### Keywords:  Graph Knowledge Distillation   Efficient Graph Learning
  Abs:  GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828×, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively.
## [AV-PEA: PARAMETER-EFFICIENT ADAPTER FOR AUDIO-VISUAL MULTIMODAL LEARNING](https://openreview.net/forum?id=gWw0NjTQRg) ------> [PDF](https://openreview.net/pdf/c77ce8eb907851c0efe7716c7a75c88e5a509839.pdf)
#### Keywords:  PARAMETER-EFFICIENT   ADAPTER   AUDIO-VISUAL LEARNING   MULTIMODAL LEARNING
  Abs:  Fine-tuning has emerged as a widely used transfer learning technique for leveraging pre-trained vision transformers in various downstream tasks. However, its success relies on tuning a significant number of trainable parameters, which could lead to significant costs in terms of both model training and storage. When it comes to audio-visual multimodal learning, the challenge also lies in effectively incorporating both audio and visual cues into the transfer learning process, especially when the original model has been trained with unimodal samples only. This paper introduces a novel audio-visual parameter-efficient adapter (AV-PEA) designed to improve multimodal transfer learning for audio-visual tasks. Through the integration of AV-PEA into a frozen vision transformer, like ViT (Dosovitskiy et al., 2021), the transformer becomes adept at processing audio inputs without prior knowledge of audio pre-training. This also facilitates the exchange of essential audio-visual cues between audio and visual modalities, all while introducing a limited set of trainable parameters into each block of the frozen transformer. The experimental results demonstrate that our AV-PEA consistently achieves superior or comparable performance to state-of-the-art methods in a range of audio-visual tasks, including audio-visual event localization (AVEL), audio-visual question answering (AVQA), audio-visual retrieval (AVR), and audio-visual captioning (AVC). Furthermore, it distinguishes itself from competitors by enabling seamless integration into these tasks while maintaining a consistent number of trainable parameters, typically accounting for less than 3.7% of the total parameters per task.
## [On the Effectiveness of One-Shot Federated Ensembles in Heterogeneous Cross-Silo Settings](https://openreview.net/forum?id=gTWaUlxxWi) ------> [PDF](https://openreview.net/pdf/07a18ef86783c14c2cc99480bd20ac6230b6ab39.pdf)
#### Keywords:  Federated Learning   One-Shot   Communication Efficiency   Ensembles
  Abs:  FL is a popular approach for training machine learning models on decentralized data. For communication efficiency, one-shot FL trades the iterative exchange of models between clients and the FL server for one single round of communication. However, one-shot FL does not perform as well as iterative FL, and struggles under high data heterogeneity. While ensembles have repeatedly appeared as strong contenders in one-shot FL literature, their full potential is still under-explored. In this work, we extensively examine federated ensembles across the heterogeneity spectrum, in conjunction with various aggregation functions from the ensemble literature, with a specific focus on cross-silo settings. Our experiments reveal that an aggregator based on a shallow neural network can significantly boost the performance of ensembles under high data heterogeneity. Through comprehensive evaluations on the CIFAR-10, SVHN and the cross-silo healthcare FLamby benchmark, we show that federated ensembles not only achieve up to 26% higher accuracy over current one-shot methods but can also match the performance of iterative FL under high data heterogeneity, all while being up to 9.1x more efficient in terms of communication due to their one-shot nature.
## [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://openreview.net/forum?id=gPKTTAfYBp) ------> [PDF](https://openreview.net/pdf/3e965a276675994532ba8f3c21c9a00ae7dfdcb9.pdf)
#### Keywords:  convolutions   GPUs   hardware-efficient algorithms   long context   fast fourier transform   I/O awareness
  Abs:  Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time.
A major bottleneck is the Fast Fourier Transform (FFT)---which allows long convolutions to run in $O(N\log N)$ time in sequence length $N$ but has poor hardware utilization.
In this paper, we study how to optimize the FFT convolution.
We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.
In response, we propose FlashFFTConv.
FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.
We also present two sparse convolution algorithms---1) partial convolutions and 2) frequency-sparse convolutions---which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.
FlashFFTConv speeds up exact FFT convolutions by up to 8.7$\times$ over PyTorch and achieves up to 4.4$\times$ speedup end-to-end.
Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity and M2-BERT-base to achieve 3.3 points higher GLUE score---matching models with twice the parameter count.
FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%.
Furthermore, partial convolutions enable longer-sequence models---yielding the first DNA model that can process the longest human genes (2.3M base pairs)---and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.
## [TaCA: Hot-Plugging Upgrades for Foundation Model with Task-agnostic Compatible Adapter](https://openreview.net/forum?id=gMGUa8C0tL) ------> [PDF](https://openreview.net/pdf/4b4ed513ffba9a30e256fa40c554a645cb93735c.pdf)
#### Keywords:  Visual Foundation Model   Compatible Representation Learning   Parameter-Efficient Transfer Learning
  Abs:  Visual foundation models, such as CLIP, exhibit exceptional proficiency in learning feature representations from extensive datasets via self-supervised techniques, showcasing noteworthy aptitude for transfer learning and generalization. A growing number of applications based on visual foundation models are emerging, including innovative solutions such as BLIP-2. These applications employ pre-trained CLIP models as upstream feature extractors and train various downstream modules to accomplish diverse tasks. However, scenarios necessitating system upgrades that entail updating the foundational model pose challenges, as they entail the inefficient and inflexible process of retraining all downstream modules to align with the new foundational model. In this paper, we propose an innovative and valuable task, Hot-Plugging Upgrades for visual foundation models. The aim is to seamlessly integrate superior-performing foundation models into downstream applications without adjusting the downstream modules. To realize this objective, we introduce a parameter-efficient and task-agnostic Compatible Adapter, referred to as TaCA, which promotes compatibility across distinct foundation models while concurrently enhancing performance for the new models. We conduct extensive experimental validation of TaCA using different scales of models with up to one billion parameters on various tasks such as video-text retrieval, video recognition, and visual question answering. The results consistently affirm the efficacy of TaCA in facilitating hot-plugging upgrades for visual foundation models. Codes and models will be made available.
## [Efficient Backpropagation with Variance Controlled Adaptive Sampling](https://openreview.net/forum?id=gEwKAZZmSw) ------> [PDF](https://openreview.net/pdf/f54acd37aa5d8ac7ad80c87d4209a32229e8ed2e.pdf)
#### Keywords:  efficient training algorithms   stochastic gradient descent   importance sampling   variance reduction
  Abs:  Sampling-based algorithms, which eliminate "unimportant" computations during forward and/or backpropagation (BP), offer potential solutions to accelerate neural network training. However, since sampling introduces approximations to training, such algorithms may not consistently maintain accuracy across various tasks. In this work, we introduce a variance-controlled adaptive sampling (VCAS) method designed to minimize the computational load of BP. VCAS computes an unbiased stochastic gradient with fine-grained layerwise importance sampling in data dimension for activation gradient calculation and leverage score sampling in token dimension for weight gradient calculation. To preserve accuracy, we control the additional variance introduced by learning the sample ratio jointly with model parameters during training. We assessed VCAS on multiple fine-tuning and pre-training tasks in both vision and natural language domains. On all the tasks, VCAS can preserve the original training loss trajectory and validation accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction of the whole training process.
## [WATT FOR WHAT: RETHINKING DEEP LEARNING’S ENERGY-PERFORMANCE RELATIONSHIP](https://openreview.net/forum?id=gEhGC7jaTf) ------> [PDF](https://openreview.net/pdf/ed85c9c73ac74f6c2832ee845862f3d95619e5f7.pdf)
#### Keywords:  Energy tracking   Efficient learning   Metric   Power Consumption
  Abs:  Deep learning models have revolutionized various fields, from image recognition to natural language processing, by achieving unprecedented levels of accuracy. However, their increasing energy consumption has raised concerns about their environmental impact, disadvantaging smaller entities in research and exacerbating global energy consumption. In this paper, we explore the trade-off between model accuracy and electricity consumption, proposing a metric that penalizes large consumption of electricity. We conduct a comprehensive study on the electricity consumption of various deep learning models across different GPUs, presenting a detailed analysis of their accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity consumed, we demonstrate how smaller, more energy-efficient models can significantly expedite research while mitigating environmental concerns. Our results highlight the potential for a more sustainable approach to deep learning, emphasizing the importance of optimizing models for efficiency. This research also contributes to a more equitable research landscape, where smaller entities can compete effectively with larger counterparts. This advocates for the adoption of efficient deep learning practices to reduce electricity consumption, safeguarding the environment for future generations whilst also helping ensure a fairer competitive landscape.
## [On the Parameterization of Second-Order Optimization Effective towards the Infinite Width](https://openreview.net/forum?id=g8sGBSQjYk) ------> [PDF](https://openreview.net/pdf/7d207b1a4402615c00f33c1bd0140a6d2761a0ef.pdf)
#### Keywords:  Deep learning   Second-order optimization   K-FAC   Feature learning   Infinite width   Maximum update parameterization
  Abs:  Second-order optimization has been developed to accelerate the training of deep neural networks and it is being applied to increasingly larger-scale models. In this study, towards training on further larger scales, we identify a specific parameterization for second-order optimization that promotes feature learning in a stable manner even if the network width increases significantly. Inspired by a maximal update parametrization, we consider a one-step update of the gradient and reveal the appropriate scales of hyperparameters including random initialization, learning rates, and damping terms. Our approach covers two major second-order optimization algorithms, K-FAC and Shampoo, and we demonstrate that our parametrization achieves higher generalization performance in feature learning.
In particular, it enables us to transfer the hyperparameters across models with different widths.
## [Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation](https://openreview.net/forum?id=fq1wNrC2ai) ------> [PDF](https://openreview.net/pdf/9212e19a375fa07feb7895ab823d25d70b7654db.pdf)
#### Keywords:  Reinforcement learning theory   General function approximation   Infinite-horizon Average-reward MDPs   Sample efficiency
  Abs:  We study infinite-horizon average-reward Markov decision processes (AMDPs) in the context of general function approximation. Specifically, we propose a novel algorithmic framework named Fixed-Point Local Optimization (FLOP), which incorporates both model-based and value-based incarnations. In particular, FLOP features a novel construction of confidence sets and a low-switching policy updating scheme, which are tailored to the average-reward and function approximation setting. Moreover, for AMDPs, we propose a novel complexity measure --- average-reward generalized eluder coefficient (AGEC) --- which captures the challenge of exploration in AMDPs with general function approximation. Such a complexity measure encompasses almost all previously known tractable AMDP models, such as linear AMDPs and linear mixture AMDPs, and also includes newly identified cases such as kernel AMDPs and AMDPs with low Bellman eluder dimensions. Using AGEC, we prove that FLOP achieves a sublinear  $\tilde{\mathcal{O}}(\mathrm{poly}(d, \mathrm{sp}(v^*)) \sqrt{T \beta })$ regret, where $d$  and  $\beta$ correspond to  AGEC and the log-covering number of the hypothesis class respectively,  $\mathrm{sp}(v^*)$ represents the span of the optimal state bias function, $T$ denotes the number of steps, and $\tilde{\mathcal{O}} (\cdot) $ omits logarithmic factors.  When specialized to concrete AMDP models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases.  To the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all AMDPs.
## [Efficient Dynamics Modeling in Interactive Environments with Koopman Theory](https://openreview.net/forum?id=fkrYDQaHOJ) ------> [PDF](https://openreview.net/pdf/db7dad24a08d538b4331e65c59c0a21036453924.pdf)
#### Keywords:  Koopman Theory   Reinforcement Learning   Dynamical System   Planning   Longe range dynamics prediction models   Efficient forward dynamics
  Abs:  The accurate modeling of dynamics in interactive environments is critical for successful long-range prediction. Such a capability could advance Reinforcement Learning (RL) and Planning algorithms, but achieving it is challenging. Inaccuracies in model estimates can compound, resulting in increased errors over long horizons.
We approach this problem from the lens of Koopman theory, where the nonlinear dynamics of the environment can be linearized in a high-dimensional latent space. This allows us to efficiently parallelize the sequential problem of long-range prediction using convolution while accounting for the agent's action at every time step.
Our approach also enables stability analysis and better control over gradients through time. Taken together, these advantages result in significant improvement over the existing approaches, both in the efficiency and the accuracy of modeling dynamics over extended horizons. We also show that this model can be easily incorporated into dynamics modeling for model-based planning and model-free RL and report promising experimental results.
## [Unnormalized Density Estimation with Root Sobolev Norm Regularization](https://openreview.net/forum?id=fapHf9fmqp) ------> [PDF](https://openreview.net/pdf/8b5106c82089ab4e0c3c59161522e3a26a963cd0.pdf)
#### Keywords:  Density estimation   Sobolev norm regularization   Score based methods   Fisher divergence for hyperparameter tuning   Anomaly detection   High dimensional data   Kernel Density Estimation (KDE)
  Abs:  We propose a new approach to non-parametric density estimation that is based on regularizing a Sobolev norm of the density. This method is consistent, different from Kernel Density Estimation, and makes the inductive bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 algorithms.
## [SMOOT: Saliency Guided Mask Optimized Online Training](https://openreview.net/forum?id=fMqP9Ohm7M) ------> [PDF](https://openreview.net/pdf/edd576865d5c8fa8ffa4458a2709b651b1904a86.pdf)
#### Keywords:  Saliency   Modified gradients   Interpretability   Hyperparameter tuning   Information loss
  Abs:  Deep Neural Networks are powerful tools for understanding complex patterns and making decisions. However, their black-box nature impedes a complete understanding of their inner workings. Saliency-Guided Training (SGT) methods try to highlight the prominent features in the model's training based on the output to alleviate this problem. These methods use back-propagation and modified gradients to guide the model toward the most relevant features while keeping the impact on the prediction accuracy negligible. SGT makes the model's final result more interpretable by masking input partially. In this way, considering the model's output, we can infer how each segment of the input affects the output. In the particular case of image as the input, masking is applied to the input pixels. However, the masking strategy and number of pixels which we mask, are considered as a hyperparameter. Appropriate setting of masking strategy can directly affect the model's training. In this paper, we focus on this issue and present our contribution. We propose a novel method to determine the optimal number of masked images based on input, accuracy, and model loss during the training. The strategy prevents information loss which leads to better accuracy values. Also, by integrating the model's performance in the strategy formula, we show that our model represents the salient features more meaningful. Our experimental results demonstrate a substantial improvement in both model accuracy and the prominence of saliency, thereby affirming the effectiveness of our proposed solution.
## [Convergence of Bayesian Bilevel Optimization](https://openreview.net/forum?id=fLXpXa7iiz) ------> [PDF](https://openreview.net/pdf/5f2961831645f7d9ff4ae26a442590ea99984b67.pdf)
#### Keywords:  Hyperparameter optimization   Bayesian optimization   Convergence rate   Bilevel optimization   Learning theory
  Abs:  This paper presents the first theoretical guarantee for Bayesian bilevel optimization (BBO) that we term for the prevalent bilevel framework combining Bayesian optimization at the outer level to tune hyperparameters, including the inner-level stochastic gradient descent (SGD). We prove sublinear regret bounds suggesting simultaneous convergence of the inner-level model parameters and outer-level hyperparameters to optimal configurations for generalization capability. A pivotal, technical novelty in the proofs is modeling the excess risk of the SGD-trained parameters as evaluation noise during Bayesian optimization. Our theory implies the inner unit horizon, defined as the number of SGD iterations, shapes the convergence behavior of BBO. This suggests practical guidance on configuring the inner unit horizon to enhance training efficiency and model performance.
## [Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation](https://openreview.net/forum?id=fDZumshwym) ------> [PDF](https://openreview.net/pdf/54d23069816aef07db5c423f624fa1c6be538719.pdf)
#### Keywords:  Data condensation; Data distillation; Data parameterization
  Abs:  Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods.
To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level features. Another helpful property of the hierarchical architecture is that HMN naturally ensures good independence among images despite achieving information sharing. This enables instance-level pruning for HMN to reduce redundant information, thereby further minimizing redundancy and enhancing performance. We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results show that our proposed method outperforms all baselines, even when trained with a batch-based loss consuming less GPU memory.
## [ZEST: ZEROSHOT SPARSE FINE-TUNING](https://openreview.net/forum?id=f7t7AOseAa) ------> [PDF](https://openreview.net/pdf/da903458301c99c46c34d34ece4e36362ed1dbb4.pdf)
#### Keywords:  deep learning; surgical fine-tuning; efficient fine-tuning
  Abs:  Recent studies have pointed out that fine-tuning a subset of layers from the model can match or even outperform the performance of full fine-tuning, known as surgical fine-tuning ~\citep{lee2022surgical}. This method effectively helps reduce the risks of overfitting and accelerates the fine-tuning process. However, swiftly and accurately identifying the "right" layers is not straightforward. Existing approaches naively train each layer until convergence and find the best candidates, which is not scalable, especially given the rapid growth in model sizes.
In this paper, we propose $\textbf{ZEST}$: $\textbf{Ze}$roshot $\textbf{S}$parse fine-$\textbf{T}$uning. We first study and compare the zero-shot metrics acquired from a single forward and backward pass. We observe that the metrics are inconsistent for different model and dataset combinations, thus we train a universal \method predictor to generalize this method. We use the zero-shot \method predictor to rank layers by the estimated importance and fine-tune only the important parameters.
By doing so, we can decrease the number of trainable parameters by up to 99\%, being on par or outperforming full fine-tuning in terms of model performance. We thoroughly evaluate the effectiveness of \method on various tasks and modalities. We train a universal predictor for ResNet50, MobilenetV2, and EfficientNet on 8 different datasets. We also scale this method up to BERT and LLAMA. Our results demonstrate that fine-tuning just five layers can closely match or even outperform the performance achieved through full fine-tuning on LLaMA-7B. Specifically, fine-tuning only the \textbf{5} fully connected layers on LLaMA chosen by \method can result in improvements of up to 5\% over full fine-tuning
## [UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling](https://openreview.net/forum?id=f5H8WGLQm5) ------> [PDF](https://openreview.net/pdf/b0299eb9e947ef318ff3ce6a20287eb8cb03c624.pdf)
#### Keywords:  parameter-efficient transfer learning   cross-modal modeling
  Abs:  Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 7 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, VQA and Caption) show that in most cases, UniAdapter not only outperforms the state-of-the-arts, but even beats the full fine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with 2.2% model parameters, outperforming the latest competitors by 2.0%. The code and models are available at https://github.com/UniAdapter/UniAdapter.
## [One-stage Prompt-based Continual Learning](https://openreview.net/forum?id=f3TQxxuquZ) ------> [PDF](https://openreview.net/pdf/a1301cc5dd00d76f5762f651df5d655e119157f7.pdf)
#### Keywords:  Efficient learning   Continual learning   Transfer learning
  Abs:  Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in $\sim 50\%$ computational cost reduction for both training and inference with marginal accuracy drop ($\le 1\%$). We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, so there is no computational overhead at inference from the QR loss. With the QR loss, our approach maintains $\sim 50\%$ computational cost reduction during inference as well as outperforms the prior two-stage PCL methods by $\sim 1.4\%$ on public class-incremental continual learning benchmarks including CIFAR-100 and ImageNet-R.
## [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://openreview.net/forum?id=ezscMer8L0) ------> [PDF](https://openreview.net/pdf/1e44a630ba18af9cffbef1fce66999f3d615657d.pdf)
#### Keywords:  Parameter-efficient fine-tuning   Segment Anything Model   LoRA   Semantic Segmentation
  Abs:  The Segment-Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into LoRA, Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM’s local prior assumption. Notably, Conv-LoRA not only preserves SAM’s extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM’s foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA’s superiority in adapting SAM to real-world semantic segmentation tasks.
## [Efficient Low-Rank Diffusion Model Training for Text-to-Image Generation](https://openreview.net/forum?id=edx7LTufJF) ------> [PDF](https://openreview.net/pdf/1d04ffdc07819111deed277edba4a6d19d9a7970.pdf)
#### Keywords:  Efficiency   Diffusion
  Abs:  Recent advancements in text-to-image generation models have witnessed the success of large-scale diffusion-based generative models. However, exerting control over these models, particularly for structure-conditioned text-to-image generation, remains an open challenge. One straightforward way to achieve control is via fine-tuning, often coming at the cost of efficiency. In this work, we address this challenge by introducing ELR-Diffusion (Efficient Low-rank Diffusion), a method tailored for efficient structure-conditioned image generation. Our innovative approach leverages the low-rank decomposition of model weights, leading to a dramatic reduction in memory cost and model parameters — by up to 58\%, at the same time performing comparably to larger models trained with expansive datasets and more computational resources. At the heart of ELR-Diffusion lies a two-stage training scheme that resorts to the low-rank decomposition and knowledge distillation strategy. To provide a robust assessment of our model, we undertake a thorough comparative analysis in the controllable text-to-image generation domain. We employ a diverse array of evaluation metrics with various conditions, including edge maps, segmentation maps, and image quality measures, offering a holistic view of the model's capabilities. We believe that ELR-Diffusion has the potential to serve as an efficient foundation model for diverse user applications that demand accurate comprehension of inputs containing multiple conditional information.
## [SemSA: Semantic Sparse Attention is hidden in Large Language Models.](https://openreview.net/forum?id=eG9AkHtYYH) ------> [PDF](https://openreview.net/pdf/5722bb5d937e145ac009bdb8e0f400f88a94a19b.pdf)
#### Keywords:  large language model   sparse attention   efficient transformer
  Abs:  Sparse attention is one of the most effective approaches for addressing the $O(N^2)$ attention complexity of transformer models.
Existing methods manually designs a uniform sparse attention mask for all attention heads.
However, uniform masks treat different attention heads equally. To preserve necessary attentions for important heads, the masks are unnecessarily dense for unimportant heads, limiting the overall sparsity and wall-clock speedup. 
Thus, we propose Semantic Sparse Attention (SemSA) paradigm. It uses statistical information to evaluate, generate and optimize different sparse attention masks for different heads.
We observe that the acquired attention masks successfully learn different semantic information from the dense pre-trained large language models: some heads focus on contents while others mainly encode the token positions.
We optimize SemSA GPU operators and evaluate it on popular large language models OPT-6.7B (2k tokens) and Llama2-7B (4k tokens). Compared with dense PyTorch models, SemSA achieves $4.18\sim11.67\times$ and $1.36\sim2.34\times$ speedup for attention layer and first-token-latency with negligible accuracy loss. Compared with other sparse attention methods optimized with state-of-the-art sparse framework, SemSA achieves up to $1.6\times$ sparsity, $1.4\times$ attention speedup with higher accuracy.
## [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://openreview.net/forum?id=eFWG9Cy3WK) ------> [PDF](https://openreview.net/pdf/c67eb127b71afb6fbf439cc022f58120524a9594.pdf)
#### Keywords:  Sparse Mixture-of-Experts   Efficiency   Merging   Compression
  Abs:  Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like: ($a$) $\textit{High Memory Usage,}$ due to duplication of the network layers into multiple copies as experts; and ($b$) $\textit{Redundancy in Experts,}$ as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: ($1$) redundant information overshadows critical experts; ($2$) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address these challenges, we propose a novel merging algorithm for SMoE, $\textit{i.e.}$, $\texttt{M-SMoE}$, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their "group members" are formed based on routing policies; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we draw an interesting observation that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, $\texttt{MC-SMoE}$ ($\textit{i.e.}$, Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across $8$ benchmarks validate the effectiveness of our proposals. For instance, our $\texttt{MC-SMoE}$ achieves up to $80\%$ memory and a $20\%$ FLOPs reduction, with virtually no loss in performance. Our code is provided as supplementary material.
## [VRAda: A Variance Reduced Adaptive Algorithm for Stochastic Parameter-Agnostic Minimax Optimizations](https://openreview.net/forum?id=e5hZmQXHHg) ------> [PDF](https://openreview.net/pdf/b834321fab5b1a2ead0eadff67ed36eddbf08b35.pdf)
#### Keywords:  Stochastic minimax optimization   Parameter-agnostic   Variance-reduction
  Abs:  Stochastic parameter-agnostic minimax optimization provides a novel avenue for adjusting learning rates without relying on problem-dependent parameters, bridging the gap between theoretical and empirical machine learning results. While previous studies have successfully decoupled the timescales of primal and dual variables and proposed unified parameter-agnostic algorithms for minimax optimizations, the problem of varying inherent variances within the stochastic setting persists. Such variance degradation affects the desired ratio of learning rates. Intuitively, variance-reduced techniques hold the potential to address this issue efficiently. However, they require manually tuning problem-dependent parameters to attain an optimal solution. In this paper, we introduce the Variance-Reduced Adaptive algorithm (VRAda), a solution addressing varying inherent variances and enabling the parameter-agnostic manner in stochastic minimax optimizations. Theoretical results show that VRAda achieves an optimal sample complexity of $O(1/\epsilon^3)$ without large data batches, enabling it to find an $\epsilon$-stationary point on non-convex-strongly-concave and non-convex-Polyak-\L ojasiewicz objectives. To the best of our knowledge, VRAda is the first variance-reduced adaptive algorithm designed specifically for parameter-agnostic minimax optimization. Extensive experiments conducted across diverse applications validate the effectiveness of VRAda.
## [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://openreview.net/forum?id=dIjwC8A0N6) ------> [PDF](https://openreview.net/pdf/be172fa96666ba98aca2255900c6abdff5fe6b4a.pdf)
#### Keywords:  Quantization   Full-parameter Tuning   LLMs   Instructional Fine-tuning   Efficient Training
  Abs:  Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pre-trained models on downstream datasets provides further significant performance gains, but this process has been challenging due to its extraordinary resource requirements. To this end, existing efforts focus on parameter-efficient fine-tuning, which, unfortunately, fail to capitalize on the powerful potential of full-parameter fine-tuning. In this work, we propose QFT, a novel quantized full-parameter tuning framework for LLMs that enables memory-efficient fine-tuning without harming performance. Our framework incorporates two novel ideas: (i) we adopt the efficient Lion optimizer, which eliminates the memory usage of variances and enjoys the inherent advantage of performing robust quantization; and (ii) we quantize all model states and store them as integer values, and present a gradient backpropagation and parameter update scheme of the quantized values. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model only requires <30GB memory, met by a single A6000 GPU.
## [Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces](https://openreview.net/forum?id=coIaBY8EVF) ------> [PDF](https://openreview.net/pdf/d2ca257f2201620a1ff4b10d8587c2cab75f9696.pdf)
#### Keywords:  congestion   decongestion   online marketplaces   learning in economic settings   efficient allocation
  Abs:  Congestion is a common failure mode of markets, where consumers compete inefficiently on the same subset of goods (e.g.,  chasing the same small set of properties on a vacation rental platform). The typical economic story is that prices  decongest by balancing supply and demand. But in modern online marketplaces, prices are typically set in a decentralized way by sellers, and the information about items is inevitably partial. The power of a platform  is limited to controlling *representations*---the subset of information about items presented by default to users. This motivates the present study of *decongestion by representation*, where a platform seeks to learn representations that reduce  congestion and thus improve social welfare. The technical challenge is twofold: relying only on revealed preferences from the choices of consumers, rather than true preferences; and the combinatorial problem associated with representations that  determine the  features to reveal in the default view.  We tackle both challenges by proposing a *differentiable proxy of welfare* that can be trained end-to-end on consumer choice data. We develop sufficient conditions for when decongestion promotes welfare, and present the results of extensive experiments on both synthetic and real data that demonstrate the utility of our approach.
## [ROSA: Random Orthogonal Subspace Adaptation](https://openreview.net/forum?id=cgCKm5DOnu) ------> [PDF](https://openreview.net/pdf/b598ea550f7499dd3900bbfd13848927605f44fb.pdf)
#### Keywords:  Machine Learning   Deep Learning   language model   parameter efficient fine-tuning
  Abs:  Model training requires significantly more memory, compared with inference.  Parameter efficient fine-tuning (PEFT) methods provide a means of adapting large models to downstream tasks using less memory. However, existing methods such as adapters, prompt tuning or low-rank adaptation (LoRA) either introduce latency overhead at inference time or achieve subpar downstream performance compared with full fine-tuning. In this work we propose Random Orthogonal Subspace Adapter~(ROSA), a method that outperforms previous PEFT methods by a significant margin, while maintaining a zero latency overhead during inference time. In contrast to previous methods, ROSA is able to adapt subspaces of arbitrarily large dimension. We demonstrate both theoretically and experimentally that this makes ROSA strictly more expressive than LoRA, without consuming additional memory during runtime.  As PEFT methods are especially useful in the natural language processing domain, where models operate on scales that make full fine-tuning very expensive, we evaluate ROSA in two common NLP scenarios: natural language generation (NLG) and natural language understanding (NLU) with GPT-2 and RoBERTa, respectively. We show that on almost every GLUE task ROSA outperforms LoRA by a significant margin, while also outperforming LoRA on NLG tasks.Our code will be made publicly available on acceptance.
## [Local Expert Diffusion Models for Efficient Training in Denoising Diffusion Probabilistic Models](https://openreview.net/forum?id=cdIyf38mRw) ------> [PDF](https://openreview.net/pdf/dd0907c7eb51ed29e0b2b1f2fa92a7785b766e6e.pdf)
#### Keywords:  deep learning   generative models   diffusion models   resource efficient training
  Abs:  Diffusion models have emerged as a new standard technique in generative AI due to their huge success in various applications. However, their training can be prohibitively time-consuming, posing challenges for small businesses or academic studies. To address this issue, we propose a novel and practical training strategy that significantly reduces the training time, even enhancing generation quality. We observe that diffusion models exhibit different convergence rates and training patterns at different time steps, inspiring our MDM (Multi-expert Diffusion Model). Each expert specializes in a group of time steps with similar training patterns. We can exploit the variations in iteration required for convergence among different local experts to reduce total training time significantly. Our method improves the training efficiency of the diffusion model by (1) reducing the total GPU hours and (2) enabling parallel training of experts without overhead to further reduce the wall-clock time. When applied to three baseline models, our MDM accelerates training x2.7 - 4.7 faster than the corresponding baselines while reducing computational resources by 24 - 53%. Furthermore, our method improves FID by 7.7% on average, including all datasets and models.
## [Free Lunches in Auxiliary Learning: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost](https://openreview.net/forum?id=cINwAhrgLf) ------> [PDF](https://openreview.net/pdf/301a6606eaa653315b876f205e1828f6390d317d.pdf)
#### Keywords:  Auxiliary Learning; Neural Architecture Search; Soft Parameter Sharing; Multi-Task Learning; Single Task Inference Cost
  Abs:  We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference. Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence. These connections can be removed during the primary task inference, resulting in a single task inference cost. We achieve this by formulating a Neural Architecture Search (NAS) problem, where we initialize bi-directional connections in the search space and guide the NAS optimization converging to an architecture with only the single-side primary-to-auxiliary connections. Moreover, our method can be incorporated with existing optimization-based auxiliary learning approaches. Extensive experiments with 6 tasks on NYU v2, CityScapes, and Taskonomy datasets using VGG-16, ResNet-50, and ViTBase backbones validate the promising performance. The codes will be released.
## [GRADSIMCORE: GRADIENT SIMILARITY BASED REPRESENTATIVE INSTANCES AS CORESET](https://openreview.net/forum?id=cHy00K3Och) ------> [PDF](https://openreview.net/pdf/50a91f5bc9334543d5375784371d8400c384c4a1.pdf)
#### Keywords:  Coreset selection   Data-efficient deep learning   gradient similarity
  Abs:  The rise in size and complexity of modern datasets and deep learning models have resulted in the usage of extensive computational resources and a rise in training time and effort. It also has increased the carbon footprint of training and fine-tuning models. One way to reduce the computational requirement is to extract the most representative subset (referred to as $\textit{coreset}$) that can substitute for the larger dataset. Coresets can thus replace huge datasets to train models and tune hyperparameters, especially in the early stages of training. This will result in a significant reduction of computational resource requirement and reduce carbon footprint. We propose a simple and novel framework based on the similarity of loss gradients for identifying the representative training instances as a coreset. Our method, dubbed as $\textit{GradSimCore}$, outperforms the state-of-the-art coreset selection algorithms on popular benchmark datasets ranging from MNIST to ImageNet. Because of its simplicity and effectiveness, our method is an important baseline for evaluating the effectiveness of the coreset selection algorithms. Anonymized codes for the proposed baseline are provided at https://anonymous.4open.science/r/GradSimCore-8884
## [FedSRC: Federated Learning with Self-Regulating Clients](https://openreview.net/forum?id=cB9bAFGFAA) ------> [PDF](https://openreview.net/pdf/e634ffb1e816e509510d5ada59987223acd68dd6.pdf)
#### Keywords:  Federated learning   efficiency   computation and communication savings   client-side control
  Abs:  Federated Learning (FL) has emerged as a prominent privacy-preserving decentralized paradigm for collaborative machine learning across many devices.
However, FL suffers from performance degradation in the global model due to heterogeneity in clients' locally generated data. Some prior studies address this issue by limiting or even discarding certain clients' contributions to the global model, resulting in unnecessary computation and communication for the discarded clients. Alternatively, selectively choosing clients to participate in FL may avoid such resource waste. However, such active client selection requires client-level profiling that violates privacy.
In this paper, we present a novel FL approach, called FedSRC: Federated Learning with Self-Regulating Clients, that can save clients' resources while preserving their anonymity. In FedSRC, clients can determine if their local training is favorable to the global model and whether they should participate in an FL round using a lightweight checkpoint based on their test loss on the global model.
Through comprehensive evaluations using four datasets, we show that FedSRC can improve global model performance, all the while reducing communication costs by up to 30\% and computation costs by 55\%.
## [Denoising Diffusion Step-aware Models](https://openreview.net/forum?id=c43FGk8Pcg) ------> [PDF](https://openreview.net/pdf/7ebfcfd38d43d31b3619a6fd41375c9047b32f50.pdf)
#### Keywords:  diffusion models   efficiency   network pruning
  Abs:  Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for data generation across various domains. However, a significant bottleneck is the necessity for whole-network computation during every step of the generative process, leading to high computational overheads. This paper presents a novel framework, Denoising Diffusion Step-aware Models (DDSM), to address this challenge. Unlike conventional approaches, DDSM employs a spectrum of neural networks whose sizes are adapted according to the importance of each generative step, as determined through evolutionary search. This step-wise network variation effectively circumvents redundant computational efforts, particularly in less critical steps, thereby enhancing the efficiency of the diffusion model. Furthermore, the step-aware design can be seamlessly integrated with other efficiency-geared diffusion models such as DDIMs and latent diffusion, thus broadening the scope of computational savings. Empirical evaluations demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61% for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all without compromising the generation quality. Our code and models will be publicly available.
## [SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference](https://openreview.net/forum?id=bcHty5VvkQ) ------> [PDF](https://openreview.net/pdf/acd8412a837ea12aeca6af284f81c12dac526a82.pdf)
#### Keywords:  Adaptive Computation   Efficient Inference   Generative Models
  Abs:  Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, the high computing cost and latency resulting from token-by-token generation impede their widespread adoption. To address this issue, several approaches have been proposed that reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference – they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prior constraints by setting up a singular exit point for every token in a batch at a each sequence position. It also guarantees a monotonic decrease in exit points, thereby eliminating the need to recompute KV Caches for preceding tokens. Rather than terminating computation prematurely as in prior works, our approach bypasses lower to middle layers, devoting most of the computational resources to upper layers, allowing later tokens to benefit from the compute expenditure by earlier tokens. Our experimental results show that SkipDecode can obtain 2x to 5x inference speedups with negligible regression across a variety of tasks. This is achieved using OPT models of 1.3 billion and 6.7 billion parameters, all the while being directly compatible with batching and KV caching optimization techniques.
## [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning](https://openreview.net/forum?id=bUgni8nH8Z) ------> [PDF](https://openreview.net/pdf/4f565888a65d0d46346ccb00eb4a31e56f6bfe2a.pdf)
#### Keywords:  deep learning   neural networks   ReLU   parameterization   normalization
  Abs:  This work examines the characteristic activation values of individual ReLU units in neural networks. We refer to the set of input locations corresponding to such characteristic activation values as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into how various neural network normalization techniques used in modern deep learning architectures regularize and stabilize stochastic gradient optimization. Utilizing these insights, we propose geometric parameterization for ReLU networks to improve feature learning, which decouples the radial and angular parameters in the hyperspherical coordinate system. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report significant improvements in optimization stability, convergence speed, and generalization performance for various models on a variety of datasets, including the ResNet-50 network on ImageNet.
## [Linear programming using diagonal linear networks](https://openreview.net/forum?id=bLhqPxRy3G) ------> [PDF](https://openreview.net/pdf/ccbd1af4f0083b82fd7cfaa7d3c25118c004b41a.pdf)
#### Keywords:  Linear program   diagonally linear network   reparameterization   implicit bias
  Abs:  Linear programming has played a crucial role in shaping decision-making, resource allocation, and cost reduction in various domains. In this paper, we investigate the application of overparametrized neural networks and their implicit bias in solving linear programming problems. Specifically, our findings reveal that training diagonal linear networks with gradient descent, while optimizing the squared $L_2$-norm of the slack variable, leads to solutions for entropically regularized linear programming problems. Remarkably, the strength of this regularization depends on the initialization used in the gradient descent process. We analyze the convergence of both discrete-time and continuous-time dynamics and demonstrate that both exhibit a linear rate of convergence, requiring only mild assumptions on the constraint matrix. For the first time, we introduce a comprehensive framework for solving linear programming problems using diagonal neural networks. We underscore the significance of our discoveries by applying them to address challenges in basis pursuit and optimal transport problems.
## [READ: Recurrent Adaptation of Large Transformers](https://openreview.net/forum?id=bC50ZOyPQm) ------> [PDF](https://openreview.net/pdf/d893f7bfafb2eedef07f1c869e09d1a5ff9f4f1c.pdf)
#### Keywords:  nlp   peft   transformers
  Abs:  Fine-tuning large-scale Transformers has led to the explosion of many AI applications across Natural Language Processing and Computer Vision tasks. However, fine-tuning all pre-trained model parameters becomes impractical as the model size and number of tasks increase.
Parameter-efficient transfer learning (PETL) methods aim to address these challenges. While effective in reducing the number of trainable parameters, PETL methods still require significant energy and computational resources to fine-tune. In this paper, we introduce \textbf{RE}current \textbf{AD}aption (READ) --- a lightweight and memory-efficient fine-tuning method --- to overcome the limitations of the current PETL approaches. Specifically, READ inserts a small RNN network alongside the backbone model so that the model does not have to back-propagate through the large backbone network. Through comprehensive empirical evaluation of the GLUE benchmark, we demonstrate READ can achieve a $56\%$ reduction in the training memory consumption and an $84\%$ reduction in the GPU energy usage while retraining high model quality compared to full-tuning. Additionally, the model size of READ does not grow with the backbone model size, making it a highly scalable solution for fine-tuning large Transformers.
## [FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics](https://openreview.net/forum?id=b6LeQUnVIH) ------> [PDF](https://openreview.net/pdf/507e99553f5cba707d00ca05db8e222f0217a99c.pdf)
#### Keywords:  Model Robustness   Training Efficiency   Data Pruning
  Abs:  Despite the massive success of fine-tuning large Pre-trained Language Models (PLMs) on a wide range of Natural Language Processing (NLP) tasks, they remain susceptible to out-of-distribution (OOD) and adversarial inputs. Data map (DM) is a simple yet effective dual-model approach that enhances the robustness of fine-tuned PLMs, which involves fine-tuning a model on the original training set (i.e. reference model), selecting a specified fraction of important training examples according to the training dynamics of the reference model, and fine-tuning the same model on these selected examples (i.e. main model). However, it suffers from the drawback of requiring fine-tuning the same model twice, which is computationally expensive for large models. In this paper, we first show that 1) training dynamics are highly transferable across different model sizes and different pre-training methods, and that 2) main models fine-tuned using DM learn faster than when using conventional Empirical Risk Minimization (ERM). Building on these observations, we propose a novel fine-tuning approach based on the DM method: Fine-Tuning by transFerring Training dynamics (FTFT). Compared with DM, FTFT uses more efficient reference models and then fine-tunes more capable main models for fewer steps. Our experiments show that FTFT achieves better generalization robustness than ERM while spending less than half of the training cost.
## [Efficient Stagewise Pretraining via Progressive Subnetworks](https://openreview.net/forum?id=ZyH5ijgx9C) ------> [PDF](https://openreview.net/pdf/98b6c7fbc6131763d50f9d3e1c43cfeeca3a3566.pdf)
#### Keywords:  Stage-wise training   Efficient pre-training   Implicit benefits on downstream performance
  Abs:  Recent developments in language models have sparked interest in developing efficient pretraining methods. A recent and effective paradigm is to perform stagewise training, where the depth of the model is gradually increased over the course of training starting from a shallow network (e.g. gradual stacking (Reddi et al., 2023)). While this is appealing since it yields resource and wall-time savings, it has limitations, particularly the inability to assess and evaluate the full model performance during earlier stages, and degradation in model quality due to smaller capacity of models in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We empirically focus on a simple instantiation of this framework - Random Path Training (RAPTR) - that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. We demonstrate that RAPTR achieves better pre-training loss for BERT and UL2 language models while requiring 20-33\% fewer FLOPs compared to standard training, and is competitive or better than gradual stacking at similar FLOPs. Furthermore, RAPTR shows better downstream performance on UL2, improving multiple QA and SuperGLUE tasks by 1-5\% compared to standard training and stacking. Finally, we provide theoretical basis of RAPTR for residual networks by characterizing their stability due to residual connections and layer norm.
## [Periodic and Random Sparsity for Multivariate Long-Term Time-Series Forecasting](https://openreview.net/forum?id=ZRzlhfMqHt) ------> [PDF](https://openreview.net/pdf/14c281078d77ef3f6d0bc36d244d7df4af60f76f.pdf)
#### Keywords:  Time series forecasting   Transformers   Efficiency
  Abs:  For years, Transformers have achieved remarkable success in various domains such as language and image processing. Due to their capabilities to capture long-term relationships, they are expected to give potential benefits in multivariate long-term time-series forecasting. Recent works have proposed segment-based Transformers, where each token is represented by a group of consecutive observations rather than a single one. However, the quadratic complexity of self-attention leads to intractable costs under high granularity and large feature size. In response, we propose Efficient Segment-based Sparse Transformer (ESSformer), which incorporates two sparse attention modules tailored for segment-based Transformers. To efficiently capture temporal dependencies, ESSformer utilizes Periodic Attention (PeriA), which learns  interactions between periodically distant segments. Furthermore, inter-feature dependencies are captured via Random-Partition Attention (R-PartA) and ensembling, which leads to additional cost reduction. Our empirical studies on real-world datasets show that ESSformer surpasses the forecasting capabilities of various baselines while reducing the quadratic complexity.
## [Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention](https://openreview.net/forum?id=ZGBOfAQrMl) ------> [PDF](https://openreview.net/pdf/50d9a5757c70e20b8a2b2c3de85840db57e6d597.pdf)
#### Keywords:  video super-resolution   adaptive   memory and computation efficiency
  Abs:  Recently, Vision Transformer has achieved great success in recovering missing details in low-resolution sequences, i.e. the video super-resolution (VSR) task.
Despite its superiority VSR accuracy, the heavy computational burden as well as the large memory footprint hinders the deployment of Transformer-based VSR models on constrained devices, e.g. smart
phones and consumer electronic products. 
In this paper, we address the above issue by proposing a novel feature-level masked processing framework: VSR with Masked Intra and inter frame Attention (MIA-VSR).
The core of MIA-VSR is leveraging feature-level temporal continuity between adjacent frames to reduce redundant computations and make more rational use of previously enhanced SR features.
Concretely, we propose an intra-frame and inter-frame attention block which takes the respective roles of past features and input
features into consideration and only exploits previously enhanced features to provide supplementary information.
In addition, an adaptive block-wise mask predicting module is developed to skip unimportant computations according to feature similarity between adjacent frames.
We conduct detailed ablation studies to validate our contributions and compare the proposed method with recent state-of-the-art VSR approaches.
The experimental results demonstrate that MIA-VSR improves
the memory and computation efficiency over state-of-the-art methods, without trading off PSNR accuracy.
## [One is More: Diverse Perspectives within a Single Network for Efficient DRL](https://openreview.net/forum?id=ZD7g4Y3PxP) ------> [PDF](https://openreview.net/pdf/218f9b3a37defa5e7bffaeda4412414628357ffd.pdf)
#### Keywords:  Deep Reinforcement Learning   Sample Efficiency   Generalization in Reinforcement Learning
  Abs:  Deep reinforcement learning has achieved remarkable performance in various domains by leveraging deep neural networks for approximating value functions and policies. However, using neural networks to approximate value functions or policy functions still faces challenges including low sample efficiency and overfitting. In this paper, we introduce OMNet, a novel learning paradigm utilizing multiple subnetworks within a single network, offering diverse outputs efficiently. We provide a systematic pipeline, including initialization, training, and sampling with OMNet. OMNet can be easily applied to various deep reinforcement learning algorithms with minimal additional overhead. Through comprehensive evaluations conducted on MuJoCo benchmark, our findings highlight OMNet's ability to strike an effective balance between performance and computational cost.
## [RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets](https://openreview.net/forum?id=You77eOFDv) ------> [PDF](https://openreview.net/pdf/e5c344591db629eef64e1f6d36a838977f98879d.pdf)
#### Keywords:  Deep Learning Architectures   Re-parameterization Method   Convolutional Neural Network
  Abs:  We propose Re-parameterized Refocusing Convolution (RefConv) as a replacement for regular convolutional layers, which is a plug-and-play module to improve the performance without any inference costs. Specifically, given a pre-trained model, RefConv applies a trainable Refocusing Transformation to the basis kernels inherited from the pre-trained model to establish connections among the parameters. For example, a depth-wise RefConv can relate the parameters of a specific channel of convolution kernel to the parameters of the other kernel, i.e., make them refocus on the other parts of the model they have never attended to, rather than focus on the input features only. From another perspective, RefConv augments the priors of existing model structures by utilizing the representations encoded in the pre-trained parameters as the priors and refocusing on them to learn novel representations, thus further enhancing the representational capacity of the pre-trained model. Experimental results validated that RefConv can improve multiple CNN-based models by a clear margin on image classification (up to 1.47\% higher top-1 accuracy on ImageNet), object detection and semantic segmentation without introducing any extra inference costs or altering the original model structure. Further studies demonstrated that RefConv can reduce the redundancy of channels and smooth the loss landscape, which explains its effectiveness.
## [The Power of Linear Combinations: Learning with Random Convolutions](https://openreview.net/forum?id=YSTaRLVP2G) ------> [PDF](https://openreview.net/pdf/9d75155dbdad3cbd03883b21f158266a3b6788fb.pdf)
#### Keywords:  Convolution   random parameters
  Abs:  Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example, transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of *learned* convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall performance and robustness. Conversely, retaining the ability to learn filter updates can impair network performance. Finally, although the improvement we see from learning $3\times 3$ convolutions is relatively small, the learning gains increase proportionally with kernel size. We attribute this to the independently and identically distributed (*i.i.d.*) nature of default initialization schemes.
## [Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning](https://openreview.net/forum?id=YR3ETaElNK) ------> [PDF](https://openreview.net/pdf/4a62199adf4a83d614f9bf9f6bc027c8255d2d13.pdf)
#### Keywords:  multi-modality; large language models; generation; model efficiency;
  Abs:  This paper introduces an efficient strategy to transform Large Language Models
(LLMs) into Multi-Modal Large Language Models (MLLMs). By conceptualizing
this transformation as a domain adaptation process, i.e., transitioning from text
understanding to embracing multiple modalities, we intriguingly note that, within
each attention block, tuning LayerNorm suffices to yield strong performance.
Moreover, when benchmarked against other tuning approaches like full parameter
finetuning or LoRA, its benefits on efficiency are substantial. For example, when
compared to LoRA on a 13B model scale, performance can be enhanced by an
average of over 20% across five multi-modal tasks, and meanwhile, results in a
significant reduction of trainable parameters by 41.9% and a decrease in GPU
memory usage by 17.6%. On top of this LayerNorm strategy, we showcase that
selectively tuning only with conversational data can improve efficiency further.
Beyond these empirical outcomes, we provide a comprehensive analysis to explore
the role of LayerNorm in adapting LLMs to the multi-modal domain and improving
the expressive power of the model.
## [Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection](https://openreview.net/forum?id=YNmnGzttMJ) ------> [PDF](https://openreview.net/pdf/9a170c08c85cf013d4147b3e0af5b6fb6d2f3316.pdf)
#### Keywords:  Dataset Distillation   Data Selection   Efficiency
  Abs:  Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information transport. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g., in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of distillation and networks. Our method is able to extend the distillation algorithms to much larger-scale datasets and more heterogeneous datasets, e.g., ImageNet-1K and Kinetics-400. Our code will be made publicly available.
## [Model-based Reinforcement Learning for Parameterized Action Spaces](https://openreview.net/forum?id=YH9tnuUYds) ------> [PDF](https://openreview.net/pdf/185aee555941f101cf3dc8305a38f79bad9b58e5.pdf)
#### Keywords:  Model-based Reinforcement Learning   Parameterized Action Markov Decesion Process   deep reinforcement learning
  Abs:  We propose a novel model-based reinforcement learning algorithm---Dynamics Learning and predictive control with Parameterized Actions (DLPA)---for Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a parameterized-action-conditioned dynamics model and plans with a modified Model Predictive Path Integral control. Our empirical results on several standard benchmarks show that our algorithm achieves superior sample efficiency and asymptotic performance than state-of-the-art PAMDP methods.
## [Differentially Private Bias-Term Fine-tuning of Foundation Models](https://openreview.net/forum?id=YH3tFtwuzb) ------> [PDF](https://openreview.net/pdf/8c3c4a8d5195916ca08e5c79f46678ed44a059e6.pdf)
#### Keywords:  deep learning   differential privacy   algorithm   fine-tuning   computation efficiency
  Abs:  We study the problem of differentially private (DP) fine-tuning of large pre-trained models — a recent privacy-preserving approach suitable for solving downstream tasks with sensitive data. Existing work has demonstrated that high accuracy is possible under strong privacy constraint, yet requires significant computational overhead or modifications to the network architecture.

We propose differentially private bias-term fine-tuning (DP-BiTFiT), which matches the state-of-the-art accuracy for DP algorithms and the efficiency of the standard BiTFiT. DP-BiTFiT is model agnostic (not modifying the network architecture), parameter efficient (only training about 0.1% of the parameters), and computation efficient (almost removing the overhead caused by DP, in both the time and space complexity). On a wide range of tasks, DP-BiTFiT is 2 - 30X faster and uses 2 - 8X less memory than DP full fine-tuning, even faster than the standard full fine-tuning. This amazing efficiency enables us to conduct DP fine-tuning on language and vision tasks with long-sequence texts and high-resolution images, which were computationally difficult using existing methods.
## [Always-Sparse Training with Guided Stochastic Exploration](https://openreview.net/forum?id=XMaPp8CIXq) ------> [PDF](https://openreview.net/pdf/5024ceba5bb9c2a8525b3efee1dcad8c4236fdcc.pdf)
#### Keywords:  sparse training   sparsity   pruning   lottery tickets   scalable   efficient
  Abs:  The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm which improves the accuracy over previous methods. Additionally, our method has excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods.
## [Attention Prompt Tuning](https://openreview.net/forum?id=XCEc63Hbar) ------> [PDF](https://openreview.net/pdf/7085000dc9ce55733cd71a60d702d4bc61cfe869.pdf)
#### Keywords:  prompt tuning   efficient fine-tuning
  Abs:  In this paper, we introduce Attention Prompt Tuning (APT) – a computationally efficient variant of prompt tuning for video-based applications. Prompt tuning approaches involve injecting a set of learnable prompts along with data tokens during fine-tuning while keeping the backbone frozen. This approach greatly reduces the number of learnable parameters compared to full tuning. For image-based downstream tasks, normally a couple of learnable prompts achieve results close to those of full tuning. However, videos, which contain more complex spatiotemporal information, require hundreds of tunable prompts to achieve reasonably good results. This reduces the parameter efficiency observed in images and significantly increases latency and the number of floating-point operations (FLOPs) during inference. To tackle these issues, we directly inject the prompts into the keys and values of the non-local attention mechanism within the transformer block. Additionally, we introduce a novel prompt reparameterization technique to make APT more robust against hyperparameter selection. The proposed APT approach greatly reduces the number of FLOPs and latency while achieving a significant performance boost over the existing parameter-efficient tuning methods on the UCF101, HMDB51, and SSv2 datasets for action recognition. \textit{The code and pre-trained models will be publically available after the review process.}
## [POPULATION DESCENT: A NATURAL-SELECTION BASED HYPER-PARAMETER TUNING FRAMEWORK](https://openreview.net/forum?id=W2HJKGnb5y) ------> [PDF](https://openreview.net/pdf/630e696b3122aae4a98b3cfb3756932de07e43a7.pdf)
#### Keywords:  hyperparameter tuning   memetic algorithm   differential optimization   Gaussian mutations   supervised learning
  Abs:  First-order gradient descent has been the base of the most successful optimization algorithms ever implemented.
On supervised learning problems with very high dimensionality, such as neural network optimization, it is almost always the algorithm of choice, mainly due to its memory and computational efficiency.
However, it is a classical result in optimization that gradient descent converges to local minima on non-convex functions. Even more importantly, in certain high-dimensional cases, escaping the plateaus of large saddle points becomes intractable.
On the other hand, black-box optimization methods are not sensitive to the local structure of a loss function's landscape but suffer the curse of dimensionality.
Instead, memetic algorithms aim to combine the benefits of both.
Inspired by this, we present Population Descent, a memetic algorithm focused on hyperparameter optimization.
We show that an adaptive $m$-elitist selection approach combined with a normalized-fitness-based randomization scheme outperforms more complex state-of-the-art algorithms by up to 13\% on common benchmark tasks.
## [ColA: Collaborative Adaptation with Gradient Learning](https://openreview.net/forum?id=VfPWJM5FMr) ------> [PDF](https://openreview.net/pdf/b728eaf208039e173e95bf70975073311bcc382e.pdf)
#### Keywords:  Parameter Efficiency   Fine-Tuning   Adaptation   Transfer Learning   Distributed Machine Learning
  Abs:  A primary function of back-propagation is to compute both the gradient of hidden representations and parameters for optimization with gradient descent. Training large models requires high computational costs due to their vast parameter sizes. While Parameter-Efficient Fine-Tuning (PEFT) methods aim to train smaller auxiliary models to save computational space, they still present computational overheads, especially in Fine-Tuning as a Service (FTaaS) for numerous users. We introduce Collaborative Adaptation (ColA) with Gradient Learning (GL), a parameter-free, model-agnostic fine-tuning approach that decouples the computation of the gradient of hidden representations and parameters. In comparison to PEFT methods, ColA facilitates more cost-effective FTaaS by offloading the computation of the gradient to low-cost devices. We also provide a theoretical analysis of ColA and experimentally demonstrate that ColA can perform on par or better than existing PEFT methods on various benchmarks.
## [E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation](https://openreview.net/forum?id=Vdb8oSyEay) ------> [PDF](https://openreview.net/pdf/931a5f9510cd566aeec8f9c1d1a85f13edc6fadf.pdf)
#### Keywords:  generative adversarial network   diffusion model   efficient training
  Abs:  One highly promising direction for enabling flexible *real-time on-device* image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). 
This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. 
However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts.
In this work, we introduce and address a novel research direction: *can the process of distilling GANs from diffusion models be made significantly more efficient?*
To achieve this goal, we propose a series of innovative techniques.
First, we develop an attention-based network architecture tailored for efficient image-to-image translation on mobile devices, which yields faster inference speeds, reduces the number of parameters, and lowers computational costs compared to existing image-to-image models.
Second, we introduce a hybrid training pipeline that efficiently adapts a pre-trained text-conditioned GAN model to different concepts while substantially reducing computational costs. 
Moreover, this approach significantly minimizes the storage requirements for each concept.
Third, we investigate the minimal amount of data necessary to train each GAN, further reducing the overall training time.
Extensive experiments demonstrate that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.
## [Gradient-free Proxy for Efficient Language Model Search](https://openreview.net/forum?id=Vc52HZwNwe) ------> [PDF](https://openreview.net/pdf/2798b3f26765162f75c8fb0bd7ccb241182220d4.pdf)
#### Keywords:  gradient-free   neural architecture search   efficient language model
  Abs:  The rising demand for efficient natural language processing (NLP) systems has underscored the significance of developing lightweight language models. However, prevailing approaches to neural architecture search (NAS) often confront issues such as biased evaluation metrics and computational inefficiencies. This paper introduces weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored for lightweight language models. Our approach leverages two evaluation proxies, namely parameter count and principal component analysis (PCA) value of the feed-forward neural (FFN) layer, to provide a comprehensive and unbiased assessment. Additionally, by eliminating the need for gradient computations, we optimize the evaluation time, thereby enhancing the efficiency of designing and evaluating lightweight language models. Comparative analysis on the GLUE and SQuAD datasets demonstrates that our approach not only significantly reduces training time compared to one-shot NAS methods but also achieves higher scores in the testing phase compared to previous state-of-the-art training-based methods. Furthermore, ranking evaluations on a dataset sampled from the FlexiBERT search space reveal that our approach exhibits superior ranking correlation and further reduces solving time compared to other zero-shot NAS methods that require gradient computation.
## [Hyperion: Fused Multi-Trial and Gradient Descent for Joint Hyperparameter and Neural Architecture Optimization](https://openreview.net/forum?id=UyPmWupphV) ------> [PDF](https://openreview.net/pdf/d840d3ad779bcbcea06a43cfe529a7ee4a402613.pdf)
#### Keywords:  AutoML   Hyperparameter Optimization   Neural Architecture Search
  Abs:  We consider the fusion of multi-trial optimizers and gradient descent based oneshot algorithms to jointly optimize neural network hyperparameters and architectures. To combine strengths of optimizers from both categories, we propose Hyperion, which smartly distributes searched parameters into different involved optimizers, efficiently samples sub-search-spaces to reduce exploration costs of one-shot algorithms, and orchestrates co-optimization of both hyperparameters and network architectures. We demonstrate with open and industrial datasets that Hyperion outperforms non-fused optimization algorithms in optimized metrics, while significantly reducing GPU resources required for one-shot algorithms.
## [Query-Policy Misalignment in Preference-Based Reinforcement Learning](https://openreview.net/forum?id=UoBymIwPJR) ------> [PDF](https://openreview.net/pdf/e5a41a76b1dfd82ae86d55a55728e5c7e469123b.pdf)
#### Keywords:  preference-based reinforcement learning   human feedback efficiency   query-policy misalignment
  Abs:  Preference-based reinforcement learning (PbRL) provides a natural way to align RL agents’ behavior with human desired outcomes, but is often restrained by costly human feedback. To improve feedback efficiency, most existing PbRL methods focus on selecting queries to maximally improve the overall quality of the reward model, but counter-intuitively, we find that this may not necessarily lead to improved performance. To unravel this mystery, we identify a long-neglected issue in the query selection schemes of existing PbRL studies: Query-Policy Misalignment. We show that the seemingly informative queries selected to improve the overall quality of reward model actually may not align with RL agents’ interests, thus offering little help on policy learning and eventually resulting in poor feedback efficiency. We show that this issue can be effectively addressed via policy-aligned query and a specially designed hybrid experience replay, which together enforce the bidirectional query-policy alignment. Simple yet elegant, our method can be easily incorporated into existing approaches by changing only a few lines of code. We showcase in comprehensive experiments that our method achieves substantial gains in both human feedback and RL sample efficiency, demonstrating the importance of addressing query-policy misalignment in PbRL tasks.
## [EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models](https://openreview.net/forum?id=UmMa3UNDAz) ------> [PDF](https://openreview.net/pdf/3f4402abaa0b4ef7a71bc09c27c29f108bc3c83d.pdf)
#### Keywords:  Diffusion Models   Model Quantization   Model Compression   Efficient Models
  Abs:  Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization (PTQ) and quantization-aware training (QAT) are two main approaches, each bearing its own properties. While PTQ exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width settings. On the other hand, QAT can help alleviate performance degradation but comes with substantial demands on computational and data resources. To capitalize on the advantages while avoiding their respective drawbacks, we introduce a data-free, quantization-aware and parameter-efficient fine-tuning framework for low-bit diffusion models, dubbed EfficientDM, to achieve QAT-level performance with PTQ-like efficiency. Specifically, we propose a quantization-aware variant of the low-rank adapter (QALoRA) that can be merged with model weights and jointly quantized to low bit-width. The fine-tuning process distills the denoising capabilities of the full-precision model into its quantized counterpart, eliminating the requirement for training data. To further enhance performance, we introduce scale-aware optimization to address ineffective learning of QALoRA due to variations in weight quantization scales across different layers. We also employ temporal learned step-size quantization to handle notable variations in activation distributions across denoising steps. Extensive experimental results demonstrate that our method significantly outperforms previous PTQ-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a marginal $0.05$ sFID increase when quantizing both weights and activations of LDM-4 to 4-bit on ImageNet $256\times256$. Compared to QAT-based methods, our EfficientDM also boasts a $16.2\times$ faster quantization speed with comparable generation quality, rendering it a compelling choice for practical applications.
## [Retentive Network: A Successor to Transformer for Large Language Models](https://openreview.net/forum?id=UU9Icwbhin) ------> [PDF](https://openreview.net/pdf/564439d7ce449f81f7f18ca0fe02525176a438bf.pdf)
#### Keywords:  Large Language Models   Inference Efficiency
  Abs:  In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models.
## [Non-uniform Noise Injection For Enhancing DNN Adversarial Robustness And Efficiency](https://openreview.net/forum?id=UMwn5l37gU) ------> [PDF](https://openreview.net/pdf/5b3cbebb3734a89b4ed1e731c32f0c27cffd21ab.pdf)
#### Keywords:  Adversarial Robustness   Efficient Neural Networks   Hardware and Software Co-design
  Abs:  Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and safeguards essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robustness and efficiency across diverse attack scenarios, model architectures, and datasets.
## [EVEREST: Efficient Masked Video Autoencoder by Removing Redundant Spatiotemporal Tokens](https://openreview.net/forum?id=UM6QLuOVNi) ------> [PDF](https://openreview.net/pdf/c631aa765903beb1c75dfca62c0fe76889d09626.pdf)
#### Keywords:  Efficient Learning   Masked Modeling   Video Representation Learning
  Abs:  Masked video autoencoder approaches have demonstrated their potential by significantly outperforming previous self-supervised learning methods in video representation learning. However, they require an excessive amount of computations and memory while predicting uninformative tokens/frames due to random masking strategies, requiring excessive computing power for training. (e.g., over 16 nodes with 128 NVIDIA A100 GPUs). To resolve this issue, we exploit the unequal information density among the patches in videos and propose Efficient Masked Video AutoEncoder by Removing REdundant Spatiotemporal Tokens (EVEREST), a new token selection method for video representation learning that finds tokens containing rich motion features and drops uninformative ones during both pre-training and fine-tuning. We further present an information-intensive frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. Our method significantly reduces the computation and memory requirements of Masked video autoencoder, enabling the pre-training and fine-tuning on a single machine with 8 GPUs while achieving comparable performance to computation- and memory-heavy state-of-the-art methods on multiple benchmarks and on the uncurated Ego4D dataset. We hope that our work contributes to reducing the barrier to further research on video understanding.
## [Provably Accurate ODE Forecasting Through Explicit Trajectory Optimization](https://openreview.net/forum?id=UH4HinPK9d) ------> [PDF](https://openreview.net/pdf/bd8aeec0a1bd318451d6a19922291942b91ff46c.pdf)
#### Keywords:  Forecasting   Dynamical Systems   Parameter Estimation   Bayesian Inference
  Abs:  This work introduces a method to enable accurate forecasting of time series governed by ordinary differential equations (ODE) through the usage of cost functions explicitly dependent on the future trajectory rather than the past measurement times. We prove that the space of solutions of an $N$-dimensional, smooth, Lipschitz ODE on any given finite time horizon is an $N$-dimensional Riemannian manifold embedded in the space of square integrable continuous functions. This finite dimensional manifold structure enables the application of common statistical objectives such as maximum likelihood (ML), maximum a posteriori (MAP), and minimum mean squared error (MMSE) estimation directly in the space of feasible ODE solutions. The restriction to feasible trajectories of the system limits known issues such as oversmoothing seen in unconstrained MMSE forecasting. We demonstrate that direct optimization of trajectories reduces error in forecasting when compared to estimating initial conditions or minimizing empirical error. Beyond theoretical justifications, we provide Monte Carlo simulations evaluating the performance of the optimal solutions of six different objective functions: ML, MAP state estimation, MMSE state estimation, MAP trajectory estimation, MMSE trajectory estimation over all square integrable functions, and MMSE trajectory estimation over solutions of the differential equation.
## [SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings](https://openreview.net/forum?id=U9NHClvopO) ------> [PDF](https://openreview.net/pdf/41da583954eb64d25aa785dbf60f3d3af6281826.pdf)
#### Keywords:  Parameter-Efficient Fine-tuning   Prompt Learning   Soft Prompt Tuning   Langauge Models
  Abs:  Soft prompt tuning techniques have recently gained traction as an effective strategy for the parameter-efficient tuning of pretrained language models, particularly minimizing the required adjustment of model parameters. Despite their growing use, achieving optimal tuning with soft prompts, especially with smaller datasets, remains a substantial challenge. This study makes two contributions in this domain: (i) we introduce SuperPos-Prompt, a new reparameterization technique employing the superposition of multiple pretrained vocabulary embeddings to improve the learning of soft prompts.  Our experiments across several GLUE and SuperGLUE benchmarks consistently highlight SuperPos-Prompt's superiority over \textit{Residual Prompt} tuning, exhibiting an average score increase of +4.7 in T5-Small and $+3.9$ in T5-Base along with a faster convergence. Remarkably, SuperPos-Prompt occasionally outperforms even full fine-tuning methods. (ii) Additionally, we demonstrate enhanced performance and rapid convergence by omitting dropout from the frozen network, yielding consistent improvements across various scenarios and tuning methods. Unlike many existing strategies, our approach does not rely on the availability of a proficient pretrained source prompt for initialization, thereby ensuring notable flexibility and more effective combination of related prompt candidates.
## [Bi-GCL: Efficient Search on Networks](https://openreview.net/forum?id=U2To7z3JOw) ------> [PDF](https://openreview.net/pdf/5e1b2a9b55cc04aeb5eef4e0aa1458b9f61355eb.pdf)
#### Keywords:  Graph Contrastive Learning   Similarity Search   Binary Coding   Efficiency
  Abs:  Recent research shows the promising potential of node continuous embedding methods in Top-K network node similarity search, which often involves finding nearest neighbors measured by similarity in a continuous embedding space.However, these methods poorly scale to searching on large networks, since their embeddings demand significant storage and entail tremendous computation costs.In this paper, we introduce a graph contrastive learning framework for compressing continuous node embeddings into binary codes that enable customized bits per dimension, striking a balance between retrieval accuracy, speed, and storage.Specifically, a recurrent binarization with GNNs is presented, which consists of two components, a GNN encoder for learning node continuous representations, and a residual multilayer perception module for encoding representations to binary codes.The whole architecture is trained end-to-end by jointly optimizing three losses, i.e., contrastive loss from giving closely aligned representations to  positives, information bottleneck loss from superfluous information minimization, and representation distillation loss from aligning binary codes and their continuous counterparts.Extensive experiments demonstrates that our method achieves approximately 6x-19x faster retrieval and 16x-32x space reduction compared to traditional continuous-based embedding methods.Moreover, it significantly outperforms state-of-the-art continuous- and hash-based network embedding methods on several real-world networks.
## [Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning](https://openreview.net/forum?id=TilcG5C8bN) ------> [PDF](https://openreview.net/pdf/c0dda9fab9f0c7510b09173d59925afc2aa24857.pdf)
#### Keywords:  Self-supervised learning   efficient training   image similarity
  Abs:  Deep Neural Networks (DNNs), essential for diverse applications such as visual recognition and eldercare, often require a large amount of labeled data for training, making widespread deployment of DNNs a challenging task. Self-supervised learning (SSL) emerges as a promising approach, which leverages inherent patterns within data through diverse augmentations to train models without explicit labels. However, while SSL has shown notable advancements in accuracy, its high computation costs remain a daunting impediment, particularly for resource-constrained platforms. To address this problem, we introduce SimWnW, a similarity-based efficient self-supervised learning framework. By strategically removing less important regions in augmented images and feature maps, SimWnW not only reduces computation costs but also eliminates irrelevant features that might slow down the learning process, thereby accelerating model convergence. The experimental results show that SimWnW effectively reduces the amount of computation costs in self-supervised model training without compromising accuracy. Specifically, SimWnW yields up to 54\% and 51\% computation savings in training from scratch and transfer learning tasks, respectively.
## [Adaptive Compression of the Latent Space in Variational Autoencoders](https://openreview.net/forum?id=TYMeXb6PAw) ------> [PDF](https://openreview.net/pdf/c851b76b859e305eba3e073a10800beb58d3d053.pdf)
#### Keywords:  variational autoencoders   hyperparameter tuning   generative models
  Abs:  Variational Autoencoders (VAEs) are powerful generative models that have been widely used in various fields, including image and text generation. However, one of the known challenges in using VAEs is the model's sensitivity to its hyperparameters, such as the latent space size. This paper presents a simple extension of VAEs for automatically determining the optimal latent space size during the training process by gradually decreasing the latent size through neuron removal and observing the model performance.  The proposed method is compared to traditional hyperparameter grid search and is shown to be significantly faster while still achieving the best optimal dimensionality on four image datasets. Furthermore, we show that the final performance of our method is comparable to training on the optimal latent size from scratch, and might thus serve as a convenient substitute.
## [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://openreview.net/forum?id=TWVMVPx2wO) ------> [PDF](https://openreview.net/pdf/4dbc6e19b9a7c4710d81ef2af9e1aa4dfe8a92d8.pdf)
#### Keywords:  metric learning   image retrieval   parameter-efficient fine tuning
  Abs:  Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models derived from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance. We conduct extensive experiments to demonstrate that our proposed framework is superior and efficient by evaluating popular DML benchmarks. In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters.
## [Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition](https://openreview.net/forum?id=TVg6hlfsKa) ------> [PDF](https://openreview.net/pdf/0f075ccfb86db76c6949f35327a5ac5ac85f0773.pdf)
#### Keywords:  Visual Place Recognition   Parameter-efficient Transfer Learning   Global Adaptation   Local Adaptation
  Abs:  Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pre-trained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time. Our method ranks 1st on the leaderboard of MSLS place recognition challenge, and uses about only 3% retrieval runtime of the two-stage VPR method with RANSAC-based spatial verification. The code will be publicly available.
## [Benign Overfitting in Two-Layer ReLU Convolutional Neural Networks for XOR Data](https://openreview.net/forum?id=TJ2PQ9QaDF) ------> [PDF](https://openreview.net/pdf/14083976be3cf8ee1b707c242b7971b2101a0266.pdf)
#### Keywords:  Benign overfitting   Over-parameterized   XOR   Label-flipping   Correlated Features
  Abs:  Modern deep learning models are usually highly over-parameterized so that they can overfit the training data. Surprisingly, such overfitting neural networks can usually still achieve high  prediction accuracy. To study this ``benign overfitting'' phenomenon, a line of recent works has theoretically studied the learning of linear models and two-layer neural networks. However, most of these analyses are still limited to the very simple learning problems where the Bayes-optimal classifier is linear. In this work, we investigate a class of XOR-type classification tasks with label-flipping noises. We show that, under a certain condition on the sample complexity and signal-to-noise ratio, an over-parameterized ReLU CNN trained by gradient descent can achieve near Bayes-optimal accuracy. Moreover, we also establish a matching lower bound result showing that when the previous condition is not satisfied, the prediction accuracy of the obtained CNN is an absolute constant away from the Bayes-optimal rate. Our result demonstrates that CNNs have a remarkable capacity to efficiently learn XOR problems, even in the presence of highly correlated features.
## [NAP2: Neural Networks Hyperparameter Optimization Using Weights and Gradients Analysis](https://openreview.net/forum?id=TDxtP8nxkh) ------> [PDF](https://openreview.net/pdf/7d291df40c9cae79d04e897e2482fe693e01a36e.pdf)
#### Keywords:  hyper-parameter optimization   neural networks performance prediction   meta-learning
  Abs:  Recent hyper-parameter tuning methods for deep neural networks (DNNs) generally rely on first using low-fidelity methods to identify promising configurations and then using high-fidelity methods for further evaluation. While effective, existing solutions treat DNNs as `black boxes', which limits their predictive abilities. In this work, we propose Neural Architectures Performance Prediction (NAP2), a `white box' hyperparameter optimization approach. NAP2 models the changes in the weights and gradients of the analyzed networks over time and can predict their final performance with high accuracy, even after a short training period. Our evaluation shows that NAP2 outperforms the current state-of-the-art both in its ability to identify top-performing architectures and in the amount of resources it utilizes. Moreover, we show that our approach is transferable, meaning it is possible to train NAP2 on one dataset and apply it to another.
## [BiLoRA: A Bi-level Optimization Framework for Low-rank Adapters](https://openreview.net/forum?id=Svy1XoOLXj) ------> [PDF](https://openreview.net/pdf/adf35fc50da8d914f7b7935c32fbe22428e40dd9.pdf)
#### Keywords:  Parameter-efficient fine-tuning   Natural language processing   Low-rank adaptation
  Abs:  Low-rank adaptations (LoRA) are widely employed for fine-tuning large-scale pretrained models in downstream tasks, by learning low-rank incremental matrices. LoRA and its variants such as AdaLoRA train an entire low-rank incremental matrix on a single training dataset, which often leads to overfitting to training data and inferior generalization on test data. To address this problem, we propose a bi-level optimization (BLO) based method for alleviating overfitting. Our method parameterizes a low-rank incremental matrix in a pseudo singular value decomposition form, and separates the training of pseudo singular  vectors and values onto different data subsets in different optimization problems. This separation alleviates the risk of overfitting to a single dataset and improves generalization on other data. 
Specifically, in the lower level of our BLO formulation, we train  the pseudo singular vectors on a subset of the training data. In the upper level, we learn  the pseudo singular values on the other subset of the training data. The two levels of optimization problems are mutually dependent on each other and solved jointly. On ten datasets from natural language understanding and generation tasks and on various popular large pretrained models, our method achieves significantly better performance than LoRA,  AdaLoRA, and other fine-tuning baseline methods  with similar amounts of trainable parameters.
## [MiniFold: Simple, Fast and Accurate Protein Structure Prediction](https://openreview.net/forum?id=SjgfWbamtN) ------> [PDF](https://openreview.net/pdf/b4516f45afd71c33efba929027b5e5dd63cd484c.pdf)
#### Keywords:  protein   structure prediction   efficiency   hardware-optimization
  Abs:  Protein structure prediction has emerged as a powerful tool for biologists and drug makers. However, the computational toll associated with state-of-the-art models, such as AlphaFold2 or ESMFold, hinders their use in large-scale applications like virtual screening or mutational scanning, where a single experiment may involve processing millions of protein sequences. In an effort to develop a more efficient model, we aimed to understand which of the complex architectural choices proposed in AlphaFold2 were essential to achieve high performance, and which could be omitted without significantly compromising accuracy. This analysis culminated in a simple, yet highly expressive architecture for protein structure prediction. Our model, MiniFold, consists of a minimal Evoformer variant, a parameter-free coordinate recovery algorithm, and a custom hardware-optimized implementation composed of newly designed GPU kernels. When compared against ESMFold, MiniFold achieves over 100x speedup and shows improved scalability to long protein sequences while conserving over 95% of the original performance, making it a promising candidate for large-scale applications.
## [Masked Diffusion Models are Fast Distribution Learners](https://openreview.net/forum?id=SfNY6qAJBO) ------> [PDF](https://openreview.net/pdf/39f22d82a7282ac8984614bbd7c305bc93e99619.pdf)
#### Keywords:  image synthesis   diffusion model   masked pre-training   training efficiency
  Abs:  Diffusion models have emerged as the de-facto generative model for image synthesis, yet they entail significant training overhead, hindering the technique’s broader adoption in the research community. We observe that these models are commonly trained to learn all fine-grained visual information from scratch, thus motivating our investigation on its necessity. In this work, we show that it suffices to set up pretraining stage to initialize a diffusion model by encouraging it to learn some primer distribution of the unknown real image distribution. Then the pre-trained model can be fine-tuned for specific generation tasks efficiently. To approximate the primer distribution, our approach centers on masking a high proportion (e.g., up to 90%) of an input image and employing masked denoising score matching to denoise visible areas. Utilizing the learned primer distribution in subsequent fine-tuning, we efficiently train a ViT-based diffusion model on CelebA-HQ 256 × 256 in the raw pixel space, achieving superior training acceleration compared to denoising diffusion probabilistic model (DDPM) counterpart and a new FID score record of 6.73 for ViT-based diffusion models. Moreover, our masked pre-training technique can be universally applied to various diffusion models that directly generate images in the pixel space, aiding in the learning of pre-trained models with superior generalizability. For instance, a diffusion model pre-trained on VGGFace2 attains a 46% quality improvement through fine-tuning on only 10% data from a different dataset. Our code will be made publicly available.
## [Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting](https://openreview.net/forum?id=SLA7VOqwwT) ------> [PDF](https://openreview.net/pdf/9cc4e8699e52421af4f5f4e7e60debe790e5c4df.pdf)
#### Keywords:  OOD   Ensemble   Efficient model architecture
  Abs:  Uncertainty estimation is crucial for machine learning models to detect out-of-distribution (OOD) inputs. However, the conventional discriminative deep learning classifiers produce uncalibrated closed-set predictions for OOD data. A more robust classifiers with the uncertainty estimation typically require a potentially unavailable OOD dataset for outlier exposure training, or a considerable amount of additional memory and compute to build ensemble models. In this work, we improve on uncertainty estimation without extra OOD data or additional inference costs using an alternative Split-Ensemble method. Specifically, we propose a novel subtask-splitting ensemble training objective, where a common multiclass classification task is split into several complementary subtasks. Then, each subtask's training data can be considered as OOD to the other subtasks. Diverse submodels can therefore be trained on each subtask with OOD-aware objectives. The subtask-splitting objective enables us to share low-level features across submodels to avoid parameter and computational overheads. In particular, we build a tree-like Split-Ensemble architecture by performing iterative splitting and pruning from a shared backbone model, where each branch serves as a submodel corresponding to a subtask. This leads to improved accuracy and uncertainty estimation across submodels under a fixed ensemble computation budget. Empirical study with ResNet-18 backbone shows Split-Ensemble, without additional computation cost, improves accuracy over a single model by 0.8%, 1.8%, and 25.5% on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively. OOD detection for the same backbone and in-distribution datasets surpasses a single model baseline by, correspondingly, 2.2%, 8.1%, and 29.6% mean AUROC.
## [Simplifying and Stabilizing Model Selection in Unsupervised Domain Adaptation](https://openreview.net/forum?id=S6Xf70Y5CJ) ------> [PDF](https://openreview.net/pdf/0288606db10a11f26d05bf23ec9284cc66fdcb58.pdf)
#### Keywords:  Unsupervised Domain Adaptation; Unsupervised Model Selection; Unsupervised Hyperparameter Selection
  Abs:  Unsupervised domain adaptation (UDA) is a potent approach for enhancing model performance in an unlabeled target domain by leveraging relevant labeled data from a source domain. Despite the significant progress in UDA facilitated by deep learning, model selection, already a challenging task with deep models, becomes considerably more demanding in UDA scenarios due to the absence of labeled target data and substantial distribution shifts between domains. Existing model selection methods in UDA often struggle to maintain stable selections across diverse UDA methods and various UDA scenarios, frequently resulting in suboptimal or even the worst choices. This limitation significantly impairs their practicality and reliability for researchers and practitioners in the community. To address this challenge, we introduce a novel ensemble-based validation approach called EnsV, aiming to simplify and stabilize model selection in UDA. EnsV relies solely on predictions of unlabeled target data without making any assumptions about distribution shifts, offering high simplicity and versatility. Additionally, EnsV is built upon an off-the-shelf ensemble that is theoretically guaranteed to outperform the worst candidate model, ensuring high stability. In our experiments, we compare EnsV to 8 competitive model selection approaches. Our evaluation involves 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios. The results consistently demonstrate that EnsV stands out as a highly simple, versatile, and stable approach for practical model selection in UDA scenarios.
## [A trainable manifold for accurate approximation with ReLU Networks](https://openreview.net/forum?id=S4wo3MnlTr) ------> [PDF](https://openreview.net/pdf/05eab0d2e3fb668de7888c7dd15e0bee972b9c9d.pdf)
#### Keywords:  Deep ReLU Networks   Function Approximation   ReLU Network Efficiency
  Abs:  We present a novel technique for exercising greater control of the weights of ReLU activated neural networks to produce more accurate function approximations. Many theoretical works encode complex operations into ReLU networks using smaller base components. In these works, a common base component is a constant width approximation to $x^2$, which has exponentially decaying error with respect to depth. We extend this block to represent a greater range of convex one-dimensional functions. We derive a manifold of weights such that the output of these new networks utilizes exponentially many piecewise-linear segments. This manifold guides their training process to overcome drawbacks associated with random initialization and unassisted gradient descent. We train these networks to approximate functions which do not necessarily lie on the manifold, showing a significant reduction of error values over conventional approaches.
## [Linear Attention via Orthogonal Memory](https://openreview.net/forum?id=Rn3qJGOitY) ------> [PDF](https://openreview.net/pdf/5007fdb61ef9d3d7576452b8513e42be79b1b737.pdf)
#### Keywords:  Efficient attention; Efficiency degradation; Unbounded language modeling
  Abs:  Efficient attentions have greatly improved the computational efficiency of Transformers. However, most existing linear attention mechanisms suffer from an efficiency degradation problem, leading to inefficiencies in causal language modeling and hindering their application in long-range language models. This problem is more pronounced under language modeling with unbounded contexts. In this paper, we propose Linear Attention Via Orthogonal memory (LAVO) to address these limitations, achieving strong performance while maintaining linear complexity. LAVO employs orthogonal decomposition to compress a context into a fixed-size orthogonal memory while effectively minimizing redundancy within the context. Given that orthogonal memory compresses global information, we further dissect the context to amplify fine-grained local information. Additionally, we embed the relative position encoding into LAVO to improve the extrapolation ability. Experimental results show that LAVO greatly improves the efficiency of the causal language model with the best extrapolation performance and outperforms other efficient baselines. Further, we endeavor to employ LAVO for unbounded language modeling and successfully scale the context length to 128K.
## [LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning](https://openreview.net/forum?id=RbKThNNFxr) ------> [PDF](https://openreview.net/pdf/955372ce99cd15149a064b1a1e8459f40c95992b.pdf)
#### Keywords:  Low-rank adaptation   Memory-efficient   Large language models   Fine-tuning.
  Abs:  The low-rank adaptation (LoRA) method can largely reduce the amount of trainable parameters for fine-tuning large language models (LLMs) and it becomes a very common technique in fine-tuning LLMs. However, during fine-tuning, it still requires very expensive activation memory to update low-rank weights. Though there exist studies trying to reduce the storage of activations, they either would sacrifice model performance or take much longer time for fine-tuning models. To this end, we propose a memory-efficient fine-tuning method, named LoRA-FA, that significantly reduces the activation memory without performance degradation and extra computational costs. Specifically, LoRA-FA freezes the projection-down weight of $A$ and updates the projection-up weight of $B$ in each LoRA layer. It ensures the change of model weight reside in a low-rank space as like LoRA to preserve the fine-tuning performance while eliminating the requirement to store full-rank input activations so as to reduce the overall memory consumption. We conduct extensive experiments across multiple model types (RoBERTa, T5, LLaMA) and model scales. Our results show that LoRA-FA always preserves the fine-tuning accuracy across different tasks and it reduces the overall memory costs by up to 4$\times$ and 1.4$\times$ compared to full-parameter fine-tuning and LoRA, respectively. Furthermore, LoRA-FA is also compatible with other advanced memory optimization methods like FlashAttention, QLoRA, and ZeRO.
## [Tree Search-Based Policy Optimization under Stochastic Execution Delay](https://openreview.net/forum?id=RaqZX9LSGA) ------> [PDF](https://openreview.net/pdf/8ce53f5928127c0ee1d7ec9d9dd332389ff28598.pdf)
#### Keywords:  Reinforcement Learning   Delay   EfficientZero   Tree-search   Sample efficiency
  Abs:  The conventional formulation of Markov decision processes (MDPs) assumes that the agent's decisions are promptly executed.
However, in numerous realistic applications such as robotics or healthcare,  actions are performed with a delay which value can even be stochastic.
In this work, we introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation. 
We show that given observed delay values, it is sufficient to perform a policy search in the class of Markov policies in order to reach optimal performance, thus extending the deterministic fixed delay case. 
Armed with this insight, we devise Delayed EfficientZero, a model-based algorithm that optimizes over the class of Markov policies. Delayed EfficientZero leverages the Monte-Carlo tree search of its non-delayed variant EfficientZero to accurately infer future states from the action queue. Thus, it handles delayed execution while preserving the sample efficiency of EfficientZero. Through empirical analysis, we demonstrate that our algorithm surpasses all benchmark methods in Atari games when dealing with both constant and stochastic delays.
## [TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models](https://openreview.net/forum?id=RRayv1ZPN3) ------> [PDF](https://openreview.net/pdf/287631ae1a4b7b4aab73448e9cd0a6adcd994693.pdf)
#### Keywords:  Efficient Adaptation   Continual Learning   Robot Learning   Language-Conditioned Visuomotor Control   Few-shot Adaptation   Large Pretrained Models   Imitation Learning   Transfer Learning
  Abs:  The full potential of large pretrained models remains largely untapped in control domains like robotics. 
This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. Prior work mainly emphasizes effective \emph{pretraining} of large models for decision-making, with little exploration into how to perform data-efficient continual \emph{adaptation} of these models for new tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques---e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)---in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1\% of the trainable parameters of full fine-tuning, while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.
## [ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video](https://openreview.net/forum?id=RN2lIjrtSR) ------> [PDF](https://openreview.net/pdf/53a667e8ddbb4bcfdfc1f50492ba1ec0ec0d539e.pdf)
#### Keywords:  video understanding   action recognition   parameter-efficient transfer learning
  Abs:  Adapting image models to video domain is becoming an efficient paradigm for solving video recognition tasks. Due to the huge number of parameters and effective transferability of image models, performing full fine-tuning is less efficient and even unnecessary. Thus, recent research is shifting its focus towards parameter-efficient image-to-video adaptation. However, these adaptation strategies inevitably introduce extra computational cost to deal with the domain gap and temporal modeling in videos. In this paper, our goal is to present a zero-cost adaptation paradigm (ZeroI2V) to transfer the image transformers to video recognition tasks (i.e., introduce zero extra cost to the adapted models during inference). To achieve this goal, we present two core designs. First, to capture the dynamics in videos and reduce the difficulty of achieving image-to-video adaptation, we exploit the flexibility of self-attention and introduce the spatial-temporal dual-headed attention (STDHA) that efficiently endow the image transformers with temporal modeling capability at zero extra parameters and computation. Second, to handle the domain gap between images and videos, we propose a linear adaption strategy which utilizes lightweight densely placed linear adapters to fully transfer the frozen image models to video recognition. Due to its customized linear design, all newly added adapters could be easily merged with the original modules through structural reparameterization after training, thus achieving zero extra cost during inference. Extensive experiments on four widely-used video recognition benchmarks show that our ZeroI2V can match or even outperform previous state-of-the-art methods while enjoying superior parameter and inference efficiency.
## [Why Do We Need Weight Decay in Modern Deep Learning?](https://openreview.net/forum?id=RKh7DI23tz) ------> [PDF](https://openreview.net/pdf/8445884806a0eec0cde0a4b0ae97fc17eb1b083c.pdf)
#### Keywords:  Weight decay   overparameterization   implicit regularization   large language models   optimization dynamics.
  Abs:  Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we argue that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the *loss stabilization mechanism*. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the *bias-variance tradeoff* in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for $\texttt{bfloat16}$ mixed-precision training which is a crucial tool for LLM training.  Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way.
## [Enhancing Parameter Efficiency in Summarization via Expertise Separation](https://openreview.net/forum?id=RDU6p4Fydz) ------> [PDF](https://openreview.net/pdf/572ffca3148019cacddd3d596a66102973802aec.pdf)
#### Keywords:  summarization   parameter-efficient
  Abs:  A proficient summarization model should exhibit both flexibility -- the capacity to handle a range of in-domain summarization tasks, and adaptability -- the competence to acquire new knowledge and adjust to unseen out-of-domain tasks. Unlike large language models (LLMs) that achieve this through parameter scaling, we propose a more parameter-efficient approach in this study. Our motivation rests on the principle that while the general summarization ability to capture salient information can be shared across different tasks, the domain-specific summarization abilities need to be distinct and tailored. Concretely, we propose MoeSumm, a Mixture-of-Expert Summarization architecture, which utilizes a main expert for gaining the general summarization capability and deputy experts that selectively collaborate to meet specific summarization task requirements. We further propose a max-margin loss to stimulate the separation of these abilities. Our model's distinct separation of general and domain-specific summarization abilities grants it with notable flexibility and adaptability, all while maintaining parameter efficiency. MoeSumm achieves flexibility by managing summarization across multiple domains with a single model, utilizing a shared main expert and selected deputy experts. It exhibits adaptability by tailoring deputy experts to cater to out-of-domain few-shot and zero-shot scenarios. Experimental results on 11 datasets show the superiority of our model compared with recent baselines and LLMs. We also provide statistical and visual evidence of the distinct separation of the two abilities in MoeSumm.
## [Meta Compression: Learning to compress Deep Neural Networks](https://openreview.net/forum?id=RCKeTZKE5o) ------> [PDF](https://openreview.net/pdf/e875457aa98c22faf136516b85ba9b1d8db9902b.pdf)
#### Keywords:  Model compression   meta learning   efficient inference   deep learning
  Abs:  Deploying large pretrained deep learning models is hindered by the limitations of realistic scenarios such as resource constraints on the user/edge devices. Issues such as selecting the right pretrained model, compression method, and compression level to suit a target application and hardware become especially important. We address these challenges using a  novel meta learning framework that can provide high quality recommendations tailored to the specified resource, performance, and efficiency constraints.
For scenarios with limited to no access to unseen samples that resemble the distribution used for pretraining, we invoke diffusion models to improve generalization to test data and thereby demonstrate the promise of augmenting meta-learners with generative models. When learning across several state-of-the-art compression algorithms and DNN architectures trained on the CIFAR10 dataset, our top recommendation shows only 1\% drop in average accuracy loss compared to the optimal compression method. This is in contrast to 25\% average accuracy drop achieved by selecting the single best compression method across all constraints.
## [Efficiency Pentathlon: A Standardized Benchmark for Efficiency Evaluation](https://openreview.net/forum?id=Qyp3Rni2g1) ------> [PDF](https://openreview.net/pdf/1c0627f1fd6d450ef5185d7d3a14cbd350597656.pdf)
#### Keywords:  Efficiency   evaluation   benchmark   natural language processing
  Abs:  Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model’s lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.
## [NeuralFuse: Learning to Recover the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes](https://openreview.net/forum?id=Qvoe4wXWFi) ------> [PDF](https://openreview.net/pdf/a9cc8d2d57a413f7f688c22e9c14df4d36b92ed5.pdf)
#### Keywords:  machine learning   energy efficient inference   bit error resilience
  Abs:  Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while recovering accuracy by up to 57%. To the best of our knowledge, this is the first model-agnostic approach (i.e., no model retraining) to address low-voltage-induced bit errors.
## [Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs](https://openreview.net/forum?id=QmYNBVukex) ------> [PDF](https://openreview.net/pdf/e22abbedea87c94f75d99d09a98acc617375a4f8.pdf)
#### Keywords:  Data Selection   Large Language Model   Optimal Transport   Data-centric AI   Data Efficiency
  Abs:  This work focuses on leveraging and selecting from vast, unlabeled, open data to \emph{pre-fine-tune} a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. While many data selection algorithms have been designed for small-scale applications, rendering them unsuitable for our context, some emerging methods do cater to language data scales. However, they often prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution. Differing from prior work, our key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. We demonstrate the efficacy of our methodology across a diverse array of tasks, showing that it consistently surpasses other selection methods. Moreover, our proposed method is significantly faster than existing techniques, scaling to millions of samples within a single GPU hour. Our code is open-sourced \footnote{Code repository: \url{https://anonymous.4open.science/r/DV4LLM-D761/}}. While fine-tuning offers significant potential for enhancing performance across diverse tasks, its associated costs often limit its widespread adoption; with this work, we hope to lay the groundwork for cost-effective fine-tuning, making its benefits more accessible.
## [BOSS: Diversity-Difficulty Balanced One-Shot Subset Selection for Data-Efficient Deep Learning](https://openreview.net/forum?id=QcgvtqxRhI) ------> [PDF](https://openreview.net/pdf/8935600f4b5e12ef418054c31d9d76d4de8ad03c.pdf)
#### Keywords:  subset selection   data-efficient deep learning
  Abs:  Subset or core-set selection offers a data-efficient way for training deep learning models by identifying important data samples so that the model can be trained using the selected subset with similar performance as trained on the full set. However, most existing methods tend to choose either diverse or difficult data samples, which is likely to form a suboptimal subset, leading to a model with compromised generalization performance. One key limitation is due to the misalignment with the underlying goal of subset selection as an optimal subset should faithfully represent the joint data distribution that is comprised of both feature and label information. To this end, we propose to conduct diversity-difficulty Balanced One-shot Subset Selection (BOSS), aiming to construct an optimal subset for data-efficient deep learning. Samples are selected into the subset so that a novel balanced core-set loss bound is minimized, which theoretically justifies the need to simultaneously consider both diversity and difficulty to form an optimal subset. The loss bound also unveils the key relationship between the type of data samples to be included in the subset and the subset size. This further inspires the design of an expressive importance function to optimally balance diversity and difficulty depending on the subset size. The proposed approach is inspired by a theoretical loss bound analysis and utilizes a fine-grained importance control mechanism. A comprehensive experimental study is conducted on both synthetic and real datasets to justify the important theoretical properties and demonstrate the superior performance of BOSS as compared with the competitive baselines.
## [OneSpike: Ultra-low latency spiking neural networks](https://openreview.net/forum?id=QRWrvzRU4w) ------> [PDF](https://openreview.net/pdf/adfc1712b3334b0122827b08f292eccb8d93dd82.pdf)
#### Keywords:  Spiking Neural Network; Energy efficiency; Model Scaling
  Abs:  With the development of deep learning models, there has been growing research interest in spiking neural networks (SNNs) due to their energy efficiency resulting from their multiplier-less nature. The existing methodologies for SNN development include the conversion of artificial neural networks (ANNs) into equivalent SNNs or the emulation of ANNs, with two crucial challenges yet remaining. The first challenge involves preserving the accuracy of the original ANN models during the conversion to SNNs. The second challenge is to run complex SNNs with lower latencies. To solve the problem of high latency while maintaining high accuracy, we proposed a parallel spike-generation (PSG) method to generate all the spikes in a single timestep, while achieving a better model performance than the standard Integrate-and-Fire model. Based on PSG, we propose OneSpike, a highly effective framework that helps to convert any rate-encoded convolutional SNN into one that uses only one timestep without accuracy loss. Our OneSpike model achieves a state-of-the-art (for SNN) accuracy of $81.92\%$ on the ImageNet dataset using just a single time step. To the best of our knowledge, this study is the first to explore converting multi-timestep SNNs into equivalent single-timestep ones, while maintaining accuracy. These results highlight the potential of our approach in addressing the key challenges in SNN research, paving the way for more efficient and accurate SNNs in practical applications.
## [InfoNet: An Efficient Feed-Forward Neural Estimator for Mutual Information](https://openreview.net/forum?id=PyHRUMxKbT) ------> [PDF](https://openreview.net/pdf/1fdb0bb62e6d05ab5457763b28ff2086d9780307.pdf)
#### Keywords:  Deep Learning   Efficient Mutual Information Estimation   Real-Time Correlation Computation   Maximum Correlation Coefficient
  Abs:  Estimating mutual correlations between random variables or data streams is crucial for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been widely studied and used for its generality and equitability. However, existing methods either lack the efficiency required for real-time applications or the differentiability necessary with end-to-end learning frameworks. In this paper, we present InfoNet, a feed-forward neural estimator for mutual information that leverages the attention mechanism and the computational efficiency of deep learning infrastructures. By training InfoNet to maximize a dual formulation of mutual information via a feed-forward prediction, our approach circumvents the time-consuming test-time optimization and comes with the capability to avoid local minima in gradient descent. We evaluate the effectiveness of our proposed scheme on various families of distributions and check its generalization to another important correlation metric, i.e., the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. Our results demonstrate a graceful efficiency-accuracy trade-off and order-preserving properties of InfoNet, providing a comprehensive toolbox for estimating both the Shannon Mutual Information and the HGR Correlation Coefficient. We will make the code and trained models publicly available and hope it can facilitate studies in different fields that require real-time mutual correlation estimation.
## [SummaryMixing: A Linear-Complexity Alternative to Self-Attention for Speech Recognition and Understanding](https://openreview.net/forum?id=PoBB8n52oi) ------> [PDF](https://openreview.net/pdf/e636155ec2df7fe519772136ff6ca0aa02051fa0.pdf)
#### Keywords:  efficient deep learning   speech recognition   spoken language understanding
  Abs:  Modern speech processing systems rely on self-attention. Unfortunately, token
mixing with self-attention takes quadratic time in the length of the speech utterance,
slowing down inference as well as training and increasing memory consumption.
Cheaper alternatives to self-attention for ASR have been developed, but they fail to
consistently reach the same level of accuracy. However, attention layers in trained
speech recognizers tend to not capture fine-grained pair-wise information. This
paper, therefore, proposes a novel linear-time alternative to self-attention. It sum-
marises a whole utterance with the mean over vectors for all time steps. This single
summary is then combined with time-specific information. We call this method
“SummaryMixing”. Introducing SummaryMixing in state-of-the-art ASR models
makes it feasible to preserve or exceed previous speech recognition performance
while lowering the training and inference times by up to 28% and reducing the
memory budget by a factor of two. The benefits of SummaryMixing can also be
generalized to other speech-processing tasks, such as speech understanding.
## [On the Hyperparameter Loss Landscapes of Machine Learning Algorithms](https://openreview.net/forum?id=PlZIXgfWPH) ------> [PDF](https://openreview.net/pdf/fba06cb8afaa846cb2330ec2e2a8094647b3551a.pdf)
#### Keywords:  Landscape analysis   hyperparameter optimization   exploratory analysis
  Abs:  Hyperparameter optimization (HPO) is often formulated as a black-box, expensive optimization problem. Despite the recent success in a plethora of HPO algorithms, little has been known about the intricate play of model hyperparameters (HPs) and the resulting losses, especially when faced with different scenarios. In this paper, we aim to shed light on this black box by conducting comprehensive fitness landscape anaysis (FLA) on the HP loss landscapes of ML models under $i)$ training and test setups, and different $ii)$ fidelities, $iii)$ datasets, $iv)$ models. We do so by developing a dedicated landscape analysis framework that incorporates a combination of visual and quantitative measures, characterizing both topological structures and configuration rankings of the landscapes. We apply this framework to analyze $1,476$ HP loss landscapes of $5$ ML models, $63$ datasets with over $11$ million model evaluations of different fidelities. Our empirical results reveal a universal picture of HP loss landscapes. In this picture, landscapes feature a fairly smooth and neutral terrain where configurations are clustered with respect to their losses; there is a large plateau consisting of prominent configurations, where the landscape becomes flatter around the optimum. We also show that landscapes of different fidelities, datasets share considerable similarities that can be exploited to accelerate HPO, whereas test landscapes could significantly deviate from training landscapes due to overfitting.
## [Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](https://openreview.net/forum?id=PgBunNWJV1) ------> [PDF](https://openreview.net/pdf/431660bbc8aa3f1ba733d0898e570814fcae8525.pdf)
#### Keywords:  sparsity   sparse training   efficient training
  Abs:  Recent studies have explored the application of weight sparsity to enhance the
training efficiency of DNNs in terms of test accuracy~w.r.t training FLOPs.
These studies have focused on reducing training FLOPs, but training with sparse
weights often results in accuracy degradation or necessitates prolonged training
schedules to attain performance similar to the original dense models; making the
actual training efficiency gains less evident. In contrast, our work emphasizes
leveraging sparsity to increase accuracy while maintaining the same FLOPs as the
dense model, thereby demonstrating improved training efficiency through higher
accuracy. We introduce Sparse-IFT, a family of Sparse Iso-FLOP Transformations
that serve as drop-in replacements for dense layers, enhancing their
representational capacity and FLOP efficiency. Each transformation is
parameterized by a single hyperparameter (i.e., sparsity level), offering a
broader search space for identifying optimal sparse masks. Substituting dense
layers with Sparse-IFT, without altering any training hyperparameters, yields
substantial improvements across a range of computer vision and natural language
processing tasks; ResNet-18 on ImageNet (+3.5\%) and GPT-3 Small on
WikiText-103 (-0.4 PPL), both matching larger dense models that use 2x
or more FLOPs. To our knowledge, this is the first work to demonstrate the use
of sparsity for improving the accuracy of dense models, all while maintaining
consistent training FLOPs budgets via a simple set of sparse transformations.
## [Incremental Successive Halving for Hyperparameter Optimization with Budget Constraints](https://openreview.net/forum?id=Pa4hecILrt) ------> [PDF](https://openreview.net/pdf/b73c3fa1f1ea25c22ed5ae548883169a01c950e8.pdf)
#### Keywords:  hyperparameter optimization   sustainability   multi-fidelity
  Abs:  Hyperparameter optimization (HPO) is indispensable for achieving optimal performance in machine learning tasks. While some approaches focus on sampling more promising hyperparameter configurations, methods based on the successive halving algorithm (SHA) focus on efficiently evaluating hyperparameter configurations through the adaptive allocation of evaluation resources and stopping unpromising candidates early. Yet, SHA comes with several hyperparameters itself, one of which is the maximum budget that can be allocated to evaluate a single hyperparameter configuration. Asynchronous extensions of SHA (ASHA) devise a strategy of autonomously increasing the maximum budget and simultaneously allowing for better parallelization. However, while working well in practice with many considered hyperparameter configurations, there are limitations to the soundness of these adaptations when the overall budget for HPO is limited. This paper provides a theoretical analysis of ASHA in applications with budget constraints. We propose incremental SHA (iSHA), a synchronous extension of SHA, allowing to increment the maximum budget. A theoretical and empirical analysis of iSHA shows that soundness is maintained while guaranteeing to be more resource-efficient than SHA. In an extensive set of experiments, we also demonstrate that, in general, iSHA performs superior to ASHA and progressive ASHA.
## [Incorporating Domain Knowledge in VAE Learning via Exponential Dissimilarity-Dispersion Family](https://openreview.net/forum?id=P6gYcTj6YC) ------> [PDF](https://openreview.net/pdf/e00a874ec7f9cacd78b95bc5e2c3fbf1d6e3e21e.pdf)
#### Keywords:  variational autoencoders   deep generative models   similarity indices   domain knowledge   dispersion parameter
  Abs:  Variational autoencoder (VAE) is a prominent generative model that has been actively applied to various unsupervised learning tasks such as representation learning. Despite its representational capability, VAEs with the commonly adopted Gaussian settings typically suffer from performance degradation in generative modeling for high-dimensional natural data, which is often caused by their excessively limited model family. In this paper, we introduce the exponential dissimilarity-dispersion family (EDDF), a novel distribution family that includes a dissimilarity function and a global dispersion parameter. A decoder with this distribution family induces arbitrary dissimilarity functions as the reconstruction loss of the evidence lower bound (ELBO) objective, where the model leverages domain knowledge through this dissimilarity function. For VAEs with EDDF decoders, we also propose an ELBO optimization method that implicitly approximates the stochastic gradient of the normalizing constant using log-expected dissimilarity. Empirical evaluations of the generative performance show the effectiveness of our model family and proposed method in the vision domain, indicating that the effect of dissimilarity determines the criteria of representational informativeness.
## [STRUCTDROP: A STRUCTURED RANDOM ALGORITHM TOWARDS EFFICIENT LARGE-SCALE GRAPH TRAINING](https://openreview.net/forum?id=Oy1NtlFDmD) ------> [PDF](https://openreview.net/pdf/3d9a36d0280d517f53595e454709d52ffc475c7a.pdf)
#### Keywords:  Efficient Training   Randomized Algorithm
  Abs:  Graph neural networks (GNNs) have gained considerable success in graph-based learning tasks, yet training GNNs on large graphs is still inefficient. The root cause is the graph-based sparse operations are difficult to accelerate with commodity hardware. Prior art reduces the computation cost of sparse matrix based operations (e.g., linear) via sampling-based approximation. However, two under-explored pain points still persist in this paradigm. Inefficiency Issue: The random-based sampling approaches have the non-zero entries randomly distributing over adjacency matrix, which slows down memory access process and is difficult to accelerate with commodity hardware. Under-fitting Problem: The previous sampling methods only utilize the same subset of nodes during the training, which may cause the under-fitting problem on other remain nodes. Aiming to systematically address these two pain points, we propose StructuredDropout, a.k.a, StructDrop. This method involves the selective random sampling of columns and rows from a sparse matrix for computation. Comprehensive experiments validate the efficiency and generalization of our framework: StructDrop achieves up to 5.09x speedup for a single sparse operation and 6.48x end-to-end speedup with negligible accuracy loss or even better accuracy.
## [ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models](https://openreview.net/forum?id=OfXqQ5TRwp) ------> [PDF](https://openreview.net/pdf/98ac8eb02b92dc3e39e8ecac3ad87184924c0eca.pdf)
#### Keywords:  Memory efficient training   Activation-compressed training   Average Quantization   NLP   Transformer
  Abs:  One of the key challenges in deep neural network training is the substantial amount of GPU memory required to store activations obtained in the forward pass. Various Activation-Compressed Training (ACT) schemes have been proposed to mitigate this issue; however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training. In this paper, we introduce ALAM, a novel ACT framework that utilizes average quantization and a lightweight sensitivity calculation scheme, enabling large memory saving in LLMs while maintaining training performance. We first demonstrate that compressing activations into their group average values minimizes the gradient variance. Employing this property, we propose Average Quantization which provides high-quality deeply compressed activations with an effective precision of less than 1 bit and improved flexibility of precision allocation. In addition, we present a cost-effective yet accurate sensitivity calculation algorithm that solely relies on the L2 norm of parameter gradients, substantially reducing memory overhead due to sensitivity calculation. In experiments, the ALAM framework significantly reduces activation memory without compromising accuracy, achieving up to a 12.5$\times$ compression rate in LLMs.
## [More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory](https://openreview.net/forum?id=OdpIjS0vkO) ------> [PDF](https://openreview.net/pdf/3606acd39fb1c20e525631bb1e692ade2bb4fe11.pdf)
#### Keywords:  overparameterization   interpolation   random feature regression   kernel regression   generalization   overfitting
  Abs:  In our era of enormous neural networks, empirical progress has been driven by the philosophy that *more is better.*
Recent deep learning practice has found repeatedly that larger model size, more data, and more computation (resulting in lower training loss) optimizing to near-interpolation improves performance. In this paper, we give theoretical backing to these empirical observations by showing that these three properties hold in random feature (RF) regression, a class of models equivalent to shallow networks with only the last layer trained.

Concretely, we first show that the test risk of RF regression decreases monotonically with both the number of features and samples, provided the ridge penalty is tuned optimally. In particular, this implies that infinite width RF architectures are preferable to those of any finite width. We then proceed to demonstrate that, for a large class of tasks characterized by powerlaw eigenstructure, training to near-zero training loss is *obligatory:* near-optimal performance can *only* be achieved when the training error is much smaller than the test error. Grounding our theory in real-world data, we find empirically that standard computer vision tasks with convolutional neural kernels clearly fall into this class. Taken together, our results tell a simple, testable story of the benefits of overparameterization and overfitting in random feature models.
## [Communication-Efficient Algorithm for Asynchronous Multi-Agent Bandits](https://openreview.net/forum?id=ONnZVUrFBT) ------> [PDF](https://openreview.net/pdf/de4b92eda3fd90755055e5840b9f1689f6bcd664.pdf)
#### Keywords:  Multi-Agent Multi-Armed Bandits   Distributed Learning   Efficient Communication
  Abs:  We study the cooperative asynchronous multi-agent multi-armed bandits problem, where the active (arm pulling) decision rounds of each agent are asynchronous. In each round, only a subset of agents is active to pull arms, and this subset is unknown and time-varying. We propose a fully distributed algorithm that relies on novel asynchronous communication protocols. This algorithm attains near-optimal regret with constant (time-independent) communications for adversarial asynchronicity among agents. Furthermore, to protect the privacy of the learning process, we extend our algorithms to achieve local differential privacy with rigorous guarantees. Lastly, we report numerical simulations of our new asynchronous algorithms with other known baselines.
## [Efficient Model-Agnostic Multi-Group Equivariant Networks](https://openreview.net/forum?id=O9gstAazBM) ------> [PDF](https://openreview.net/pdf/99a81e6eb83d56b56079d3a9252e79752ca45bf8.pdf)
#### Keywords:  Group equivariant networks   efficient equivariant networks   large equivariant networks
  Abs:  Constructing model-agnostic group equivariant networks, such as equitune (Basu et al., 2023b) and its generalizations (Kim et al., 2023), can be computationally expensive for large product groups. We address this by providing efficient model-agnostic equivariant designs for two related problems: one where the network has multiple inputs each with potentially different groups acting on them, and another where there is a single input but the group acting on it is a large product group. For the first design, we initially consider a linear model and characterize the entire equivariant space that satisfies this constraint. This characterization gives rise to a novel fusion layer between different channels that satisfies an invariance-symmetry (IS) constraint, which we call an IS layer. We then extend this design beyond linear models, similar to equitune, consisting of equivariant and IS layers. We also show that the IS layer is a universal approximator of invariant-symmetric functions. Inspired by the first design, we use the notion of the IS property to design a second efficient model-agnostic equivariant design for large product groups acting on a single input. For the first design, we provide experiments on multi-image classification where each view is transformed independently with transformations such as rotations. We find equivariant models are robust to such transformations and perform competitively otherwise. For the second design, we consider three applications: language compositionality on the SCAN dataset to product groups; fairness in natural language generation from GPT-2 to address intersec- tionality; and robust zero-shot image classification with CLIP. Overall, our methods are simple and general, competitive with equitune and its variants, while also being computationally more efficient.
## [MoDA: Mixture of Domain Adapters for Parameter-efficient Generalizable Person Re-Identification](https://openreview.net/forum?id=O1cLOzgi81) ------> [PDF](https://openreview.net/pdf/292d41916351303fe14f44dbf510ec41c1008028.pdf)
#### Keywords:  Generalizable Person Re-Identification   Domain Generalization   Parameter-efficient Fine-tuning
  Abs:  Domain Generalizable ReID task has garnered much attention in recent years, as a more challenging task but more closely aligned with practical applications. Mixture-of-experts (MoE) based methods has been studied for DG ReID to exploit the discrepancies and inherent correlations between diverse domains. However, most of DG ReID methods, including MoE-based methods, have to full fine-tune the large amount of parameters of backbones, classifier heads and experts. And in the set of DG ReID, the number of person IDs is particularly large which results in that parameters of classifier heads increases sharply. And make it difficult for MoE-based method to scale up to larger vision models. For this motivation, we propose a novel MoE-based DG ReID method, named mixture of do- main adapters (MoDA), to mitigate the issues men- tioned above. We apply adapter and CLIP to DG ReID in a parameter-efficient way. Extensive experiments verify that MoDA achieves competitive end even better results with state-of-the-art methods with much fewer tunable parameters.
## [A General Framework for User-Guided Bayesian Optimization](https://openreview.net/forum?id=NjU0jtXcYn) ------> [PDF](https://openreview.net/pdf/dfefca6ed627d064d60f6ea6685e0e5c67b77cff.pdf)
#### Keywords:  Bayesian Optimization   Hyperparameter Optimization   Gaussian Processes
  Abs:  The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight  budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substantially accelerate optimization when the prior information is accurate, and to retain approximately default performance when it is misleading.
## [ELoRA: Efficient Low-Rank Adaptation with Random Matrices](https://openreview.net/forum?id=NjNfLdxr3A) ------> [PDF](https://openreview.net/pdf/83f8db4285547c957b261812d18065dcbc1363e1.pdf)
#### Keywords:  Parameter-efficient fine-tuning   Transfer learning   Low-rank   NLP
  Abs:  It is becoming common practice for natural language processing to finetune pretrained language models for several downstream tasks at the same time. In practice, one might see several use cases based on the same model running simultaneously. Yet, this practice comes with considerable storage requirements, an issue that becomes particularly acute when scaling to large models or deploying numerous per-user or per-task adapted models. Although parameter-efficient finetuning methods such as LoRA exist, they do not fully mitigate this storage challenge. To this end, we introduce Efficient Low-Rank Adaptation with Random Matrices (ELoRA), which takes parameter efficiency to the extreme. By freezing a single pair of random low-rank matrices, shared across all layers, and using small layer-wise trainable scaling vectors, ELoRA achieves a 10x reduction in trainable parameters compared to LoRA without compromising performance levels. We demonstrate the effectiveness of the method on the GLUE benchmark and analyze its parameter-performance trade-off. Finally, using the Llama2 7B model, we show that ELoRA can also be used for instruction-tuning with merely 1.4M parameters.
## [Improving LoRA in Privacy-preserving Federated Learning](https://openreview.net/forum?id=NLPzL6HWNl) ------> [PDF](https://openreview.net/pdf/941e0cc8465527cd17b4ac976413cf513a863f31.pdf)
#### Keywords:  Federated Learning   Parameter-efficient Fine-tuning   Differential Privacy   Large Language Model
  Abs:  Low-rank adaptation (LoRA) is one of the most popular task-specific parameter-efficient fine-tuning (PEFT) methods on pre-trained language models for its good performance and computational efficiency.
LoRA injects a product of two trainable rank decomposition matrices over the top of each frozen pre-trained model module.
However, when applied in the setting of privacy-preserving federated learning (FL), LoRA may become unstable due to the following facts: 1) the effects of data heterogeneity and multi-step local updates are non-negligible, 2) additive noise enforced on updating gradients to guarantee differential privacy (DP) can be amplified and 3) the final performance is susceptible to hyper-parameters.
A key factor leading to these phenomena is the discordance between jointly optimizing the two low-rank matrices by local clients and separately aggregating them by the central server.
Thus, this paper proposes an efficient and effective version of LoRA, Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further halve the communication cost of federated fine-tuning LLMs.
The core idea of FFA-LoRA is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices.
Compared to LoRA, FFA-LoRA is motivated by practical and theoretical benefits in privacy-preserved FL. 
Our experiments demonstrate that FFA-LoRA provides more consistent performance with better computational efficiency over vanilla LoRA in various FL tasks.
## [Branch-level Network Re-parameterization with Neural Substitution](https://openreview.net/forum?id=NJyCoAIPln) ------> [PDF](https://openreview.net/pdf/4207a5482d33a3e0d457a246f842cba4785dbe29.pdf)
#### Keywords:  Neural substitution   Re-parameterization   Branch-level connectivity
  Abs:  In this paper, we propose the neural substitution method for network re-parameterization at branch-level connectivity. The proposed neural substitution method learns a variety of network topologies, allowing our re-parameterization method to exploit the ensemble effect fully. In addition, we introduce a guiding method for reducing the non-linear activation function in a linear transformation. Because branch-level connectivity necessitates multiple non-linear activation functions, they must be reduced to a single activation with our guided activation method during the re-parameterization process. 
Being able to reduce the non-linear activation function in this manner is significant as it overcomes the limitation of the existing re-parameterization method, which works only at block-level connectivity.
If re-parameterization is applied only at the block-level connectivity, the network topology can only be exploited in a limited way, which makes it harder to learn diverse feature representations.
On the other hand, the proposed approach learns a considerably richer representation than existing re-parameterization methods due to the unlimited topology with branch-level connectivity, providing a generalized framework to be applied with other methods. 
The proposed method improves the re-parameterization performance, but it is also a general framework that enables existing methods to benefit from branch-level connectivity.
In our experiments, we show that the proposed re-parameterization method works favorably against existing methods while significantly improving their performance when applied to the proposed branch-level connectivity.
Upon acceptance, we will make our implementation publicly available.
## [Efficient Streaming Language Models with Attention Sinks](https://openreview.net/forum?id=NG7sS51zVF) ------> [PDF](https://openreview.net/pdf/2aec886c83abd0b11f676ec02200edb04ac75920.pdf)
#### Keywords:  Large Language Models   Length Extrapolation   Efficiency
  Abs:  Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.
Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory.
Secondly, popular LLMs cannot generalize to longer texts than the training sequence length.
Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size.
We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important.
Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.
We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.
In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2$\times$ speedup.
Code and datasets are provided in the anonymous link.
## [LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices](https://openreview.net/forum?id=MU6jInwj7p) ------> [PDF](https://openreview.net/pdf/b023ad7228205ec74c0f004a2510f3884b3e0a73.pdf)
#### Keywords:  Efficient Inference   Post-Training Quantization   Large Language Models
  Abs:  With the commercialization of large language models (LLMs), weight-activation quantization has emerged to compress and accelerate LLMs, achieving high throughput while reducing inference costs. However, existing post-training quantization (PTQ) techniques for quantizing both weights and activations of LLMs still suffer from non-negligible performance drops, especially on massive multitask language understanding. To address this issue, we propose Low-Rank Quantization (LRQ) - a simple yet effective post-training weight quantization method for LLMs that reconstructs the outputs of an intermediate Transformer block by leveraging low-rank weight-scaling matrices, replacing the conventional full weight-scaling matrices that entail as many learnable scales as their associated weights. Thanks to parameter sharing via low-rank structure, LRQ only need to learn significantly fewer parameters while enabling the individual scaling of weights, thus boosting the generalization capability of quantized LLMs. Through extensive experiments, we demonstrate the superiority of LRQ to prior LLM INT8 PTQ works. Remarkably, we confirm for the first time the possibility of 4-bit weight and 8-bit activation quantization for LLMs with minimal accuracy loss among LLM PTQ studies.
## [Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition](https://openreview.net/forum?id=MRSNGbgjkf) ------> [PDF](https://openreview.net/pdf/a7f3e007a8a01d2253d37b935745610cb5ad89f1.pdf)
#### Keywords:  parameter-efficient transfer learning   dynamic adapter   inference efficiency
  Abs:  Parameter-efficient transfer learning (PETL) is a promising task, aiming to adapt the large-scale pretrained model to downstream tasks with a relatively modest cost. However, current PETL methods struggle in compressing computational complexity and bear heavy inference burden due to the complete forward process. This paper presents an efficient visual recognition paradigm, called Dynamic Adapter (Dyn-Adapter), that  boosts PETL efficiency by subtly disentangling features in multiple levels. Our approach is simple: first, we devise a dynamic architecture with balanced early heads for multi-level feature extraction, along with adaptive training strategy. Second, we introduce a bidirectional sparsity strategy driven by the pursuit of powerful generalized ability. These qualities enable us to fine-tune efficiently and effectively:  we reduce FLOPs during inference by 50%,  while maintaining or even yielding higher recognition accuracy. Extensive experiments on diverse datasets and pretrained backbones demonstrate the potential of Dyn-Adapter serving as a general efficiency booster for PETL. We will make the code publicly available.
## [Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance](https://openreview.net/forum?id=LajkZlgD83) ------> [PDF](https://openreview.net/pdf/879072ca2aaca1c477ba5db1725d1df6f535671d.pdf)
#### Keywords:  hybrid AI systems   retrieval augmentation   natural language generation   efficient AI
  Abs:  Retrieval augmentation enhances performance of traditional language models by incorporating additional context. However, the computational demands for retrieval augmented large language models (LLMs) poses a challenge when applying them to real-time tasks, such as composition assistance. To address this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework, a novel approach that efficiently combines a cloud-based LLM with a client-side smaller language model through retrieval augmented memory. This integration enables the client model to generate effective responses, benefiting from the LLM's capabilities and contextual information. Additionally, through an asynchronous memory update mechanism, the client model can deliver real-time completions promptly to user inputs without the need to wait for responses from the cloud. Our experiments on Wikitext dataset and Pile subsets demonstrate that HybridRAG significantly improves utility over client-only models while maintaining low latency.
## [FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning](https://openreview.net/forum?id=LZZD649RsD) ------> [PDF](https://openreview.net/pdf/ed142e308cd05165564aeedf2cdd0e3b074c1902.pdf)
#### Keywords:  Data efficient LLM tuning   Multilingual modeling
  Abs:  Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called FIAT that fuses the best of these paradigms together, enabling prompt-engineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety of multilingual tasks and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100-10,000 training examples. We hope that FIAT provides a practical way of harnessing the full potential of LLMs without needing to make a hard choice between learning paradigms.
## [Fairness Metric Impossibility: Investigating and Addressing Conflicts](https://openreview.net/forum?id=LIBZ7Mp0OJ) ------> [PDF](https://openreview.net/pdf/2c5566525169ab4a4fba09eb39a60d230d0f259b.pdf)
#### Keywords:  Fairness   Multi-objective optimization   Hyperparameter Optimization
  Abs:  Fairness-aware ML (FairML) applications are often characterized by intricate social objectives and legal requirements, often encompassing multiple, potentially conflicting notions of fairness. Despite the well-known Impossibility Theorem of Fairness and vast theoretical research on the statistical and socio-technical trade-offs between fairness metrics, many FairML approaches still optimize for a single, user-defined fairness objective. However, this one-sided optimization can inadvertently lead to violations of other pertinent notions of fairness, resulting in adverse social consequences. In this exploratory and empirical study, we address the presence of fairness-metric conflicts by treating fairness metrics as conflicting objectives in a multi-objective (MO) sense. To efficiently explore multiple fairness-accuracy trade-offs and effectively balance conflicts between various fairness objectives, we introduce the ManyFairHPO framework, a novel many-objective (MaO) hyper-parameter optimization (HPO) approach. By enabling fairness practitioners to specify and explore complex and multiple fairness objectives, we open the door to further socio-technical research on effectively combining the complementary benefits of different notions of fairness.
## [Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks](https://openreview.net/forum?id=L3yJ54gv3H) ------> [PDF](https://openreview.net/pdf/9d7f9740086e81d952563d8395b2fd6cb22c9d84.pdf)
#### Keywords:  Nonparametric Classification;  Low Dimensional Manifolds; Overparameterized ResNets; Function Approximation
  Abs:  Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
## [FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent](https://openreview.net/forum?id=Kl9CqKf7h6) ------> [PDF](https://openreview.net/pdf/915e1c7d3834d87bb16b9966f816240da2a9a422.pdf)
#### Keywords:  Federate Learning   Learning rate   Hyperparameter   Hypergradient
  Abs:  The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper’s convergence rate and conduct extensive experiments on vision and language benchmark datasets. The results demonstrate that FEDHYPER consistently converges 1.1-3× faster than FedAvg and the competing baselines while achieving superior final accuracy. Moreover, FEDHYPER catalyzes a remarkable surge in accuracy, augmenting it by up to 15% compared to FedAvg under suboptimal initial learning rate settings.
## [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=KjegfPGRde) ------> [PDF](https://openreview.net/pdf/85de2d7cb96877ffe0b294fe0cd70f925917d27f.pdf)
#### Keywords:  Natural Language Processing   Language Models   Parameter-efficient Fine-tuning
  Abs:  Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance while saving over 20% memory and time costs compared to vanilla PT and its variants, without changing trainable parameter sizes. Through extensive experiments on 23 natural language processing (NLP) and vision-language (VL) tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches, including the full fine-tuning baseline in some scenarios. Additionally, we empirically show that DEPT grows more efficient as the model size increases. Our further study reveals that DePT integrates seamlessly with parameter-efficient transfer learning in the few-shot learning setting and highlights its adaptability to various model architectures and sizes.
## [Cascading Reinforcement Learning](https://openreview.net/forum?id=KjOAHlKMF5) ------> [PDF](https://openreview.net/pdf/1c7bc9ec725c0e6776cf5fc4e47b62cbb86a9339.pdf)
#### Keywords:  reinforcement learning   cascading bandits   combinatorial action space   computational and sample efficiency
  Abs:  Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with  large attraction probabilities but also leading to good successor states. This imposes a huge computational challenge due to the combinatorial action space. To tackle this challenge, we delve into the properties of value functions, and design an oracle BestPerm to efficiently find the optimal item list. Equipped with BestPerm, we develop two algorithms CascadingVI and CascadingBPI, which are both computationally-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. Furthermore, we present experiments to show the improved computational and sample efficiencies of our algorithms compared to straightforward adaptations of existing RL algorithms in practice.
## [Language Model Cascades: Token-Level Uncertainty And Beyond](https://openreview.net/forum?id=KgaBScZ4VI) ------> [PDF](https://openreview.net/pdf/398e21186e34947c7c8eab1053d87a9f2b9702a5.pdf)
#### Keywords:  Cascades   Efficient Inference   Language Models
  Abs:  Recent advances in language models (LMs) have led to significant improvements in quality on complex NLP tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most “easy” instances, while a few “hard” instances are deferred to the large model. While the principles underpinning effective cascading are well-studied for classification tasks — with deferral based on predicted class uncertainty favored theoretically and practically — a similar understanding is lacking for generative LM tasks. In this work, we initiate a systematic study of deferral rules for LM cascades. We begin by examining the natural extension of predicted class uncertainty to generative LM tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. This is because LMs produce a sequence of uncertainty values, one for each output token; and moreover, the number of output tokens is variable across different examples. To mitigate the length bias, we propose to exploit the richer token-level uncertainty information implicit in generative LMs. We argue that naive predicted sequence uncertainty corresponds to a simple aggregation of these uncertainties. By contrast, we show that incorporating token-level uncertainty through learned post-hoc deferral rules can significantly outperform such simple aggregation strategies, via experiments on a range of natural language benchmarks with FLAN-T5 models. We further show that incorporating embeddings from the smaller model and intermediate layers of the larger model can give an additional boost in the overall cost-quality tradeoff.
## [Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit](https://openreview.net/forum?id=KZJehvRKGD) ------> [PDF](https://openreview.net/pdf/3d7aeb14b83976b087f1d8c04107fedca1bfbebe.pdf)
#### Keywords:  Deep Learning Theory   Hyperparameter Transfer   Training Dynamics   Residual Networks
  Abs:  The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks *transfer* to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and vision transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet.  Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show that this parameterization of ResNets admits a well-defined feature learning joint infinite-width and infinite-depth limit and show convergence of finite-size network dynamics towards this limit.
## [Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective](https://openreview.net/forum?id=KZA5gDk2Jj) ------> [PDF](https://openreview.net/pdf/7b73099bcd325f792802d7f929cc738cda0f8376.pdf)
#### Keywords:  Overparameterized machine learning   training data memorization   autoencoders   training data recovery   inverse problem
  Abs:  We study the recovery of training data from overparameterized autoencoder models. 
Given a degraded training sample, we define the recovery of the original sample as an inverse problem and formulate it as an optimization task. In our inverse problem, we use the trained autoencoder to implicitly define a regularizer for the particular training dataset that we aim to retrieve from. We develop the intricate optimization task into a practical method that iteratively applies the trained autoencoder and relatively simple computations that estimate and address the unknown degradation operator. We evaluate our method for blind inpainting where the goal is to recover training images from degradation of many missing pixels in an unknown pattern. We examine various deep autoencoder architectures, such as fully connected and U-Net (with various nonlinearities and at diverse train loss values), and show that our method significantly outperforms previous methods for training data recovery from autoencoders. Importantly, our method greatly improves the recovery performance also in settings that were previously considered highly challenging, and even impractical, for such retrieval.
## [Simplicity Bias in Overparameterized Machine Learning](https://openreview.net/forum?id=KNQJtoPZmz) ------> [PDF](https://openreview.net/pdf/6ff4015de0a405618221b563cd733e317652c234.pdf)
#### Keywords:  Simplicity Bias; Overparameterized models; deep learning
  Abs:  A thorough theoretical understanding of the surprising generalization ability of deep networks (and other overparameterized models) is still lacking. Here we demonstrate that simplicity bias is a major phenomenon to be reckoned with in overparameterized machine learning.
In addition to explaining the outcome of simplicity bias, we also study its source:  following concrete rigorous examples, we argue that (i) simplicity bias can explain generalization in overparameterized learning models such as neural networks; (ii) simplicity bias and excellent generalization are optimizer-independent, as our examples show, and although the optimizer  affects training,  it is not the driving force behind simplicity bias; (iii) simplicity bias in pre-training models, and subsequent posteriors,  is  universal and stems from the subtle fact that uniformly-at-random constructed priors are not uniformly-at-random sampled; and (iv) in neural network models, the biasing mechanism in wide (and shallow) networks is different from the biasing mechanism in deep (and narrow) networks.
## [One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning](https://openreview.net/forum?id=K7KQkiHanD) ------> [PDF](https://openreview.net/pdf/82568381124eb845b124785fb2a09cfbc29d35ca.pdf)
#### Keywords:  Generalized Low-Rank Adaptation   Parameter-efficient Fine-tuning   Large VIsion Models   Large Language Models
  Abs:  We present Generalized LoRA (GLoRA), a flexible approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. 
Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured benchmarks in the vision field, achieving superior accuracy with fewer parameters and computations. Our models on LLaMA-1 and 2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications.
## [Universal Metric Learning with Parameter-Efficient Transfer Learning](https://openreview.net/forum?id=Jr3X3BTwsj) ------> [PDF](https://openreview.net/pdf/334b420aa8596f4ea1a9e615ea92a4130e040771.pdf)
#### Keywords:  deep metric learning   parameter-efficient transfer learning
  Abs:  A common practice in metric learning is to train and test an embedding model for each dataset. This dataset-specific approach fails to simulate real-world scenarios that involve multiple heterogeneous distributions of data. In this regard, we introduce a novel metric learning paradigm, called Universal Metric Learning (UML), which learns a unified distance metric capable of capturing relations across multiple data distributions. UML presents new challenges, such as imbalanced data distribution and bias towards dominant distributions. To address these challenges, we propose Parameter-efficient Universal Metric leArning (PUMA), which consists of a pre-trained frozen model and two additional modules, stochastic adapter and prompt pool. These modules enable to capture dataset-specific knowledge while avoiding bias towards dominant distributions. Additionally, we compile a new universal metric learning benchmark with a total of 8 different datasets. PUMA outperforms the state-of-the-art dataset-specific models while using about 69 times fewer trainable parameters.
## [Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning](https://openreview.net/forum?id=JnRStoIuTe) ------> [PDF](https://openreview.net/pdf/229efe054bf2cf41d6a2729948f844fb0a316af9.pdf)
#### Keywords:  data pruning   dataset distillation   random sampling   corset selection   data-efficient learning
  Abs:  Methods for carefully selecting or generating a small set of training data to learn from, i.e., data pruning, coreset selection, and dataset distillation, have been shown to be effective in reducing the ever-increasing cost of training neural networks. Behind this success are rigorously designed, yet expensive, strategies for identifying the most informative training examples out of large datasets. In this work, we revisit these methods to understand if the additional computational costs associated with such strategies are justified from the perspective of time-to-accuracy, which has become a critical efficiency measure of deep neural network training over large datasets. Surprisingly, we find that many of the recently proposed methods underperform what we call Repeated Sampling of Random Subsets (RSRS or RS2), a powerful yet overlooked extension of the standard random baseline that learns from repeatedly sampled data throughout training instead of a fixed random subset. We test RS2 against thirty-two state-of-the-art data pruning and distillation methods across four datasets including ImageNet. Our results demonstrate that RS2 significantly reduces time-to-accuracy, particularly in practical regimes where accuracy, but not runtime, is similar to that of training on full dataset. For example, when training ResNet-18 on ImageNet, with 10\% of the dataset each epoch RS2 reaches an accuracy of 66\% versus 69\% when training with the full dataset. The best competing method achieves only 55\% while training 1.6$\times$ slower than RS2. Beyond the above meta-study, we discuss the theoretical properties of RS2 such as its convergence rate and generalization error. Our primary goal is to highlight that future works that aim to minimize total training cost by using subset selection, need to consider 1) the total computation cost (including preparing the subset) and 2) should aim to outperform a simple extension of random sampling (i.e., RS2).
## [Bellman Optimal Step-size Straightening of Flow-Matching Models](https://openreview.net/forum?id=Iyve2ycvGZ) ------> [PDF](https://openreview.net/pdf/0600c5d57e385b527bab9b1bff3d970f9facd242.pdf)
#### Keywords:  flow matching   generative model   efficient sampling   distillation   responsible ML
  Abs:  Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in efficiency while maintaining competitive sample quality, effectively bridging the gap between low-resource constraints and the demanding requirements of flow-matching generative models. Our paper also fortifies the responsible development of artificial intelligence, offering a more sustainable generative model that reduces computational costs and environmental footprints.
## [Efficient and scalable reinforcement learning via hypermodel](https://openreview.net/forum?id=IiimxXqxNP) ------> [PDF](https://openreview.net/pdf/3163a21a4b3b9cc154951a49a68131713b2b97de.pdf)
#### Keywords:  Reinforcement Learning; exploration; sample efficiency; computation efficiency
  Abs:  Data-efficient reinforcement learning(RL) requires deep exploration.
    Thompson sampling is a principled method for deep exploration in reinforcement learning.
    However, Thompson sampling need to track the degree of uncertainty by maintaining the posterior distribution of models, which is computationally feasible only in simple environments with restrictive assumptions.
    A key problem in modern RL is how to develop data and computation efficient algorithm that is scalable to large-scale complex environments.
    We develop a principled framework, called HyperFQI, to tackle both the computation and data efficiency issues.
    HyperFQI can be regarded as approximate Thompson sampling for reinforcement learning based on hypermodel. Hypermodel in this context serves as the role for uncertainty estimation of action-value function.
    HyperFQI demonstrates its ability for efficient and scalable deep exploration in DeepSea benchmark with large state space.
    HyperFQI also achieves super-human performance in Atari benchmark with 2M interactions with low computation costs.
    We also give a rigorous performance analysis for the proposed method, justifying its computation and data efficiency.
    To the best of knowledge, this is the first principled RL algorithm that is provably efficient and also practically scalable to complex environments such as Arcade learning environment that requires deep networks for pixel-based control.
## [From Random to Relevant: Harnessing Salient Masks in Non-IID Federated Learning](https://openreview.net/forum?id=IQZuCuFeAM) ------> [PDF](https://openreview.net/pdf/adf931174a05d09b2875906daf4cc17237ebc1a6.pdf)
#### Keywords:  Sparsity   Pruning   Federated Learning   Sparse Federated Learning   Communication efficiency   Efficient FL   Pruning at Initialization
  Abs:  Federated learning (FL) offers the ability to train models using decentralized data at client sites, ensuring data privacy by eliminating the need for data centralization. A predominant challenge with FL is the constrained computation and narrow communication bandwidth, particularly evident in resource-restricted edge client nodes. Various solutions, such as transmitting sparse models and iterative pruning have been suggested to tackle this. However, many existing methods necessitate the transmission of full model weights throughout the training, rely heavily on arbitrary or random pruning criteria or costly iterative pruning schedules. 

In this work, we propose SSFL, a streamlined approach for sparse decentralized FL training and communication. SSFL identifies a subnetwork prior to training, leveraging parameter saliency scores keeping in mind the distribution of local client data in non-IID scenarios. Distinctively, only the sparse model weights are communicated in each round between client models in a decentralized manner, sidestepping the conventional need of transferring the complete dense model at any phase of training. We validate SSFL's effectiveness using standard non-IID benchmarks, noting marked improvements in the sparsity-accuracy trade-offs. Finally, we deploy our method in a real-world federated learning framework and report improvement in communication time.
## [Improving MLP Module in Vision Transformer](https://openreview.net/forum?id=I8pdQLfR77) ------> [PDF](https://openreview.net/pdf/ba57ffe5e62173f7d4f0c02d49036a74b56eff59.pdf)
#### Keywords:  vision transformer   MLP   efficient model design
  Abs:  Transformer models have been gaining substantial interest in the field of computer vision tasks nowadays. Although a vision transformer contains two important components which are self-attention module and multi-layer perceptron (MLP) module, the majority of research tends to concentrate on modifying the former while leaving the latter in its original form. In this paper, we focus on improving the MLP module within the vision transformer. Through theoretical analysis, we demonstrate that the effect of the MLP module primarily lies in providing non-linearity, whose degree corresponds to the hidden dimensions. Thus, the computational cost of the MLP module can be reduced by enhancing the degree of non-linearity in the nonlinear function. Leveraging this insight, we propose an improved MLP (IMLP) module for vision transformers which involves the usage of the arbitrary GeLU (AGeLU) function and integrating multiple instances of it to augment non-linearity so that the number of hidden dimensions can be effectively reduced. Besides, a spatial enhancement part is involved to further enrich the non-linearity in the proposed IMLP module. Experimental results show that we can apply our method to a wide range of state-of-the-art vision transformer models irrespective of how they modify their self-attention part and the overall architecture, and reduce FLOPs and parameters without compromising classification accuracy on the ImageNet dataset.
## [Memory Efficient Neural Processes via Constant Memory Attention Block](https://openreview.net/forum?id=I0gwsdSgsk) ------> [PDF](https://openreview.net/pdf/7db33d10d744d5a7d6679ed25bb9317f4a4e0236.pdf)
#### Keywords:  Attention   Neural Processes   Meta-learning   Efficiency
  Abs:  Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attention Block (CMAB),  a novel general-purpose attention block that (1) is permutation invariant, (2) computes its output in constant memory, and (3) performs updates in constant computation. Building on CMAB, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant which only requires **constant** memory. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks (meta-regression and image completion) while being significantly more memory efficient than prior methods.
## [Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once](https://openreview.net/forum?id=HsJzGWvg7K) ------> [PDF](https://openreview.net/pdf/ab42afe8e345e520b54a38d0d74e40094382e536.pdf)
#### Keywords:  Sparse co-training   pruning   efficient and flexible NN inferencing
  Abs:  Sparse Neural Networks (SNNs) have received voluminous attention for mitigating the explosion in computational costs and memory footprints of modern deep neural networks. Despite their popularity, most state-of-the-art training approaches seek to find a single high-quality sparse subnetwork with a preset sparsity pattern and ratio, making them inadequate to satiate platform and resource variability. Recently proposed approaches attempt to jointly train multiple subnetworks (we term as ``sparse co-training") with a \ul{fixed sparsity pattern}, to allow switching sparsity ratios subject to resource requirements. In this work, we take one more step forward and expand the scope of sparse co-training to cover \underline{diverse sparsity patterns} and \underline{multiple sparsity ratios} \textit{at once}. We introduce \textbf{Sparse Cocktail}, the \underline{first} sparse co-training framework that co-trains a suite of sparsity patterns simultaneously, loaded with multiple sparsity ratios which facilitate harmonious switch across various sparsity patterns and ratios at inference depending on the hardware availability. More specifically, Sparse Cocktail alternatively trains subnetworks generated from different sparsity patterns with a gradual increase in sparsity ratios across patterns and relies on an \textit{unified mask generation process} and the \textit{Dense Pivot Co-training}  to ensure the subnetworks of different patterns orchestrate their shared parameters without canceling each other’s performance. Experiment results on image classification, object detection and instance segmentation illustrate the favorable effectiveness and flexibility of Sparse Cocktail, pointing to a promising direction for sparse co-training. Codes will be released.
## [Oracle Efficient Algorithms for Groupwise Regret](https://openreview.net/forum?id=HrRKc9ei7h) ------> [PDF](https://openreview.net/pdf/b571ab9c582da868a9f946503a78929e4024dfdb.pdf)
#### Keywords:  Oracle efficiency   online learning   groupwise regret   sleeping experts
  Abs:  We study the problem of online prediction, in which at each time step $t \in \{1,2, \cdots T\}$, an individual $x_t$ arrives, whose label we must predict. Each individual is associated with various groups, defined based on their features such as age, sex, race etc., which may intersect. Our goal is to make predictions that have regret guarantees not just overall but also simultaneously on each sub-sequence comprised of the members of any single group. Previous work such as  [Blum & Lykouris][1] and [Lee et al][2] provide attractive regret guarantees for these problems; however, these are computationally intractable on large model classes (e.g., the set of all linear models, as used in linear regression). We show that a simple modification of the sleeping experts technique of [Blum & Lykouris][1] yields an efficient *reduction* to the well-understood problem of obtaining diminishing external regret *absent group considerations*. 
Our approach gives similar regret guarantees compared to [Blum & Lykouris][1]; however, we run in time linear in the number of groups, and are oracle-efficient in the hypothesis class. This in particular implies that our algorithm is efficient whenever the number of groups is  polynomially bounded and the external-regret problem can be solved efficiently, an improvement on [Blum & Lykouris][1]'s stronger condition that the model class must be small. Our approach can handle online linear regression and online combinatorial optimization problems like online shortest paths. Beyond providing theoretical regret bounds, we evaluate this algorithm with an extensive set of experiments on synthetic data and on two real data sets --- Medical costs and the Adult income dataset, both instantiated with intersecting groups defined in terms of race, sex, and other demographic characteristics. 
We find that uniformly across groups, our algorithm gives substantial error improvements compared to running a standard online linear regression algorithm with no groupwise regret guarantees.
## [Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models](https://openreview.net/forum?id=HnVtsfyvap) ------> [PDF](https://openreview.net/pdf/aef8aa51d377ddce7656d8d30bcdc17e72b61658.pdf)
#### Keywords:  Foundation models   Knowledge Distillation   Label-efficiency
  Abs:  Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive perform on various downstream tasks, especially with limited labeled target data. However, due to their high memory and compute requirements, these models cannot be deployed in resource constrained settings. This raises an important question: How can we utilize the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data? In this work, we answer this question by proposing a simple yet highly effective task-oriented knowledge transfer approach to leverage pretrained VFMs for effective training of small task-specific models. Our experimental results on three target tasks under limited labeled data settings show that the proposed knowledge transfer approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining and supervised ImageNet pretraining approaches by 1-10.5%, 2-21%, and 2-14%, respectively. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and propose a retrieval-based approach to curate effective transfer sets.
## [Tree-based Action-Manipulation Attack Against Continuous Reinforcement Learning with Provably Efficient Support](https://openreview.net/forum?id=HZnnHDrBXD) ------> [PDF](https://openreview.net/pdf/a7573380c0f79cff38ecfa6b49087186afb4fb3a.pdf)
#### Keywords:  reinforcement learning security   adversarial   provably efficient
  Abs:  Due to the widespread application of reinforcement learning, research on its adversarial attacks is necessary for building secure reinforcement learning applications. However, most of the current security research focuses only on reinforcement learning with discrete states and actions, and these methods cannot be directly applied to reinforcement learning in continuous state and action spaces. In this paper, we investigate attacks on continuous reinforcement learning. Rather than manipulating observations or environments, our focus lies in action-manipulation attacks that impose more restrictions on the attacker. Our study investigates the action-manipulation attack in both white-box and black-box scenarios. We propose a black-box attack method called LCBT, which uses a layered binary tree structure-based refinement and segmentation method to handle continuous action spaces. Additionally, we prove that under the condition of a sublinear relationship between the dynamic regret and total step counts of the reinforcement learning agent, LCBT can force the agent to frequently take actions according to specified policies with only sublinear attack cost. We conduct experiments to evaluate the effectiveness of the LCBT attack on three widely-used reinforcement learning algorithms: DDPG, PPO, and TD3.
## [Principled Architecture-aware Scaling of Hyperparameters](https://openreview.net/forum?id=HZndRcfyNI) ------> [PDF](https://openreview.net/pdf/722b88056197329f897498ba4f2f86a6df9bf2bc.pdf)
#### Keywords:  Hyperparameter Transfer   Neural Network Architecture   Neural Network Initialization   Learning Rate
  Abs:  Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs of principles or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologies. We verify our principles with comprehensive experiments. More importantly, our strategy further sheds light on advancing current benchmarks for architecture design. A fair comparison of AutoML algorithms requires accurate network rankings. However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization.
## [ShareFormer: Share Attention for Efficient Image Restoration](https://openreview.net/forum?id=HXZK1Z8tHa) ------> [PDF](https://openreview.net/pdf/0b28555143ae48c5aa6872fe2fa0e2c098dda1d5.pdf)
#### Keywords:  Efficient Image Restoration   Image Super Resolution   Image Denoising   Self Attention   Transformer
  Abs:  Transformer-based networks are gaining popularity due to their superior ability to handle long-range information. However, they come with significant drawbacks, such as long inference time, and challenging training processes. These limitations become even more pronounced when performing high-resolution image restoration tasks. We have noticed that there is a trade-off between models' latency time and their trainability. Including a convolutional module can improve the networks' trainability but not reduce their latency. Conversely, sparsification notably reduces latency but renders networks harder to optimize. To address these issues, a novel Transformer for image restoration called ShareFormer is proposed here. ShareFormer offers optimal performance with lower latency and better trainability than other Transformer-based methods. It achieves this by facilitating the sharing of the attention maps amongst neighboring blocks in the network, thereby considerably improving the inference speed. To maintain the model's information flow integrity, residual connections are added to the "Value" of self-attention. Several lesion studies indicate that incorporating residual connections on "Value" can aggregate the shallow transformers with shared attention, introducing a local inductive bias and making the network easier to optimize without the need for additional convolution. The effectiveness, efficiency, and easy-to-train of our ShareFormer is supported by numerous experimental results. Our code and pre-trained models will be open-sourced upon publication of the paper.
## [Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://openreview.net/forum?id=H3IUunLy8s) ------> [PDF](https://openreview.net/pdf/ba0d71f824b2c3d8ccd0f8a770755b7548a118ed.pdf)
#### Keywords:  Parameter-efficient Fine-tuning   Model Capacity
  Abs:  Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has become the prevailing approach for downstream tasks. While parameter-efficient fine-tuning methods have been proposed and proven effective without retraining all model parameters, their performance is limited by the capacity of incremental modules, especially under constrained parameter budgets.
To overcome this challenge, we propose CAPABOOST, a simple yet effective strategy that enhances model capacity by leveraging low-rank updates through parallel weight modules in target layers. By applying static random masks to the shared weight matrix, CAPABOOST constructs a diverse set of weight matrices, effectively increasing the rank of incremental weights without adding parameters. Notably, our approach can be seamlessly integrated into various existing parameter-efficient fine-tuning methods. We extensively validate the efficacy of CAPABOOST through experiments on diverse downstream tasks, including natural language understanding, question answering, and image classification. Our results demonstrate significant improvements over baselines, without incurring additional computation
or storage costs. We will make our code and benchmark publicly available.
## [FedPop: Federated Population-based Hyperparameter Tuning](https://openreview.net/forum?id=GzAk5WmCYP) ------> [PDF](https://openreview.net/pdf/56cfc603f2f338a751f1bb54946adfaf31a6e006.pdf)
#### Keywords:  Federated Learning   Hyperparameter Tuning   Evolutionary Methods
  Abs:  Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both the client and server sides. Compared with prior tuning methods, FedPop employs an online  "tuning-while-training" framework, offering computational efficiency and enabling the exploration of a broader HP search space. Our empirical validation on the common FL benchmarks and complex real-world FL datasets, including full-sized Non-IID ImageNet-1K, demonstrates the effectiveness of the proposed method, which substantially outperforms the concurrent state-of-the-art HP tuning methods in FL.
## [Addressing Sample Inefficiency in Multi-View Representation Learning](https://openreview.net/forum?id=GwBTlCIGs5) ------> [PDF](https://openreview.net/pdf/59592bf36a0318cd4f3d12560b03b416c321491b.pdf)
#### Keywords:  representation learning   self-supervised learning   eigenfunctions   data-augmentation   learning dynamics   sample efficient
  Abs:  Non-contrastive self-supervised learning (NC-SSL) methods like BarlowTwins and VICReg have shown great promise for label-free representation learning in computer vision. Despite the apparent simplicity of these techniques, researchers must rely on several empirical heuristics to achieve competitive performance, most notably using high-dimensional projector heads and two augmentations of the same image. In this work, we provide theoretical insights on the implicit bias of the BarlowTwins and VICReg loss that can explain these heuristics and guide the development of more principled recommendations. Our first insight is that the orthogonality of the features is more important than projector dimensionality for learning good representations. Based on this, we empirically demonstrate that low-dimensional projector heads are sufficient with appropriate regularization, contrary to the existing heuristic. Our second theoretical insight suggests that using multiple data augmentations better represents the desiderata of the SSL objective. Based on this, we demonstrate that leveraging more augmentations per sample improves representation quality and trainability. In particular, it improves optimization convergence, leading to better features emerging earlier in the training. Remarkably, we demonstrate that we can reduce the pretraining dataset size by up to 4x while maintaining accuracy and improving convergence simply by using more data augmentations. Combining these insights, we present practical pretraining recommendations that improve wall-clock time by 2x and improve performance on CIFAR-10/STL-10 datasets using a ResNet-50 backbone. Thus, this work provides a theoretical insight into NC-SSL and produces practical recommendations for improving its sample and compute efficiency.
## [REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes](https://openreview.net/forum?id=Gf15GsnfTy) ------> [PDF](https://openreview.net/pdf/28e68cec379f09d88eab2cb1775c81ab4770eb1f.pdf)
#### Keywords:  reinforcement learning   learning efficiency   ensembles   value-decomposition
  Abs:  Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, REValueD, tested on discretised versions of the DeepMind Control Suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing REValueD's performance, evaluating the significance of the regularisation loss and the scalability of REValueD with increasing sub-actions per dimension.
## [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](https://openreview.net/forum?id=Gdm87rRjep) ------> [PDF](https://openreview.net/pdf/d790d7128bdd17f1dcc9262a97a9d15489b10d21.pdf)
#### Keywords:  Large Language Models   Efficiency   Prompt
  Abs:  While the numerous parameters in Large Language Models (LLMs) contribute to their superior performance, this massive scale makes them inefficient and memory-hungry. Thus, they are hard to deploy on the commodity hardware, such as one single GPU.
Given the memory and power constraints of such devices, model compression methods are widely employed to reduce the model size and inference latency, which essentially trades off model quality in return for improved efficiency.
Thus, optimizing this accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware.
In this paper, we introduce a new perspective to optimize this trade-off by prompting compressed models.
Specifically, we first observe that for certain questions, the generation quality of a compressed LLM can be significantly improved by adding carefully designed hard prompts, though this isn't the case for all questions.
Based on this observation, we propose a soft prompt learning method where we expose the compressed model to the prompt learning process, aiming to enhance the performance of prompts.
Our experimental analysis suggests our soft prompt strategy greatly improves the performance of the $8\times$ compressed Llama-7B model (with a joint 4-bit quantization and 50\% weight pruning compression), allowing them to match their uncompressed counterparts on popular benchmarks.
Moreover, we demonstrate that these learned prompts can be transferred across various datasets, tasks, and compression levels.
Hence with this transferability, we can stitch the soft prompt to a newly compressed model to improve the test-time accuracy in an ``in-situ'' way.
## [Parameter-Efficient Long-Tailed Recognition](https://openreview.net/forum?id=GT57SN8xt9) ------> [PDF](https://openreview.net/pdf/7d2418443b9049dbbcd80bea538efb7a69042beb.pdf)
#### Keywords:  long-tailed recognition   class-imbalanced learning   parameter-efficient fine-tuning
  Abs:  The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to expedite convergence, PEL presents a novel semantic-aware classifier initialization technique derived from the CLIP textual encoder without adding any computational overhead. Our experimental results on four long-tailed datasets demonstrate that PEL consistently outperforms previous state-of-the-art approaches. The source code is available in the supplementary material.
## [Hybrid Reinforcement Learning for Optimizing Pump Sustainability in Real-World Water Distribution Networks](https://openreview.net/forum?id=GGo5eMv45h) ------> [PDF](https://openreview.net/pdf/371611c85105fe4657c914c32af9be37d3e4e982.pdf)
#### Keywords:  Reinforcement Learning   Pump Optimization   Hybrid RL   Operational Efficiency   Sustainability   Water Distribution Networks
  Abs:  This article addresses the pump-scheduling optimization problem to enhance real-time control of real-world water distribution networks (WDNs). Our primary objectives are to adhere to physical operational constraints while reducing energy consumption and operational costs. Traditional optimization techniques, such as evolution-based and genetic algorithms, often fall short due to their lack of convergence guarantees. Conversely, reinforcement learning (RL) stands out for its adaptability to uncertainties and reduced inference time, enabling real-time responsiveness. However, the effective implementation of RL is contingent on building accurate simulation models for WDNs, and prior applications have been limited by errors in simulation training data. These errors can potentially cause the RL agent to learn misleading patterns and actions and recommend suboptimal operational strategies. To overcome these challenges, we present an improved "hybrid RL" methodology. This method integrates the benefits of RL while anchoring it in historical data, which serves as a baseline to incrementally introduce optimal control recommendations. By leveraging operational data as a foundation for the agent's actions, we enhance the explainability of the agent's actions, foster more robust recommendations, and minimize error. Our findings demonstrate that the hybrid RL agent can significantly improve sustainability, operational efficiency, and dynamically adapt to emerging scenarios in real-world WDNs.
## [Pre-Training and Fine-Tuning Image Super-Resolution Models for Efficient Video Super-Resolution](https://openreview.net/forum?id=GDNo5oLpMx) ------> [PDF](https://openreview.net/pdf/bbf22217a6ae61508269df2fdc157cc31eccffbe.pdf)
#### Keywords:  Image Super-Resolution; Efficient Video Super-Resolution; Pre-Training and Fine-Tuning
  Abs:  In this paper, we propose a novel framework for adapting pre-trained image super-resolution (SR) models to tackle the challenging task of efficient video SR. This is achieved by freezing the pre-trained image SR model and fine-tuning it with the addition of several lightweight adapter modules. These adapters facilitate spatial and temporal learning, progressively equipping the image SR model with spatiotemporal reasoning capabilities for video SR. Also, these Adapters are compact and extendable, embedding only a few trainable parameters for each video dataset. Moreover, the parameters of the image SR model remain unchanged, resulting in substantial parameter sharing. This allows us to train video SR models quickly and efficiently. Remarkably, despite having significantly fewer parameters, our proposed method achieves competitive or even superior performance compared to existing video SR methods across multiple benchmarks.
## [Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning](https://openreview.net/forum?id=G1Hlubz1fR) ------> [PDF](https://openreview.net/pdf/571661a9cf69fc9e98d973aeb3e327b6fed7e2be.pdf)
#### Keywords:  Modular skill learning   Multi-task learning   Parameter-Efficient   Fine-Tuning
  Abs:  Modular skill learning is an emerging direction in the field of Parameter Efficient Fine-Tuning (PEFT), as it enables neural networks to better organize and clarify various aspects of knowledge, leading to improved knowledge transfer for new tasks. In this paper, we introduce a novel approach that categorizes skills into shared domain skills and specialized skills, with the skill parameters being highly parameterized using low-rank or sparse techniques. Each task is associated with an exclusive specialized skill while also benefiting from shared domain skills. Moreover, tasks can selectively utilize specialized skills from other tasks as needed. To facilitate this approach, we propose a skill assignment matrix that can be jointly learned, and the task network is instantiated based on the skill parameters. To evaluate the effectiveness of our approach, we conducted extensive experiments on the Super Natural Instructions and SuperGLUE datasets. Our results demonstrate that compared to fully-shared, task-specific, or skill-indistinguishable baselines. Modular learning with skill-type discrimination significantly enhances the sample efficiency of multi-task learning. Furthermore, the freezing of a substantial number of base model parameters greatly improves parameter efficiency, leading to boosted training efficiency.
## [A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning with General Function Approximation](https://openreview.net/forum?id=G1DoOVM3xZ) ------> [PDF](https://openreview.net/pdf/bab5182e53c3196b30460f116cf1d512fd03d178.pdf)
#### Keywords:  Reinforcement learning   sample efficiency   function approximation
  Abs:  The exploration-exploitation dilemma has been a central challenge in reinforcement learning (RL) with complex model classes. In this paper, we propose a new algorithm, Monotonic  Q-Learning with Upper Confidence Bound (MQL-UCB) for RL with general function approximation, where the Bellman operator of the underlying Markov decision process (MDP) is assumed to map any value functions into a function class with a bounded eluder dimension. Our key algorithmic design includes: (1) a general deterministic policy-switching strategy that achieves low switching cost, (2) a monotonic value function structure with carefully controlled function class complexity, and (3) a variance weighted regression scheme that exploits historical trajectories with high data efficiency. MQL-UCB achieves minimax optimal regret of $\tilde{O}(d\sqrt{HK})$ when $K$ is sufficiently large and near optimal policy switching cost of $\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$ being the planning horizon, and $K$ being the number of episodes. Our work sheds light on designing provably sample-efficient and deployment-efficient Q-learning with nonlinear function approximation.
## [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters](https://openreview.net/forum?id=FddFxi08J3) ------> [PDF](https://openreview.net/pdf/449e6346664ef46764bb3410fa95cb6d52ff3c04.pdf)
#### Keywords:  WL test   graph neural networks   graph motif parameters   subgraph counting
  Abs:  Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional 
Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test.
A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $p$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive problem: "graph motif parameters". In this paper, we provide a precise characterization of the WL-dimension of labeled graph motif parameters. As specific instances of this result, we obtain characterizations of the WL-dimension of the subgraph counting and induced subgraph counting problem for every labeled pattern $p$. Particularly noteworthy is our resolution of a problem left open in previous work 
concerning induced copies.
We additionally demonstrate that in cases where the $k$WL test distinguishes between graphs with varying occurrences of a pattern $p$, the exact number of occurrences of $p$ can be computed uniformly using only local information of the last layer of a corresponding GNN.
We finally delve into the challenge of recognizing the WL-dimension of various graph parameters. We give a polynomial time algorithm for determining the WL-dimension of the subgraph counting problem for given pattern $p$, answering an open question from previous work.
We additionally show how to utilize deep results from the field of graph motif parameters, together with our characterization, to determine the WL-dimension of induced subgraph counting and counting $k$-graphlets.
## [OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning](https://openreview.net/forum?id=FbuyDzZTPt) ------> [PDF](https://openreview.net/pdf/d4ba15c5a06090e287984fc46145d962ca91c1d8.pdf)
#### Keywords:  Rehearsal-free continual learning   Class-incremental learning   Parameter-efficient fine-tuning   Outlier regularization
  Abs:  Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods
for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones.
Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together.
In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier,
such that confusion of classes among different tasks is mitigated.
Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge
of previous tasks with that of the new task, leading to extra computation in querying and composing an
appropriate prompt from the pool.
This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper.
We illustrate that a simplified prompt-based method can achieve results comparable to
previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost.
Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting
those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks.
## [Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices](https://openreview.net/forum?id=FAO4VS9QRV) ------> [PDF](https://openreview.net/pdf/bc510b286dcdd6d1f9da21f8ee7f75fefa66828e.pdf)
#### Keywords:  Parameter-Efficient Fine-Tuning
  Abs:  In this paper, we present \textbf{Delta-LoRA}, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $A$ and $B$, but also propagate the learning to the pre-trained weights $W$ via updates utilizing the delta of the product of two low-rank matrices ($A^{(t+1)}B^{(t+1)} - A^{(t)}B^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $W$ does not need to compute the gradients of $W$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.
## [Bridge-TTS: Text-to-Speech Synthesis with Schrodinger Bridge](https://openreview.net/forum?id=F9ApWtHVac) ------> [PDF](https://openreview.net/pdf/88b87ec9afb06b2235794c583efbd6a3d33bec7a.pdf)
#### Keywords:  Diffusion Models   Schrodinger Bridge   Text-to-Speech Synthesis   High-Quality Generation   Efficient Sampling
  Abs:  In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality. However, with the pre-defined data-to-noise diffusion process, their prior distribution is restricted to a noisy representation, which provides little information of the generation target. In this work, we present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior in established diffusion-based TTS methods with a clean and deterministic one, which provides strong structural information of the target. Specifically, we leverage the latent representation obtained from text input as our prior, and build a fully tractable Schrodinger bridge (SB) between it and the ground-truth mel-spectrogram, leading to a faster generation process. Moreover, the tractability and flexibility of our proposed SB formulation allow us to empirically study the noise schedule and the model parameterization in training, as well as developing training-free stochastic and deterministic samplers with theory-grounded analyses of the bridge SDE and ODE, which further enrich our design spaces for exploring better generation performance. Experimental results on the LJ-Speech dataset illustrate the effectiveness of our method in terms of synthesis quality and sampling efficiency, outperforming the diffusion counterpart Grad-TTS in 50-step synthesis and strong fast TTS models in few-step scenario.
## [Few Heads are Enough](https://openreview.net/forum?id=EyDPfGy4Wh) ------> [PDF](https://openreview.net/pdf/e490346807d5560678cd1b42566dda8469148ebe.pdf)
#### Keywords:  transformers   attention   moe   mixture of experts   efficient transformers   language modelling
  Abs:  The costly self-attention layers in modern Transformers require memory and compute quadratic in sequence length. Existing approximation methods usually underperform and fail to obtain significant speedups in practice. The recently proposed Flash-Attention reduces both compute and memory through a *hardware*-aware implementation. Can we achieve this also through *algorithmic* improvements? Here we present Expert Projection Attention (EPA) - a novel method that reduces both compute and memory requirements, while matching the language modeling performance of baseline Transformers using the same parameter budget. EPA uses Mixture-of-Experts (MoE) layers for the value and output projections and requires 4 to 8 times fewer attention matrices than standard Transformers. Our novel attention can also be combined with MoE MLP layers, resulting in an efficient "Fast Transformer".
## [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://openreview.net/forum?id=EvDeiLv7qc) ------> [PDF](https://openreview.net/pdf/d1bff90017e6209ef7709c4d4eeeb0baaa4ddbec.pdf)
#### Keywords:  Mixture of Experts   Parameter-Efficient Fine-Tuning
  Abs:  The Mixture of Experts (MoE) is a widely known neural architecture where  an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional MoEs pose challenges at scale due to the need to store all experts in memory. In this paper, we push MoE to the limit. We propose extremely parameter-efficient MoE by uniquely combining MoE architecture with lightweight experts.Our MoE architecture outperforms standard parameter-efficient fine-tuning (PEFT) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1\% of an 11B parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints.
## [Searching for Parameter-Efficient Tuning Architecture for Text-to-image Diffusion Models](https://openreview.net/forum?id=EmuZOYfPNf) ------> [PDF](https://openreview.net/pdf/acb2cf4e984adf51c7e75e98936b31fede611be1.pdf)
#### Keywords:  architecture search   parameter-efficient tuning   image generation
  Abs:  The large-scale text-to-image diffusion model, represented by Stable Diffusion, has achieved remarkable success in the field of image generation.  Transferring pretrained diffusion models to downstream domains with parameter-efficient tuning (PEFT)  methods such as Adapter and LoRa have become the most common paradigms. Despite their widespread usage, there has been limited research on systematically studying how the design of these components would impact the final tuning effectiveness.
In this paper, we investigate the automatic design of an optimal tuning architecture. Specifically, we employ a reinforcement learning-based neural network search method to facilitate the automatic design of the tuning architecture for PEFT of Stable Diffusion with few-shot training data. Our search space includes micro-structures similar to Adapter, LoRa, as well as their insertion positions. 
For effective searching and evaluation, we build a large-scale tuning dataset. Through our search, we successfully obtained a novel tuning architecture that reduces parameter count by 18\% compared to the widely adopted LoRa approach but still surpasses across various downstream tasks hugely.   We also conduct extensive analysis of the searched results, aiming to provide valuable insights to the community regarding parameter-efficient tuning for large-scale diffusion models.
## [Private Overparameterized Linear Regression without Suffering in High Dimensions](https://openreview.net/forum?id=DxM73sxtna) ------> [PDF](https://openreview.net/pdf/4b48c959a7a4ed289b80f35b3b3f4aa5ea04f8d4.pdf)
#### Keywords:  Differential Privacy   Overparameterization   Linear Regression   Optimization
  Abs:  This study focuses on differentially private linear regression in the over-parameterized regime. We propose a new variant of the differentially private Follow-The-Regularized-Leader (DP-FTRL) algorithm that uses a random noise with a general covariance matrix for differential privacy. This leads to improved privacy and utility (excess risk) trade-offs. Firstly, even when reduced to an existing DP-FTRL algorithm that uses an isotropic noise, our excess risk bound is sharper as a function of the eigenspectrum of the data covariance matrix and the ground truth model parameter. Furthermore, when unlabeled public data is available, we can design a better noise covariance matrix structure to improve the utility. For example, when the ground truth has a bounded $\ell_2$-norm, and the eigenspectrum decays polynomially (i.e., $\lambda_i=i^{-r}$ for $r>1$), our method achieves $\mathcal{\tilde O}(N^{-\frac{r}{1+2r}})$ and $\mathcal{\tilde O}(N^{-\frac{r}{3+r}\wedge\frac{2r}{1+3r}})$ excess error for identity and specially designed covariance matrices, respectively. Notably, our method with a specially designed covariance matrix outperforms the one with an identity matrix when the eigenspectrum decays at least quadratically fast, i.e., $r\geq 2$. Our proposed method significantly improves upon existing differentially private methods for linear regression, which tend to scale with the problem dimension, leading to a vacuous guarantee in the over-parameterized regime.
## [SpaFL: Communication-Efficient Federated Learning with Sparse Models and Low Computational Overhead](https://openreview.net/forum?id=DZyhUXpEee) ------> [PDF](https://openreview.net/pdf/e61143e17336dcec04e213ec85fffbf4933f68cd.pdf)
#### Keywords:  Federated Learning   Communication efficiency   Sparse training   Computational overhead
  Abs:  The large communication and computation overhead of federated learning (FL) is one of the main challenges facing its practical deployment over resource-constrained clients and systems. In this work, SpaFL: a communication-efficient FL framework is proposed to optimize both personalized model parameters and sparse model structures  with low computational overhead. In SpaFL, a trainable threshold is defined for each neuron/filter to prune its connected parameters. Both model parameters and thresholds are jointly optimized to enable the automatic sparsification of the models while recovering prematurely pruned parameters during training. To reduce communication costs, only thresholds are communicated between a server and clients instead of parameters, thereby enabling the clients to learn how to prune. Further, global thresholds are used to update model parameters by extracting aggregated parameter importance. The convergence of SpaFL is analyzed, and the results provide new insights into the tradeoff between computation overhead and learning performance. Experimental results show that SpaFL improves accuracy while requiring much less communication and computing resources compared to both dense and sparse personalized baselines.
## [HiddenKey: Parameter-Efficient FineTuning Meets Dropout under a Unified Framework](https://openreview.net/forum?id=DZBpVcc2Xc) ------> [PDF](https://openreview.net/pdf/ded35493034dc1a4f93ddae20f3394fc5cdb8cf2.pdf)
#### Keywords:  Dropout   LoRA   Parameter-Efficient FineTuning   NLU   NLG   NLP
  Abs:  The emerging powerful capabilities exhibited by large language models (LLMs) have established them as a fundamental element in various applications that rely on advanced language understanding. At the same time, fine-tuning has become the standard learning approach to adapting LLMs to a concrete application (e.g., instruction tuning, alignment tuning, and task/user-specific specialization). Due to the high cost associated with full finetuning, parameter-efficient finetuning (PEFT) methods, especially LoRA, have gained popularity due to their lower storage, memory, and computation requirements. However, the possible contradiction between limited trainable parameters and the dropout regularization methods (which aim at alleviating overfitting associated with excessive parameter redundancy), has been largely overlooked. With extensive experiments of LoRA-based PEFT, we first confirm that PEFT is also overfitting-prone. We then revisit transformer-specific dropout methods, and validate their equivalence and differences mathematically and empirically. To facilitate a comprehensive comparison, we introduce a unified framework to instantiate them along dropping position, structural pattern and compensation measure, and uncover their new preferences and performance comparisons in PEFT scenarios. This framework also enables us to integrate the best of all into a new dropout method named HiddenKey, which shows performance superiority over existing methods on both NLU and NLG tasks. Compared to baselines, it also achieves better performance with less finetuning time, and offers continuous improvement with further finetuning. These highlight HiddenKey as the better practice for high-performance and parameter-efficient finetuning of LLMs.
## [Efficient Large Language Models Fine-Tuning on Graphs](https://openreview.net/forum?id=DVA0NDUdCQ) ------> [PDF](https://openreview.net/pdf/61a9523218ffd49172508d4eaf12578e0aa481a4.pdf)
#### Keywords:  Graph Neural Networks   Large Language Models   Scalability   Label Efficiency
  Abs:  Learning from Text-Attributed Graphs (TAGs) has attracted significant attention due to its wide range of real-world applications. The rapid evolution of large language models (LLMs) has revolutionized the way we process textual data, which indicates a strong potential to replace shallow text embedding generally used in Graph Neural Networks (GNNs). However, we find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. In this work, we introduce a novel and efficient approach for the end-to- end fine-tuning of Large Language Models (LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover, it transfers the rick knowledge in LLMs to downstream graph learning tasks effectively with limited labeled data in semi-supervised learning. Its superior computation and data efficiency are demonstrated through comprehensive experiments, offering a promising solution for a wide range of LLMs and graph learning tasks on TAGs.
## [ReLoRA: High-Rank Training Through Low-Rank Updates](https://openreview.net/forum?id=DLJznSp6X3) ------> [PDF](https://openreview.net/pdf/d6c927c0ff55ef2336db2a0f2d40006856b5d5a3.pdf)
#### Keywords:  language models   pre-training   training efficiency   parameter-efficient fine-tuning   lora
  Abs:  Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, while training costs grow exponentially. In this paper, we explore parameter-efficient training techniques as an approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to training transformer language models with up to 1.3B parameters and demonstrate comparable performance to regular neural network training. ReLoRA saves up to 5.5Gb of RAM per GPU and improves training speed by 9-40% depending on the model size and hardware setup. Our findings show the potential of parameter-efficient techniques for large-scale pre-training.
## [Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks](https://openreview.net/forum?id=CwAY8b8i97) ------> [PDF](https://openreview.net/pdf/221f8faa7e7a55d673c447e3946d5910b21640b9.pdf)
#### Keywords:  Spiking Neural Networks   Efficient SNN Training   Deep Learning   Supervised Learning
  Abs:  In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the above contents through experiments and showed that it is possible to reduce memory and training time while maintaining accuracy.
## [Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities](https://openreview.net/forum?id=CppEmee0u6) ------> [PDF](https://openreview.net/pdf/b53b13c96e8dbb76cb94533d7bd5ad1773bc7030.pdf)
#### Keywords:  Transformer   Multimodal Learning   Pretraining   Re-parameterization
  Abs:  We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway: given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model via a proposed method named Cross-Modal Re-parameterization, which exploits the auxiliary weights without any inference costs. We observe significant and consistent performance improvements with irrelevant data of image, point cloud, video, and audio. For example, on ImageNet-1K, a point-cloud-trained auxiliary transformer can improve an MAE-pretrained ViT by 0.6\% and a ViT trained from scratch by 5.4\%. The code and models will be publicly available.
## [DECOUPLING REASONING FROM OBSERVATIONS FOR EFFICIENT AUGMENTED LANGUAGE MODELS](https://openreview.net/forum?id=CpgoO6j6W1) ------> [PDF](https://openreview.net/pdf/73cdd90ef10de8fc6c45bacc06477bfdf23f5d5c.pdf)
#### Keywords:  Tool augmented language model   Efficiency   Prompt redundancy   Instruction fine-tuning
  Abs:  Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model sizes. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA successfully, demonstrating the significant potential for truly efficient and scalable ALM systems. Full code, model, and curated data are released for reproduction.
## [Bit Cipher — A Simple yet Powerful Word Representation System that Integrates Efficiently with Language-Models](https://openreview.net/forum?id=BqJamqGwp1) ------> [PDF](https://openreview.net/pdf/ab14bd2231a1f804e2c5bf26bb26efb61153e7df.pdf)
#### Keywords:  representation learning   word embedding   efficient NLP   language language model training   fine-tuning
  Abs:  While Large Language Models (LLMs) become ever more dominant, classic pre-trained word embeddings sustain their relevance through computational efficiency and nuanced linguistic interpretation. Drawing from recent studies demonstrating that the convergence of GloVe and word2vec optimizations _all_ tend towards log-co-occurrence matrix variants, we construct a novel word representation system called  _**Bit-cipher**_ that eliminates the need of backpropagation while leveraging contextual information and hyper-efficient dimensionality reduction techniques based on unigram frequency, providing strong interpretability, alongside efficiency. 
We use the bit-cipher algorithm to train word vectors via a two-step process that critically relies on a hyperparameter---_bits_---that controls the vector dimension. While the first step trains the bit-cipher, the second utilizes it under two different aggregation modes---_summation_ or _concatenation_---to produce contextually rich representations from word co-occurrences. 
We extend our investigation into bit-cipher's efficacy, performing probing experiments on part-of-speech (POS) tagging and named entity recognition (NER) to assess its competitiveness with classic embeddings like word2vec and GloVe. Additionally, we explore its applicability in LM training and fine-tuning. By replacing embedding layers with cipher embeddings, our experiments illustrate the notable efficiency of cipher in accelerating the training process and attaining better optima compared to conventional training paradigms. In fine-tuning experiments, training cipher embeddings on target datasets and replacing the embedding layer of the LMs to be fine-tuned negates the need for extensive model adjustments, offering a highly efficient transfer learning alternative. Experiments on the integration of bit-cipher embedding layers with Roberta, T5, and OPT, prior to or as a substitute for fine-tuning, showcase a promising enhancement to transfer learning, allowing rapid model convergence while preserving competitive performance.
## [Subgraph-To-Node Translation for Efficient Representation Learning of Subgraphs](https://openreview.net/forum?id=BeuTCoe3bf) ------> [PDF](https://openreview.net/pdf/6a4a5fc5094f8983610c03a9115e08da799f01c3.pdf)
#### Keywords:  Efficiency   Subgraphs   Graph Neural Networks
  Abs:  Subgraph representation learning has emerged as an important problem, but it is by default approached with the graph neural networks (GNNs) on a large global graph, an approach that demands extensive memory and computational resources. We argue that resource requirements can be reduced by designing an efficient data structure to store and process subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation to learn representations of subgraphs efficiently. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. We theoretically and empirically show that S2N significantly reduces memory and computational costs compared to using state-of-the-art models with conventional data structures. We also suggest Coarsened S2N (CoS2N), which combines S2N with graph coarsening methods for improved results in a data-scarce setting where there are not sufficient subgraphs to cover the global graph. Our experiments on four real-world benchmarks demonstrate that fined-tuned models with S2N translation can process 183 -- 711 times more subgraph samples than state-of-the-art models at a similar or better performance level.
## [PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought](https://openreview.net/forum?id=B4E2BW27MP) ------> [PDF](https://openreview.net/pdf/1b7c8e6cc2947b6fa0d95bc736610e1114843fd0.pdf)
#### Keywords:  text-to-image generative models   prompt engineering   Chain of Thought   parameter efficient adaptation   Large Language Models
  Abs:  Diffusion-based generative models have exhibited remarkable capability in the production of high-fidelity visual content such as images and videos. However, their performance is significantly contingent upon the quality of textual inputs, commonly referred to as "prompts".
  The process of traditional prompt engineering, while effective, necessitates empirical expertise and poses challenges for inexperienced users.
  In this paper, we introduce PromptCoT, an innovative enhancer that autonomously refines prompts for users.
  The design of PromptCoT is based on the observation that, prompts resembling textual information corresponding to high-quality images within the training set tend to yield superior generation performance.
  As such, we fine-tune the pre-trained Large Language Models (LLM) using a curated text dataset comprising solely of high-quality visual content descriptions. By doing so, the LLM becomes capable of capturing the distribution of high-quality training texts, enabling it to generate aligned continuations and revisions to boost the original texts.
  Nonetheless, one drawback of pre-trained LLMs is their tendency to generate extraneous or irrelevant information. To enhance the alignment between the original text prompts and the refined counterparts, we leverage the Chain-of-Thought (CoT) mechanism. CoT can extract and amalgamate crucial information from the aligned continuation and revision, enabling reasonable inferences based on the contextual cues to produce a more comprehensive and nuanced final output. 
  Considering computational efficiency, instead of allocating a dedicated LLM for prompt enhancement to each individual model or dataset, we integrate adapters that facilitate dataset-specific adaptation, leveraging a shared pre-trained LLM as the foundation for this process. 
  By fine-tuning these adapters independently, we can adapt PromptCoT to new datasets with minimal increase in training cost and memory usage.
  We assess the performance of PromptCoT on widely-used latent diffusion models for image and video generation to validate the effectiveness. The results demonstrate significant improvements in key performance metrics.
## [FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis](https://openreview.net/forum?id=ArpwmicoYW) ------> [PDF](https://openreview.net/pdf/63ffe7eaf91aa4b300787c71561f52918545b4cf.pdf)
#### Keywords:  Fairness   PEFT   Hyperparameter Optimization   Medical Imaging
  Abs:  Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across sub-groups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between updating more parameters, enabling a better fit to the task of interest vs. fewer parameters, potentially reducing the generalisation gap. To manage this tradeoff, we propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness. We demonstrate empirically that FairTune leads to improved fairness on a range of medical imaging datasets.
## [Risk Bounds of Accelerated SGD for Overparameterized Linear Regression](https://openreview.net/forum?id=AcoXPIPh4A) ------> [PDF](https://openreview.net/pdf/2cb46a1ccdd0326144742962e7cc0f063738e48b.pdf)
#### Keywords:  Accelerated stochastic gradient descent   excess risk   linear regression   overparameterization
  Abs:  Accelerated stochastic gradient descent (ASGD) is a workhorse in deep learning and often achieves better generalization performance than SGD. However, existing optimization theory can only explain the faster convergence of ASGD, but cannot explain its better generalization. In this paper, we study the generalization of ASGD for overparameterized linear regression, which is possibly the simplest setting of learning with overparameterization. We establish an instance-dependent excess risk bound for ASGD within each eigen-subspace of the data covariance matrix. Our analysis shows that (i) ASGD outperforms SGD in the subspace of small eigenvalues, exhibiting a faster rate of exponential decay for bias error, while in the subspace of large eigenvalues, its bias error decays slower than SGD; and (ii) the variance error of ASGD is always larger than that of SGD. Our result suggests that ASGD can outperform SGD when the difference between the initialization and the true weight vector is mostly confined to the subspace of small eigenvalues. Additionally, when our analysis is specialized to linear regression in the strongly convex setting, it yields a tighter bound for bias error than the best-known result.
## [Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems](https://openreview.net/forum?id=ADDCErFzev) ------> [PDF](https://openreview.net/pdf/eb4307b47ce6390c45e1d03c0c1f2124ceb4b384.pdf)
#### Keywords:  Efficient coding   object representation   dropout   robustness   human fMRI   occipitotemporal cortex   cognitive neuroscience   distributed coding
  Abs:  According to the efficient coding hypothesis, neural populations encode information optimally when representations are high-dimensional and uncorrelated. However, such codes may carry a cost in terms of generalization and robustness. Past empirical studies of early visual cortex (V1) in rodents have suggested that this tradeoff indeed constrains sensory representations. However, it remains unclear whether these insights generalize across the hierarchy of the human visual system, and particularly to object representations in high-level occipitotemporal cortex (OTC). To gain new empirical clarity, here we develop a family of object recognition models with parametrically varying dropout proportion $p$, which induces systematically varying dimensionality of internal responses (while controlling all other inductive biases). We find that increasing dropout produces an increasingly smooth, low-dimensional representational space. Optimal robustness to lesioning is observed at around 70% dropout, after which both accuracy and robustness decline. Representational comparison to large-scale 7T fMRI data from occipitotemporal cortex in the Natural Scenes Dataset reveals that this optimal degree of dropout is also associated with maximal emergent neural predictivity. Finally, using new techniques for achieving denoised estimates of the eigenspectrum of human fMRI responses, we compare the rate of eigenspectrum decay between model and brain feature spaces. We observe that the match between model and brain representations is associated with a common balance between efficiency and robustness in the representational space. These results suggest that varying dropout may reveal an optimal point of balance between the efficiency of high-dimensional codes and the robustness of low dimensional codes in hierarchical vision systems.
## [On Double-Descent in Reinforcement Learning with LSTD and Random Features](https://openreview.net/forum?id=9RIbNmx984) ------> [PDF](https://openreview.net/pdf/015372f92f6a4d659e563b89d0da8c18782bca1f.pdf)
#### Keywords:  Regularized Least-Square Temporal Difference   double-descent   over-parameterization   random features
  Abs:  Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Square Bellman Error (MSBE) that feature correction terms responsible for the double-descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.
## [LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning](https://openreview.net/forum?id=9KVT1e1qf7) ------> [PDF](https://openreview.net/pdf/cbf590a9acf0c0106865b22f399166912819cea4.pdf)
#### Keywords:  Neural Network Pruning   Parameter Efficient Tuning
  Abs:  Large pre-trained models (LPMs), such as LLaMA and GLM, have shown exceptional performance across various tasks through fine-tuning. Although low-rank adaption (LoRA) has emerged to cheaply fine-tune these LPMs on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a way to compress LPMs. However, the current pruning methods designed for LPMs are not compatible with LoRA. This is due to their utilization of unstructured pruning on LPMs, impeding the merging of LoRA weights, or their dependence on the gradients of pre-trained weights to guide pruning, which can impose significant memory overhead.
To this end, we propose LoRAPrune, a new framework that delivers an accurate, compact model for efficient inference in a highly memory-effective manner. Specifically, we first design a LoRA-guided pruning criterion, which uses the weights and gradients of LoRA, rather than the gradients of pre-trained weights for importance estimation. We then propose a structured iterative pruning procedure, to remove redundant channels and heads. 
Extensive experimental results demonstrate the superior performance of our LoRAPrune over existing approaches on the LLaMA series models.
For instance, at a 50\% compression rate, LoRAPrune outperforms LLM-Pruner by a perplexity reduction of 8.0 on WikiText2 and 16.05 on PTB datasets, while concurrently reducing memory usage by 52.6\%.
## [Constrained Parameter Regularization](https://openreview.net/forum?id=9GviaQcGnx) ------> [PDF](https://openreview.net/pdf/e886c15661e5d458e409818bfdfbc51e9f31ee1b.pdf)
#### Keywords:  Weight Decay   Parameter Regularization   Augmented Lagrangian   Deep Learning
  Abs:  In this work, we present constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L2-norm) of parameter groups. Consequently, learning becomes a constraint optimization problem, which we address by an adaptation of the augmented Lagrangian method. This formulation permits varying regularization strengths for each parameter group, eliminating the need for explicit penalty coefficients for regularization terms. CPR only requires two hyperparameters and incurs no measurable runtime overhead. Additionally, we propose a simple but efficient mechanism to adapt the upper bounds during the optimization. We provide empirical evidence of CPR's efficacy in experiments on the ``grokking'' phenomenon, computer vision, and language modeling tasks. Our results demonstrate that CPR counteracts the effects of grokking and consistently matches or outperforms traditional weight decay.
## [FedMef: Towards Memory-efficient Federated Dynamic Pruning](https://openreview.net/forum?id=9G2IVZIh4H) ------> [PDF](https://openreview.net/pdf/997acac1382064ab4c547385faca10dfe6623dc6.pdf)
#### Keywords:  federated learning   memory efficient training   nerual network pruning
  Abs:  Federated learning (FL) promotes decentralized training while prioritizing data confidentiality. However, its application on resource-constrained devices is challenging due to the high demand for computation and memory resources for training deep learning models. Neural network pruning techniques, such as dynamic pruning, could enhance model efficiency, but directly adopting them in FL still poses substantial challenges, including post-pruning performance degradation, high activation memory, etc. To address these challenges, we propose FedMef, a novel and memory-efficient federated dynamic pruning framework. FedMef comprises two key components. First, we introduce the budget-aware extrusion that maintains pruning efficiency while preserving post-pruning performance by salvaging crucial information from parameters marked for pruning within a given budget. Second, we propose scaled activation pruning to effectively reduce activation memory, which is particularly beneficial for deploying FL to memory-limited devices. Extensive experiments
demonstrate the effectiveness of our proposed FedMef. In particular, it achieves a significant reduction of 28.5\% in memory footprint compared to state-of-the-art methods while obtaining superior accuracy.
## [BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges](https://openreview.net/forum?id=9FXGX00iMF) ------> [PDF](https://openreview.net/pdf/997eed28816a456a625887866defe6b5207c8133.pdf)
#### Keywords:  Data subset selection   data pruning   data-efficient learning
  Abs:  Data subset selection aims to find a smaller yet informative subset of a large dataset that can approximate the full-dataset training, addressing challenges associated with training neural networks on large-scale datasets. However, existing methods tend to specialize in either high or low selection ratio regimes, lacking a universal approach that consistently achieves competitive performance across a broad range of selection ratios. We introduce a universal and efficient data subset selection method, Best Window Selection (BWS), by proposing a method to choose the best window subset from samples ordered based on their difficulty scores. This approach offers flexibility by allowing the choice of window intervals that span from easy to difficult samples. Furthermore, we provide an efficient mechanism for selecting the best window subset by evaluating its quality using kernel ridge regression. Our experimental results demonstrate the superior performance of BWS compared to other baselines across a broad range of selection ratios over datasets, including CIFAR-10/100 and ImageNet, and the scenarios involving training from random initialization or fine-tuning of pre-trained models.
## [LegoNet: Piecing Together and Breaking Apart Sub-Networks for Scalable Multi-task Learning](https://openreview.net/forum?id=8wFNfTxM6i) ------> [PDF](https://openreview.net/pdf/d0413092a0b5e73c254873b173efbcfb7abce57a.pdf)
#### Keywords:  multi-task learning; continous learning; efficient adaptation
  Abs:  Despite considerable progress in general-purpose vision models, most efforts focus on designing a new unified structure that can handle different types of input and supervision. In contrast, we believe each vision task requires its specific designed module to use different forms of perception. For example, a feature pyramid network is commonly used in segmentation but not in classification. We present LegoNet, a general Multi-Task Learning (MTL) framework that is assembled with many small sub-networks from different vision tasks, similar to how Lego pieces can be pieced together into larger structures. By leveraging this property, LegoNet can borrow design elements from single-task models and combine them to create a scalable multi-task model. We demonstrate its efficiency on mainstream vision datasets such as ImageNet, COCO, and ADE20K, and show it achieves comparable results to state-of-the-art single-task models. Moreover, like a Lego creation capable of dynamically piecing together or breaking apart pieces, our model exhibits scalability in both its model capacity and adaptability to a multitude of tasks. It can remove sub-networks and decompose into high-performing components for efficient adaptation, or add sub-networks for learning new tasks in a continuous learning scenario. On downstream tasks, it can be fine-tuned with fewer training parameters, fewer model parameters, and even transformed to a low computation shape. These functions can be controlled and combined to meet various demands of downstream applications.
## [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://openreview.net/forum?id=8dkp41et6U) ------> [PDF](https://openreview.net/pdf/fda32d0b399dd15091f036ba313b978622823dfe.pdf)
#### Keywords:  Prompt Compression   Long Context   LLMs   Black-box LLMs   Efficient Method
  Abs:  In long context scenarios, large language models (LLMs) face three main challenges: higher computational/financial cost, longer latency, and inferior performance. Some studies reveal that the performance of LLMs depends on both the density and the position of the key information (question relevant) in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs’ perception of the key information to simultaneously address the three challenges. We conduct evaluation on a wide range of long context scenarios including single-/multi-document QA, few-shot learning, summarization, synthetic tasks, and code completion. and experimental results show that LongLLMLingua compressed prompt can derive higher performance with much less cost. The latency of the end-to-end system is also reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance boost of up to 17.1% over the original prompt with ∼4x fewer tokens as input to GPT-3.5-Turbo. It can derive cost savings of `$`28.5 and `$`27.4 per 1,000 samples from the LongBench and ZeroScrolls benchmark, respectively. Additionally, when compressing prompts of ∼10k tokens at a compression rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
## [Neural Network Diffusion](https://openreview.net/forum?id=8Q6UmFhhQS) ------> [PDF](https://openreview.net/pdf/282375a796f9f2a4e5b89619437d51f053ee1404.pdf)
#### Keywords:  Parameter Generation   Diffusion Model
  Abs:  Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also generate high-performing neural network parameters. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts the latent representation of trained model parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder’s decoder, whose outputs are ready to use as new sets of network parameters. Across various tasks and datasets, our diffusion process consistently generates models of comparable or improved performance over SGD-trained models, with minimal additional cost. Our results encourage more exploration on the versatile use of diffusion models.
## [Fast-ELECTRA for Efficient Pre-training](https://openreview.net/forum?id=8OBuqbLb8h) ------> [PDF](https://openreview.net/pdf/ec83017cab49946fb5f76c893123986b79e319ea.pdf)
#### Keywords:  Language model Pre-training   ELECTRA   Efficiency
  Abs:  ELECTRA pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although ELECTRA offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose Fast-ELECTRA, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art ELECTRA-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability.
## [HashOrder: Accelerating Graph Processing Through Hashing-based Reordering](https://openreview.net/forum?id=8LBS1nixTJ) ------> [PDF](https://openreview.net/pdf/dc00caa6bd7a7292091b2cf52784464e4623d49c.pdf)
#### Keywords:  graph processing   graph reordering   efficiency   hashing
  Abs:  Graph processing systems are a fundamental tool across various domains such as machine learning, and their efficiency has become increasingly crucial due to the rapid growth in data volume. A major bottleneck in graph processing systems is poor cache utilization. Graph reordering techniques can mitigate this bottleneck and significantly speed up graph workloads by improving the data locality of the graph memory layout. However, since existing approaches use greedy algorithms or simple heuristics to find good orderings, they suffer from either high computational overhead or suboptimal ordering quality. To this end, we propose HashOrder, a probabilistic algorithm for graph reordering based on randomized hashing. We theoretically show that hashing-based orderings have quality guarantees under reasonable assumptions. HashOrder produces high-quality orderings while being lightweight and parallelizable. We empirically show that HashOrder beats the efficiency-quality tradeoff curve of existing algorithms. Evaluations on various graph processing workloads and GNN data loaders reveal that HashOrder is competitive with or outperforms the existing best method while being 592$\times$ more efficient in reordering, speeding up PageRank by up to 2.49$\times$ and GNN data loaders by up to 2.33$\times$.
## [MatFormer: Nested Transformer for Elastic Inference](https://openreview.net/forum?id=89XNDtqhpL) ------> [PDF](https://openreview.net/pdf/30d25bf6108016e820170143df66190249670a32.pdf)
#### Keywords:  Transformer   Neural Architecture Design   Large-scale deployment   Efficiency
  Abs:  Transformer models are deployed in a wide range of settings, from multi-accelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a  MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix'n'Match of model granularities across layers -- i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized.  We empirically demonstrate MatFormer's effectiveness across different model classes (decoders & encoders), modalities (language & vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from  MatFormer can further reduce inference latency.
## [Resource Efficient Test-Time Training with Slimmable Network](https://openreview.net/forum?id=7iuFxx9Ccx) ------> [PDF](https://openreview.net/pdf/f2c2abe2ecaebbda78fa65687bc46b6f2b64de66.pdf)
#### Keywords:  Test-Time Training   Resource Efficient   Slimmable Neural Network
  Abs:  Test-Time Training (TTT), an innovative paradigm for enhancing a model's generalization in a specific future scenario, commonly leverages self-supervised learning to adapt the model to the unlabeled test data under distribution shifts. However, previous TTT methods tend to disregard resource constraints during the deployment phase in real-world scenarios and have two fundamental shortcomings. Firstly, they are obligated to retrain adapted models when deploying across multiple devices with diverse resource limitations, causing considerable resource inefficiency. Secondly, they are incapable of coping with computational budget variations during the testing stage. To tackle these issues, we propose a resource-adaptive test-time training framework called SlimTTT, which allows for the seamless switching of different sub-networks for adaptive inference. Furthermore, we discover that different width of sub-networks can capture different views of images and these views are complementary and beneficial to the ones created by data augmentation, which is widely used in TTT. To utilize these views, we introduce Width-enhance Contrastive Learning (WCL), Logits Consistency Regularization (LCR) and Global Feature Alignment (GFA) to promote representation consistency at both feature and prediction space in a self-supervised manner, enabling networks of different widths to excel in TTT tasks. Our proposed method, SlimTTT, has achieved state-of-the-art (SOTA) results across a variety of adaptation methods and four different datasets with varying backbones. Remarkably, despite a significant reduction in computational complexity - over 70% less than the current SOTA method - SlimTTT continues to deliver competitive performance, rendering it highly conducive for adoption in practice.
## [Proper Laplacian Representation Learning](https://openreview.net/forum?id=7gLfQT52Nn) ------> [PDF](https://openreview.net/pdf/b34e5e1dd36b0b73e1611e5f84dc375f7048a65f.pdf)
#### Keywords:  Reinforcement learning   Graph Laplacian   Representation learning   Augmented Lagrangian optimization   Hyperparameter robustness
  Abs:  The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems, by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning techniques. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees of our method and we also show that those results translate empirically into robust learning across multiple environments.
## [Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design](https://openreview.net/forum?id=7UhxsmbdaQ) ------> [PDF](https://openreview.net/pdf/c60555e0c3337ee5d14fc5cf2955aaf4a9ca516e.pdf)
#### Keywords:  Molecular generative models   reinforcement learning   natural language processing   drug discovery   sample-efficiency   explainability
  Abs:  Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration is the first method to jointly address explainability and sample efficiency for molecular design.
## [Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization](https://openreview.net/forum?id=7NzgkEdGyr) ------> [PDF](https://openreview.net/pdf/e19188965734c518283647cf0730faa7d335c7a5.pdf)
#### Keywords:  Parameter-efficient finetuning   orthogonal   Butterfly matrix
  Abs:  Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using  butterfly structures. We apply this parameterization to OFT, creating  a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in computer vision and natural language. The results validate the effectiveness of BOFT as a generic finetuning method.
## [Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors](https://openreview.net/forum?id=7ArYyAmDGQ) ------> [PDF](https://openreview.net/pdf/59d05d5c5940cc90609ac29fea62658b308ea089.pdf)
#### Keywords:  prediction risk   estimation risk   generalization   statistical learning   overparameterization   interpolation   ridgeless regression   benign overfitting   double descent   nonspherical errors
  Abs:  In recent years, there has been a significant growth in research focusing on minimum $\ell_2$ norm (ridgeless) interpolation least squares estimators. However, the majority of these analyses have been limited to a simple regression error structure, assuming independent and identically distributed errors with zero mean and common variance. In this paper, we explore prediction risk as well as estimation risk under more general regression error assumptions, highlighting the benefits of overparameterization in a \emph{finite} sample. We find that including a large number of \emph{unimportant} parameters relative to the sample size can effectively reduce both risks. Notably, we establish that the estimation difficulties associated with the variance components of both risks 
can be summarized through the trace of the variance-covariance matrix of the regression errors.
## [Neural Architecture Search for TinyML with Reinforcement Learning](https://openreview.net/forum?id=70rlVBPX6Y) ------> [PDF](https://openreview.net/pdf/0aab8cbb44b8262c8addb6953fef6ac7c45b0b10.pdf)
#### Keywords:  Hyperparameter Optimization   TinyML   Microcontrollers   Reinforcement Learning   Augmented Random Search   Multi-Objective Optimization
  Abs:  Deploying Deep Neural Networks (DNNs) on microcontrollers (TinyML) is a common trend to process the increasing amount of sensor data generated at the edge, but in practice, resource and latency constraints make it difficult to find optimal DNN candidates. Neural Architecture Search (NAS) is an excellent approach to automate this search and can easily be combined with DNN compression techniques commonly used in TinyML. However, many NAS techniques are not only computationally expensive, especially hyperparameter optimization (HPO), but also often focus on optimizing only a single objective, e.g., maximizing accuracy, without considering additional objectives such as memory consumption or computational complexity of a model, which are key to making deployment at the edge feasible. In this paper we propose a novel NAS strategy for TinyML based on multi-objective Bayesian optimization (MOBOpt) and an ensemble of competing parametric policies trained using Augmented Random Search (ARS) Reinforcement Learning (RL) agents. Our methodology aims at efficiently finding tradeoffs between a DNN's predictive accuracy, memory consumption on a given target system, and computational complexity. Our experiments show that we outperform existing MOBOpt approaches consistently on different data sets and architectures such as ResNet-18 and MobileNetV3.
## [FEATHER: Lifelong Test-Time Adaptation with Lightweight Adapters](https://openreview.net/forum?id=6yJuDK1DsK) ------> [PDF](https://openreview.net/pdf/4b18b4819f84ac8aaf72e1bdb35e94fd5c0da1cc.pdf)
#### Keywords:  test-time adaptation   source free test-time domain adaptation   parameter efficient test-time adaptation
  Abs:  Lifelong/continual test-time adaptation (TTA) refers to the problem where a pre-trained source domain model needs to be continually adapted at inference time to handle non-stationary test distributions. Continuously updating the source model over long horizons can result in significant drift in the source model, forgetting the source domain knowledge. Moreover, most of the existing approaches for lifelong TTA require adapting all the parameters, which can incur significant computational cost and memory consumption, limiting their applicability on edge devices for faster inference. We present FEATHER (liFelong tEst-time Adaptation wiTH lightwEight adapteRs), a novel lightweight approach that introduces only a small number of additional parameters to a pre-trained source model which can be unsupervisedly and efficiently adapted during test-time for the new test distribution(s), keeping the rest of the source model frozen. FEATHER disentangles the source domain knowledge from the target domain knowledge, making it robust against error accumulation over time. Another distinguishing aspect of FEATHER is that, unlike some recent approaches for lifelong TTA that require access to the source data for warm-starting the adaptation at test time, FEATHER does not have such a requirement. FEATHER is also orthogonal to the existing lifelong TTA approaches and can be augmented with these approaches, resulting in a significant reduction in the number of additional parameters needed to handle the lifelong TTA setting. Through extensive experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC Robustbench benchmark datasets, we demonstrate that, with substantially (85% to 94%) fewer trainable parameters, FEATHER achieves better/similar performance compared to existing SOTA lifelong TTA methods, resulting in faster adaptation and inference at test-time. The source code for FEATHER will be released upon publication.
## [ScaLearn: Simple and Highly Parameter-Efficient Task Transfer by Learning to Scale](https://openreview.net/forum?id=6cMmSnOpCs) ------> [PDF](https://openreview.net/pdf/01baee9167f3c2f51a54b4020af5ecccd331dcb8.pdf)
#### Keywords:  natural language processing   multi-task learning   transfer learning   adapters   efficient learning   peft methods
  Abs:  Multi-task learning (MTL) has shown considerable practical benefits, particularly when using pre-trained language models (PLMs). While this is commonly achieved by simultaneously learning n tasks under a joint optimization procedure, recent methods such as AdapterFusion structure the problem into two distinct stages: (i) task learning, where knowledge specific to a task is encapsulated within sets of parameters (e.g., adapters), and (ii) transfer, where this already learned knowledge is leveraged for a target task. This separation of concerns provides numerous benefits, such as promoting reusability, and addressing cases involving data privacy and societal concerns; on the flip side, current two-stage MTL methods come with the cost of introducing a substantial number of additional parameters. In this work, we address this issue by leveraging the usefulness of linearly scaling the output representations of source adapters for transfer learning. We introduce ScaLearn, a simple and highly parameter-efficient two-stage MTL method that capitalizes on the knowledge of the source tasks by learning a minimal set of scaling parameters that enable effective knowledge transfer to a target task. Our experiments on three benchmarks (GLUE, SuperGLUE, and HumSet) show that our ScaLearn, in addition to facilitating the benefits of two-stage MTL, consistently outperforms strong baselines with only a small number of transfer parameters – roughly 0.35% of those of AdapterFusion. Remarkably, we observe that ScaLearn maintains its strong abilities even when further reducing parameters through uniform scaling and layer-sharing, achieving similarly competitive results with only 8 transfer parameters for each target task. Our proposed approach thus demonstrates the power of simple scaling as a promise for more efficient task transfer.
## [IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs](https://openreview.net/forum?id=6RR3wU4mSZ) ------> [PDF](https://openreview.net/pdf/57f00a45ec4e0d3a09b4a9ca464fa6e23c2fab95.pdf)
#### Keywords:  Efficient Transformers   Inference-time Efficiency   CPU
  Abs:  One limitation of existing transformer-based models is that they cannot handle very long sequences as input since their self-attention operations exhibit quadratic time and space complexity. This problem becomes especially acute when transformers are deployed on hardware platforms equipped only with CPUs. To address this issue, we propose a novel method for accelerating self-attention at inference time that works with pretrained transformer models out-of-the-box without requiring retraining. We experiment using our method to accelerate various long-sequence transformers on various benchmarks and demonstrate a greater speedup compared to the baselines.
## [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://openreview.net/forum?id=6PmJoRfdaK) ------> [PDF](https://openreview.net/pdf/c4f75b87d9cc82dde385ebb39020e237c65d9b9a.pdf)
#### Keywords:  Efficient fine-tuning   Long context   Large language model
  Abs:  We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA adopts Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8$\times$ A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning on our LongLoRA models, with long instruction-following data. Our code and models will be publicly available.
## [Rep-Adapter: Parameter-free Automatic Adaptation of Pre-trained ConvNets via Re-parameterization](https://openreview.net/forum?id=6PVgHZUepm) ------> [PDF](https://openreview.net/pdf/c91887f8fea2af84e4b40f1b802571a3690348f5.pdf)
#### Keywords:  Parameter-free Automatic Adaptation
  Abs:  Recent advances in visual pre-training have demonstrated the advantage of transferring pre-trained models to target tasks. However, different transfer learning protocols have distinctive advantages regarding target tasks, and are nontrivial to choose without repeated trial and error. This paper presents a parameter-free automatic model adaptation protocol for ConvNets, aiming at automatically balancing between fine-tuning and linear probing, by using adaptive learning rate for each convolution filters on target tasks. First, we propose Rep-Adapter, an adapter module with re-parameterization scheme, which can achieve soft balancing between the pre-trained and fine-tuned filters, and can be equivalently converted to a single weight layer, without introducing additional parameters to the inference phase. We show by theoretical analysis that Rep-Adapter can simulate a ConvNet layer with each filter fine-tuning at different learning rate. We present a simple adapter tuning protocol with Rep-Adapter to achieve automatic adaptation of pretrained models without additional search cost. Extensive experiments on various datasets with ResNet and CLIP demonstrate the superiority of our Rep-Adapter on semi-supervised, few-shot and full dataset transfer learning scenarios.
## [Compressed Context Memory for Online Language Model Interaction](https://openreview.net/forum?id=64kSvC4iPg) ------> [PDF](https://openreview.net/pdf/df88e6f14c2a62816cd8e358367f4f1df698f430.pdf)
#### Keywords:  context compression   efficient inference   natural language processing   transformer
  Abs:  This paper presents a novel context compression method for Transformer language models in online scenarios such as ChatGPT, where the context continually expands. As the context lengthens, the attention process requires more memory and computational resources, which in turn reduces the throughput of the language model. To this end, we propose a compressed context memory system that continually compresses the growing context into a compact memory space. The compression process simply involves integrating a lightweight adapter into the language model's forward pass during inference. Based on the compressed context memory, the language model can perform inference with reduced memory and attention operations. Through evaluations on multi-task learning, personalization, and conversation, we demonstrate that our approach achieves the performance level of a full context model with $5\times$ smaller context memory space.
## [Calibrated Dataset Condensation for Faster Hyperparameter Search](https://openreview.net/forum?id=5xKixQzhDE) ------> [PDF](https://openreview.net/pdf/88045f404d9b51b46f80caa7d64f984b5f103067.pdf)
#### Keywords:  Dataset Condensation   Hyperparameter Optimization
  Abs:  Dataset condensation can be used to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients between the real and synthetic data. However, there is no theoretical guarantee of the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice. This paper considers a different condensation objective specifically geared toward hyperparameter search. We aim to generate a synthetic validation dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation (HCDC) algorithm, which obtains the synthetic validation dataset by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of models and speeds up hyperparameter/architecture search for tasks on both images and graphs.
## [Linear Log-Normal Attention with Unbiased Concentration](https://openreview.net/forum?id=5nM2AHzqUj) ------> [PDF](https://openreview.net/pdf/ce489e9f5ae3ae26f56abb9b469add2c5c1e2661.pdf)
#### Keywords:  Neural Networks   Transformers   Self-Attention   Linear Attention   Scalable Transformers   Efficient Attention   Attention with Linear Complexity   Linearized Attention   Self-Attention Analysis
  Abs:  Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary materials.
## [Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning](https://openreview.net/forum?id=5abK7RDbuW) ------> [PDF](https://openreview.net/pdf/e5e781b0eacb498ec49fead2da0894df7df49b96.pdf)
#### Keywords:  Multi-Label Image Recognition   Text to Image   Parameter-Efficient Fine-Tuning
  Abs:  Benefited from image-text contrastive learning, pre-trained vision-language models, e.g., CLIP, allow to directly leverage texts as images (TaI) for parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image feature to be similar with the corresponding text features, modality gap remains a nontrivial issue and limits the MLR performance of TaI. Using multi-label image recognition (MLR) as an example, we present a novel method, called T2I-PAL to tackle the modality gap issue when using only text captions for PEFT. The core design of T2I-PAL is to leverage pretrained text-to-image generation models to generate photo-realistic and diverse images from text captions, thereby being beneficial for reducing modality gap. For better PEFT, we further combine both prompt tuning and adapter learning for enhancing classification performance. Extensive experiments on multiple benchmarks, including MS-COCO, VOC2007, and NUS- WIDE, show that our T2I-PAL can boost recognition performance by 3.47% in average above the top-ranked state-of-the-art methods. Our code and models will be made publicly available.
## [Theoretical Analysis on the Generalization Power of Overfitted Transfer Learning](https://openreview.net/forum?id=5T46w5X3Go) ------> [PDF](https://openreview.net/pdf/779a668465f3ec91c3ce751596618a9d2d971ccc.pdf)
#### Keywords:  transfer learning   generalization performance   overfitting   overparameterization   double descent
  Abs:  Transfer learning is a useful technique for achieving improved performance and reducing training costs by leveraging the knowledge gained from source tasks and applying it to target tasks. Assessing the effectiveness of transfer learning relies on understanding the similarity between the ground truth of the source and target tasks. In real-world applications, tasks often exhibit partial similarity, where certain aspects are similar while others are different or irrelevant. To investigate the impact of partial similarity on transfer learning performance, we focus on a linear regression model with two distinct sets of features: a common part shared across tasks and a task-specific part. Our study explores various types of transfer learning, encompassing two options for parameter transfer. By establishing a theoretical characterization on the error of the learned model, we compare these transfer learning options, particularly examining how generalization performance changes with the number of features/parameters in both underparameterized and overparameterized regimes. Furthermore, we provide practical guidelines for determining the number of features in the common and task-specific parts for improved generalization performance. For example, when the total number of features in the source task's learning model is fixed, we show that it is more advantageous to allocate a greater number of redundant features to the task-specific part rather than the common part. Moreover, in specific scenarios, particularly those characterized by high noise levels and small true parameters, sacrificing certain true features in the common part in favor of employing more redundant features in the task-specific part can yield notable benefits.
## [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://openreview.net/forum?id=5HCnKDeTws) ------> [PDF](https://openreview.net/pdf/1a5c5f491774948ade2c3b1d017f59cb6c9c5f24.pdf)
#### Keywords:  LLM finetuning   Scaling Laws   Full-model finetuning   Parameter efficient tuning   Machine Translation   Multilingual Summarization
  Abs:  While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning – full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.
## [Dynamic Electroencephalography Representation Learning for Improved Epileptic Seizure Detection](https://openreview.net/forum?id=5Gt68fnttu) ------> [PDF](https://openreview.net/pdf/9c42152a403e736fc609771f448f790742aee591.pdf)
#### Keywords:  Electroencephalography   Epileptic   Seizure   Efficient   Neuroscience
  Abs:  Epileptic seizure is an abnormal brain activity that affects millions of people worldwide. Effectively detecting seizures from electroencephalography (EEG) signals with automated algorithms is essential for seizure diagnosis and treatment. Although much research has been proposed to learn EEG representations, most of them neglect the detection latency when it comes to real-world clinical scenarios where the inputs are streaming. Moreover, they fail to either capture the underlying dynamics of brain activities or localize seizure regions. To this end, we propose an improved seizure detection task named onset detection, which identifies both the presence and the specific timestamps of seizure events, and two new metrics to quantify the timeliness of detection methods. We further introduce the Dynamic Seizure Network, a framework for EEG representation learning, which models the evolutionary brain states and dynamic brain connectivity efficiently. Theoretical analysis and experimental results on three real-world seizure datasets demonstrate that our method outperforms baselines with low time and space complexity.  Our method can also provide visualizations to assist clinicians in localizing abnormal brain activities for further diagnosis.
## [Vision-Language Instruction-enhanced Tuning via Parameter-efficient Learning](https://openreview.net/forum?id=4zfbwpGhd8) ------> [PDF](https://openreview.net/pdf/75167285967e47b89da105b02dc1e1dd715142c9.pdf)
#### Keywords:  Parameter-efficient Learning   Instruction Tuning   MultiModal
  Abs:  Instruction tuning has shown promising potential for developing general-purpose AI capabilities in large-scale pretrained models. In multimodal community, this has motivated growing research on enhancing instruction tuning to integrate multimodal information for creative applications. However, existing works have two main limitations:  the high training costs and heavy computing resource dependence of full model fine-tuning, and the lack of semantic information in instructions, which hinders multimodal alignment. In this paper, we propose a novel architecture called Vision-Language Instruction-enhanced Tuning via Parameter-efficient Learning (VITAL). Our proposed VITAL first enables lightweight model training using only 2% of parameters through automatic mode approximation. More importantly, VITAL enhances instruction semantics from two perspectives: 1) aggregating more context via enhanced instruction mixture to aid multimodal fusion, and 2) strengthening the connection between the proposed parameter-efficient tuning method and mutual information through our proposed score-based information bottleneck. Validation experiments on six multimodal downstream benchmarks demonstrate that VITAL outperforms state-of-the-art approaches in most cases, even surpassing the performance of full fine-tuning. Besides, extensive experiments on the few-shot setting as well as various visualization analyses have also fully validated our advantages.
## [Parameter-Efficient Fine-Tuning via Partially Decomposable Loss Analysis and Sharing](https://openreview.net/forum?id=4WZNdnwmhk) ------> [PDF](https://openreview.net/pdf/facfaec8a04189a27d788b78631a9aad84ea7323.pdf)
#### Keywords:  Fine-tuning   efficient training
  Abs:  Large language model (LLM) has become a crucial tool for many machine learning research and applications. Due to the large parameter count of these models and the enormous amount of training data, large language models are usually strong at general tasks. For most applications however, one would like a smaller, more parameter-efficient model that is specialized in a particular field. This motivates the design of fine-tuning, which tunes a pre-trained LLM for a few iterations on a dedicated dataset for specific tasks. If not handled correctly, the fine-tuning process would create another LLM that has comparable amount of parameters, significantly slowers downstream applications.

One of the most widely-known ideas for resolving this issue is the Low-Rank Adaptation (LoRA) framework, where one assumes the fine-tuning weights are low-rank therefore the number of parameters together with the inference time is drastically improved. While performing well in practice, LoRA method is still a heuristic and lacks theoretical guarantees even though the loss function might inherit certain structures. Moreover, when fine-tuning multiple similar tasks in parallel, LoRA requires one to learn a pair of distinct low-rank matrices for each task, ignoring possible shared structure between tasks.

In this work, we design a framework that further reduces parameter count compared to LoRA and enables parameter sharing across different parallel fine-tuning tasks. When the number of parallel fine-tuning tasks grows larger, we cut the parameter count almost in half compared to LoRA. Moreover, we prove why our approach --- or more generally, LoRA works for a large class of loss functions. We empirically verify the effectiveness of our method on various benchmark models and datasets, demonstrating much improved parameter count while retaining similar performance as LoRA.
## [DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation](https://openreview.net/forum?id=3xHbRLymyZ) ------> [PDF](https://openreview.net/pdf/8803618ce326e1006184a157469550093cae065e.pdf)
#### Keywords:  Diffusion   Efficiency   Diffusion acceleration   Early Exiting
  Abs:  Diffusion models achieve great success in generating diverse and high-fidelity images. The performance improvements come with low generation speed per image, which hinders the application diffusion models in real-time scenarios. While some certain predictions benefit from the full computation of the model in each sample iteration, not every iteration requires the same amount of computation, potentially leading to computation waste. In this work, we propose DeeDiff, an early exiting framework that adaptively allocates computation resources in each sampling step to improve the generation efficiency of diffusion models. Specifically, we introduce a timestep-aware uncertainty estimation module (UEM) for diffusion models which is attached to each intermediate layer to estimate the prediction uncertainty of each layer. The uncertainty is regarded as the signal to decide if the inference terminates. Moreover, we propose uncertainty-aware layer-wise loss to fill the performance gap between full models and early-exited models. With such loss strategy, our model is able to obtain comparable results as full-layer models. Extensive experiments of class-conditional, unconditional, and text-guided generation on several datasets show that our method achieves state-of-the-art performance and efficiency trade-off compared with existing early exiting methods on diffusion models. More importantly, our method even brings extra benefits to baseline models and obtains better performance on CIFAR-10 and Celeb-A datasets.  Full code and model are released for reproduction.
## [Boosting Vanilla Lightweight Vision Transformers via Re-parameterization](https://openreview.net/forum?id=3rmpixOjPS) ------> [PDF](https://openreview.net/pdf/2b5ad2aaff84e91163b1605106bf5537f69d1d52.pdf)
#### Keywords:  Vision Transformers   Re-parameterization   Lightweight Models
  Abs:  Large-scale Vision Transformers have achieved promising performance on downstream tasks through feature pre-training. However, the performance of vanilla lightweight Vision Transformers (ViTs) is still far from satisfactory compared to that of recent lightweight CNNs or hybrid networks. In this paper, we aim to unlock the potential of vanilla lightweight ViTs by exploring the adaptation of the widely-used re-parameterization technology to ViTs for improving learning ability during training without increasing the inference cost. The main challenge comes from the fact that CNNs perfectly complement with re-parameterization over convolution and batch normalization, while vanilla Transformer architectures are mainly comprised of linear and layer normalization layers. We propose to incorporate the nonlinear ensemble into linear layers by expanding the depth of the linear layers with batch normalization and fusing multiple linear features with hierarchical representation ability through a pyramid structure. We also discover and solve a new transformer-specific distribution rectification problem caused by multi-branch re-parameterization. Finally, we propose our Two-Dimensional Re-parameterized Linear module (TDRL) for ViTs. Under the popular self-supervised pre-training and supervised fine-tuning strategy, our TDRL can be used in these two stages to enhance both generic and task-specific representation. Experiments demonstrate that our proposed method not only boosts the performance of vanilla Vit-Tiny on various vision tasks to new state-of-the-art (SOTA) but also shows promising generality ability on other networks. Code will be available.
## [Maestro: Uncovering Low-Rank Structures via Trainable Decomposition](https://openreview.net/forum?id=3mdCet7vVv) ------> [PDF](https://openreview.net/pdf/c833bc8957a299f2e4cadcf65525a22461725847.pdf)
#### Keywords:  low-rank approximation   efficient model training   trainable decomposition
  Abs:  Deep Neural Networks (DNNs) have been a large drivers and enablers for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification, or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Such techniques (e.g., SVD) also frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take a further step in designing efficient low-rank models and propose MAESTRO, a framework for trainable low-rank layers. Instead of regularly applying a priori decompositions such as SVD, the low-rank structure is built into the training process through a generalized variant of Ordered Dropout. This method imposes an importance ordering via sampling on the decomposed DNN structure. Our theoretical analysis demonstrates that our method recovers the SVD decomposition of linear mapping on uniformly distributed data and PCA for linear autoencoders. We further apply our technique on DNNs and empirically illustrate that MAESTRO enables the extraction of lower footprint models that preserve model performance while allowing for graceful accuracy-latency tradeoffs for the deployment to devices of different capabilities.
## [Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling](https://openreview.net/forum?id=3klVRLhK7w) ------> [PDF](https://openreview.net/pdf/8b323135136d41b6d767edbe265f0d1898ab9cd2.pdf)
#### Keywords:  continual learning   constraint   layer freezing   efficient learning
  Abs:  Majority of online continual learning (CL) places restrictions on the size of replay memory and a single-epoch training to ensure a prompt update of the model. However, the single-epoch training may imply a different amount of computations per CL algorithm, and additional storage for storing logit or model in addition to replay memory is largely ignored as a storage budget. Here, we used floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare CL algorithms with the same total budget. Interestingly, we found that the new and advanced algorithms often perform worse than simple baselines under the same budget, implying that their value is less beneficial in real-world deployment. To improve the accuracy of online continual learners in the same budget, we propose an adaptive layer freezing and frequency-based memory retrieval for episodic memory usage for a storage- and computationally-efficient online CL algorithm. The proposed adaptive layer freezing does not update the layers for less informative batches to reduce computational cost with a negligible loss of accuracy. The proposed memory retrieval balances the training usage count of samples in episodic memory with a negligible computational and memory cost. In extensive empirical validations using CIFAR-10/100, CLEAR-10, and ImageNet-1K datasets, we demonstrate that the proposed method outperforms the state-of-the-art in the same total budget.
## [Sparse Refinement for Efficient High-Resolution Semantic Segmentation](https://openreview.net/forum?id=3j5bsiwRv6) ------> [PDF](https://openreview.net/pdf/d1204a84f262050089a8b5fce2b7960e6b82f1e3.pdf)
#### Keywords:  Efficient machine learning   Semantic segmentation   Sparsity   Efficient model design   Model compression and acceleration
  Abs:  Semantic segmentation empowers numerous real-world applications, such as autonomous driving and augmented/mixed reality. These applications often operate on high-resolution images (e.g., 8 megapixels) to capture the fine details. However, this comes at the cost of considerable computational complexity, hindering the deployment in latency-sensitive scenarios. In this paper, we introduce SparseRefine, a novel approach that enhances dense low-resolution predictions with sparse high-resolution refinements. Based on coarse low-resolution outputs, SparseRefine first uses an entropy selector to identify a sparse set of pixels with the least confidence. It then employs a sparse feature extractor to efficiently generate the refinements for those pixels of interest. Finally, it leverages a gated ensembler to apply these sparse refinements to the initial coarse predictions. SparseRefine can be seamlessly integrated into any existing semantic segmentation model, regardless of CNN- or ViT-based. SparseRefine achieves significant speedup: 1.5 to 3.9 times when applied to HRNet-W48, SegFormer-B5, Mask2Former-T/L and SegNeXt-L on Cityscapes, with negligible to no loss of accuracy. We will release the code to reproduce our results. We hope that our "dense+sparse" paradigm could inspire future research on efficient high-resolution visual computing.
## [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training](https://openreview.net/forum?id=3Z1gxuAQrA) ------> [PDF](https://openreview.net/pdf/94ab65a6b6353919079e03271c54a18b722096f1.pdf)
#### Keywords:  context window extension   efficiency   positional skip-wise training
  Abs:  Large Language Models (LLMs) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting LLMs to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that PoSE greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens using a 2k training context window. Furthermore, we empirically confirm that PoSE is compatible with all RoPE-based LLMs and position interpolation strategies. Notably, our method can potentially support infinite length, limited only by memory usage in inference. With ongoing progress for efficient inference, we believe PoSE can further scale the context window beyond 128k.
## [LEMON: Lossless model expansion](https://openreview.net/forum?id=3Vw7DQqq7U) ------> [PDF](https://openreview.net/pdf/bd6d6a1862feb3c3c0bbc8b66332f45dd06f972d.pdf)
#### Keywords:  model growth   efficient deep learning   continual learning
  Abs:  Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models.
Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain.
To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe 
to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch.
Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT.
Our empirical results demonstrate that LEMON reduces computational costs by 56.7\% for Vision Transformers and 33.2\% for BERT when compared to training from scratch.
## [Understanding Parameter Saliency via Extreme Value Theory](https://openreview.net/forum?id=3J7foqnJkA) ------> [PDF](https://openreview.net/pdf/43f55b6f1273cb274d5ddedee85bdb623ad88624.pdf)
#### Keywords:  parameter saliency   extreme value theory   XAI
  Abs:  Deep neural networks are being increasingly implemented throughout society in recent years. It is useful to identify which parameters trigger misclassification in diagnosing undesirable model behaviors.
The concept of parameter saliency is proposed and used to diagnose convolutional neural networks (CNNs) by ranking 
convolution filters that may have caused misclassification on the basis of parameter saliency.
It is also shown that fine-tuning the top ranking salient filters  efficiently corrects misidentification on ImageNet.
However, there is still a knowledge gap in terms of understanding why parameter saliency ranking can find the filters inducing misidentification.
In this work, we attempt to bridge the gap by analyzing parameter saliency ranking from a statistical viewpoint, namely, extreme value theory.
We first show that the existing work implicitly assumes that the gradient norm computed for each filter follows a normal distribution.
Then, we clarify the relationship between parameter saliency and the score based on the peaks-over-threshold (POT) method, which is often used to model extreme values.
Finally, we reformulate parameter saliency in terms of the POT method, where this reformulation is regarded as statistical anomaly detection and does not require the implicit assumptions of the existing formulation of parameter saliency.
Our experimental results demonstrate that our reformulation can detect malicious filters as well.
Furthermore, we show that the existing parameter saliency method exhibits a bias against the depth of layers in deep neural networks.
In particular, this bias has the potential to inhibit the discovery of filters that cause misidentification in situations where domain shift occurs.
In contrast, parameter saliency based on POT shows less of this bias.
## [An Intrinsic Dimension Perspective of Transformers for Sequential Modeling](https://openreview.net/forum?id=2PKZtPMyvI) ------> [PDF](https://openreview.net/pdf/57ba6b7f0ba8cf4befa02d6db68f402c67158138.pdf)
#### Keywords:  Transformers   Intrinsic Dimension   Hyperparameter Optimization   Natural Language Processing
  Abs:  Transformers have become immensely popular for sequential modeling, particularly in domains like natural language processing (NLP).
Recent innovations have introduced various architectures based on the Transformer framework, resulting in significant advancements in applications.
However, the underlying mechanics of these architectures are still somewhat enigmatic.
In this study, we explore the geometrical characteristics of data representations learned by Transformers using a mathematical metric known as intrinsic dimension (ID). This can be conceptualized as the minimum parameter count needed for effective modeling.
A sequence of experiments, predominantly centered on text classification, support the ensuing empirical observations regarding the correlation between embedding dimension, layer depth, individual layer ID, and task performance.
Interestingly, we note that a higher terminal feature ID, when obtained from Transformers, generally correlates with a lower classification error rate. 
This stands in contrast to the behavior observed in CNNs (and other models) during image classification tasks. Furthermore, our data suggests that the ID for each layer tends to diminish as layer depth increases, with this decline being notably steeper in more intricate architectures.
We also present numerical evidence highlighting the geometrical constructs of data representations as interpreted by Transformers, indicating that only nonlinear dimension reduction is achievable.
Lastly, we delve into how varying sequence lengths impact both ID and task performance, confirming the efficacy of data reduction during training.
Our ambition is for these insights to offer direction in the choice of hyper-parameters and the application of dimension/data reduction when using Transformers for text classification and other prevalent NLP tasks.
## [Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings](https://openreview.net/forum?id=1RrOtCmuKr) ------> [PDF](https://openreview.net/pdf/edc852e25861542c385e724cf52bf27e4c178871.pdf)
#### Keywords:  quantization   codebooks   hashlists   compression   efficient inference   deep learning
  Abs:  The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.
## [Feature Learning in Infinite Depth Neural Networks](https://openreview.net/forum?id=17pVDnpwwl) ------> [PDF](https://openreview.net/pdf/1d52ad2c446c5ad4926945ecf6f9e247fd3925a0.pdf)
#### Keywords:  Tensor Programs   mup   deep learning   optimization   optimal hyperparameter transfer
  Abs:  Empirical studies have consistently demonstrated that increasing the size of neural networks often yields superior performance in practical applications. However, there is a lack of consensus regarding the appropriate scaling strategy, particularly when it comes to increasing the depth of neural networks. In practice, excessively large depths can lead to model performance degradation. In this paper, we introduce Depth-$\mu$P, a principled approach for depth scaling, allowing for the training of arbitrarily deep architectures while maximizing feature learning and diversity among nearby layers. Our method involves dividing the contribution of each residual block and the parameter update by the square root of the depth. Through the use of Tensor Programs, we rigorously establish the existence of a limit for infinitely deep neural networks under the proposed scaling scheme. This scaling strategy ensures more stable training for deep neural networks and guarantees the transferability of hyperparameters from shallow to deep models. To substantiate the efficacy of our scaling method, we conduct empirical validation on neural networks with depths up to $2^{10}$.
## [Memory-Efficient Backpropagation through Large Linear Layers](https://openreview.net/forum?id=17ZbByq95E) ------> [PDF](https://openreview.net/pdf/653c6d26c5f3f2c90bd29bd51ec1a5f0c140c992.pdf)
#### Keywords:  transformers   large language models   randomized matmul   approximate matmul   memory-efficient training
  Abs:  In modern neural networks like Transformers, linear layers require significant memory to store activations during backward pass. This study proposes a memory reduction approach to perform backpropagation through linear layers. Since the gradients of linear layers are computed by matrix multiplications, we consider methods for randomized matrix multiplications and demonstrate that they require less memory with a moderate decrease of the test accuracy. Also, we investigate the variance of the gradient estimate induced by the randomized matrix multiplication. We compare this variance with the variance coming from gradient estimation based on the batch of samples. We demonstrate the benefits of the proposed method on the fine-tuning of the pretrained RoBERTa model on GLUE tasks.
## [ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning](https://openreview.net/forum?id=0xT87opqKV) ------> [PDF](https://openreview.net/pdf/4f9e5e0bc5131db852fbe4688a51026349ec3c3d.pdf)
#### Keywords:  Pretrained Large Models   Parameter-Efficient Fine-tuning   Protein Representation Learning
  Abs:  The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Recent advancements in Large Protein Models (LPMs) have demonstrated their ability in sequence and structure understanding, suggesting the potential of directly using them for efficient protein representation learning. In this work, we introduce ProteinAdapter, to efficiently transfer the general reference from the multiple Large Protein Models (LPMs), e.g., ESM-1b, to the task-specific knowledge. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. We observe that such a simple yet effective approach works well on multiple downstream tasks. Specifically, (1) with limited extra parameters, ProteinAdapter enables multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs. (2) Based on the learned embedding, we further scale the proposed ProteinAdapter to multiple conventional protein tasks. Considering different task priors, we propose a unified multi-scale predictor to fully take advantage of the learned embeddings via task-specific focus. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task settings. We hope that the proposed method could accelerate the study of protein analysis in the future.
## [Sporadicity in Decentralized Federated Learning: Theory and Algorithm](https://openreview.net/forum?id=0fpLLsAynh) ------> [PDF](https://openreview.net/pdf/bd934b07f936b5435164b1671dee1f115b5e982c.pdf)
#### Keywords:  Decentralized Federated Learning   Distributed Optimization   Sporadicity   Resource Efficiency   Sporadic SGDs   Anarchic Federated Learning
  Abs:  Decentralized Federated Learning methods are a family of techniques employed by devices in a distributed setup to (i) reach consensus over a common model which (ii) is optimal with respect to the global objective function. As this is carried out without the presence of any centralized server, prominent challenges of conventional Federated Learning become even more significant, namely heterogeneous data distributions among devices and their varying resource capabilities. In this work, we propose $\textit{Decentralized Sporadic Federated Learning}$ ($\texttt{DSpodFL}$), which introduces sporadicity to decentralized federated learning. $\texttt{DSpodFL}$ includes sporadic stochastic gradient calculations and model exchanges for aggregations. Our motivation is to achieve joint computation and communication savings without losing statistical performance. We prove that by using a constant step size, our method achieves a geometric convergence rate to a finite optimality gap. Through numerical evaluation, we demonstrate the resource savings achieved by $\texttt{DSpodFL}$ compared to the existing baselines.
## [Visual Prompting Reimagined: The Power of Activation Prompts](https://openreview.net/forum?id=0b328CMwn1) ------> [PDF](https://openreview.net/pdf/4701de9ba7d850db19129cc3e1e49af3727b393b.pdf)
#### Keywords:  parameter-efficient fine-tuning   transfer learning
  Abs:  Visual prompting (VP) has emerged as a popular method to repurpose large pretrained models for downstream vision tasks. Unlike many parameter-efficient finetuning (PEFT) techniques that modify model parameters, VP introduces a universal perturbation directly into the input data to facilitate task-specific finetuning while keeping the pretrained model intact. However, there exists a noticeable performance gap between VP and conventional finetuning methods, highlighting an unexplored realm in theory and practice to understand and advance VP to close its performance gap. Towards this end, we introduce a novel concept, termed activation prompt (AP), which extends the scope of input-level VP by enabling universal perturbations to be applied to activation maps within the intermediate layers of the model. With the aid of AP, we show that VP, by its input perturbation design, has intrinsic limitations in both performance and efficiency. By contrast, AP shares a natural connection to normalization tuning, e.g., batch normalization for convolutional neural networks (CNNs) and layer normalization for vision transformers (ViTs). This illuminates the reason behind the observed better accuracy of normalization tuning than VP in the literature. Furthermore, we show that the choice of prompting exhibits a distinct preference for layer depth, with conclusions varying significantly between CNNs and ViTs. We theoretically elucidate the rationale behind such preference by analyzing global features across layers. By conducting extensive experiments across 29 datasets and various model architectures, we provide a thorough performance analysis of AP, comparing it with VP and PEFT baselines. Our experimental results demonstrate that AP significantly surpasses the input-level VP in terms of both accuracy and efficiency, considering factors like time, parameters, memory usage, and throughout. These results further support our new insights into the incapabilities of VP and the capabilities of AP.
## [Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages](https://openreview.net/forum?id=0aR1s9YxoL) ------> [PDF](https://openreview.net/pdf/a3edbfb6bdf65c23920dd7b5629c5629f1403b5a.pdf)
#### Keywords:  Plasticity   Visual Reinforcement Learning   Sample Efficiency
  Abs:  Plasticity, the ability of a neural network to evolve with new data, is crucial for high-performance and sample-efficient visual reinforcement learning (VRL). Although methods like resetting and regularization can potentially mitigate plasticity loss, the influences of various components within the VRL framework on the agent's plasticity are still poorly understood. In this work, we conduct a systematic empirical exploration focusing on three primary underexplored facets and derive the following insightful conclusions: (1) data augmentation is essential in maintaining plasticity; (2) the critic's plasticity loss serves as the principal bottleneck impeding efficient training; and (3) without timely intervention to recover critic's plasticity in the early stages, its loss becomes catastrophic. These insights suggest a novel strategy to address the high replay ratio (RR) dilemma, where exacerbated plasticity loss hinders the potential improvements of sample efficiency brought by increased reuse frequency. Rather than setting a static RR for the entire training process, we propose Adaptive RR, which dynamically adjusts the RR based on the critic’s plasticity level. Extensive evaluations indicate that Adaptive RR not only avoids catastrophic plasticity loss in the early stages but also benefits from more frequent reuse in later phases, resulting in superior sample efficiency.
## [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://openreview.net/forum?id=09iOdaeOzp) ------> [PDF](https://openreview.net/pdf/4d0534c2a9540a35d41933a759820017f08cb37c.pdf)
#### Keywords:  pruning   efficiency   large language models   pre-training
  Abs:  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring less than 3% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.
## [Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing](https://openreview.net/forum?id=02f3mUtqnM) ------> [PDF](https://openreview.net/pdf/275f7637b9ada26451f25c7f33bc0e0e15e36a2c.pdf)
#### Keywords:  Large language models   Efficient ML   Query Routing
  Abs:  Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade  quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.
