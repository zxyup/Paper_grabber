{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openreview\n",
    "import time\n",
    "import sys\n",
    "import tqdm\n",
    "sys.path.append('../')\n",
    "from MDsolver import  mdsolver\n",
    "client = openreview.Client(baseurl='https://api2.openreview.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPsolver(mdsolver):\n",
    "    def __init__(self, filename):  \n",
    "        super().__init__(filename) \n",
    "    \n",
    "    def __del__(self):  \n",
    "        super().__del__()  \n",
    "        \n",
    "    def write_from_opnote(self,sb):\n",
    "        forum=sb.forum\n",
    "        url='https://openreview.net/forum?id='+forum\n",
    "        title=sb.content['title']['value']\n",
    "        keys=sb.content['keywords']['value']\n",
    "        pdf=sb.content['pdf']['value']\n",
    "        abs=sb.content['abstract']['value']\n",
    "        pdf_url='https://openreview.net'+pdf\n",
    "        self.f.write(f'## [{title}]({url}) ------> [PDF]({pdf_url})\\n')\n",
    "        key_s='   '.join(keys)\n",
    "        self.f.write(f'#### Keywords:  {key_s}\\n')\n",
    "        abs.replace(\"\\n\", \" \")  \n",
    "        self.f.write('  Abs:  '+abs+'\\n')\n",
    "        # time.sleep(0.1)\n",
    "        \n",
    "    def over(self):\n",
    "        self.f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting V1 Notes: 100%|█████████▉| 7296/7304 [00:33<00:00, 215.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submissions = client.get_all_notes(invitation='ICLR.cc/2024/Conference/-/Submission', details='directReplies')\n",
    "print(len(submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "op=OPsolver('ICLR2024_PEFT.md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for sb in submissions:\n",
    "    keywords=sb.content['keywords']['value']\n",
    "    if any(['parameter'in key.lower() or 'effic'in key.lower() or 'peft' in key.lower() for key in keywords]):\n",
    "        if any(['3d' in key.lower() for key in keywords]) == 0:\n",
    "            i+=1\n",
    "            print(i,end=' ')\n",
    "            if i%20 == 0:\n",
    "                print('\\n')\n",
    "            # print(keywords)\n",
    "            op.write_from_opnote(sb)\n",
    "op.over()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parameter sharing', 'model compression', 'pruning']\n",
      "In defense of parameter sharing for model-compression\n",
      "['Transformer', 'Multi-head Attention', 'Model Pruning']\n",
      "Pruning Attention Heads with Almost-sure Sparsity Targets\n",
      "['Peer-to-peer', 'decentralized federated learning', 'pruning-aware training', 'Fisher-guided pruning']\n",
      "Silencer: Pruning-aware Backdoor Defense for Decentralized Federated Learning\n",
      "['pruning', 'retraining', 'model averaging', 'neural networks']\n",
      "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging\n",
      "['Multi-task learning', 'Gated networks', 'Sharing', 'Pruning', 'Sparsity', 'MTL']\n",
      "GatedMTL: Learning to Share, Specialize, and Prune Representations for Multi-task Learning\n",
      "['Sparse neural networks', 'expander graphs', 'pruning', 'Ramanujan graphs']\n",
      "Constructing Sparse Neural Architecture with Deterministic Ramanujan Graphs\n",
      "['Modular RL', 'Reusable RL', 'Action Pruning', 'Reward Shaping']\n",
      "Reward Adaptation Via Q-Manipulation\n",
      "['Semantic Inference', 'Semantic Pruning', 'Deep Learning', 'Efficient Neural Networks']\n",
      "Faster and Accurate Neural Networks with Semantic Inference\n",
      "['Structured', 'Sparse', 'Pruning', 'Projection']\n",
      "SAS: Structured Activation Sparsification\n",
      "['robustness', 'verification', 'adversarial', 'pruning', 'neural networks']\n",
      "Robustness Evaluation Using Local Substitute Networks\n",
      "['Large language models', 'Long context handling', 'Token pruning']\n",
      "Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs\n",
      "['coreset selection', 'data pruning', 'data', 'graph', 'message passing', 'data distillation']\n",
      "$\\mathbb{D}^2$ Pruning: Message Passing for Balancing Diversity & Difficulty in Data Pruning\n",
      "['Interpretability', 'Analysis', 'NLP', 'Pruning']\n",
      "Uncovering Causal Variables in Transformers Using Circuit Probing\n",
      "['Pruning', 'Fusion']\n",
      "Towards Meta-Pruning via Optimal Transport\n",
      "['Retraining-free', 'Pruning', 'Compression', 'Transformers']\n",
      "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models\n",
      "['structured pruning', 'neural network', 'model compression']\n",
      "Pruning via Ranking (PvR): A unified structured pruning approach\n",
      "['Super-Resolution', 'Flexible Pruning']\n",
      "Lightweight Image Super-Resolution via Flexible Meta Pruning\n",
      "['Deep Graph Neural Network', 'Graph Neural Network', 'Network Pruning', 'The lottery Ticket Hypothesis']\n",
      "The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field\n",
      "['sparsity', 'pruning', 'lottery tickets', 'learning rate rewinding', 'iterative magnitude pruning']\n",
      "Masks, Signs, And Learning Rate Rewinding\n",
      "['large language models (LLMs)', 'pruning', 'layerwise sparsity']\n",
      "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity\n",
      "['vision transformer', 'training cost', 'token pruning', 'knowledge distillation', 'supervised distilliation']\n",
      "MaskedKD: Efficient Distillation of Vision Transformers with Masked Images\n",
      "['Structured Matrix', 'Block Low Rank', 'Low Rank', 'Efficient Neural Network', 'Transformer', 'Fourier', 'Dirichlet Kernel', 'FFT', 'Boxcar', 'Pruning', 'Compression']\n",
      "Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n",
      "['Deep Learing', 'Capsule Network', 'Orthogonality', 'Pruning']\n",
      "OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning\n",
      "['Pruning', 'Pruning-at-Initialization', 'Structured Pruning', 'Efficient Deep Learning', 'Efficient Model', 'Acceleration', 'Deep Learning Acceleration', 'Convolutional Neural Network']\n",
      "Structured Pruning of CNNs at Initialization\n",
      "['large language model', 'scaling', 'in-context learning', 'pruning']\n",
      "The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning\n",
      "['Structured Pruning', 'Large Language Model']\n",
      "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models\n",
      "['learning sparse neural network', 'structured sparse inducing regularization', 'structured pruning', 'neural network compression']\n",
      "Learning Structured Sparse Neural Networks Using Group Envelope Regularization\n",
      "['Model Compression', 'Structured Pruning', 'Hashing', 'CNNs']\n",
      "Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing\n",
      "['Pruning', 'Deep Learning', 'Neural Networks', 'Interpretability', 'Loss landscapes', 'Optimization', 'Kurtosis']\n",
      "What Makes a Good Prune? Optimal Unstructured Pruning for Maximal Cosine Similarity\n",
      "['Pruning', 'Structured Pruning', 'Total Variation Distance']\n",
      "Distributional Structured Pruning by Lower bounding the Total Variation Distance using Witness functions\n",
      "['multi-modal learning', 'model pruning', 'layer-wise pruning']\n",
      "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models\n",
      "['sparsity', 'scaling', 'optimal sparsity', 'efficiency', 'foundational models', 'transformers', 'structured sparsity', 'pruning']\n",
      "Scaling Laws for Sparsely-Connected Foundation Models\n",
      "['Few-shot learning', 'ensemble learning', 'ensemble pruning']\n",
      "FusionShot: Boosting Few Shot Learners with Focal-Diversity Optimized Ensemble Method\n",
      "['Network Pruning', 'Federated Learning', 'Model Heterogeneity']\n",
      "FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity\n",
      "['pruning', 'transfer learning', 'structured pruning']\n",
      "Model Pruning with Model Transfer\n",
      "['Model Pruning', 'Embedding Dimension Search', 'Recommendation Models', 'Machine Learning Training']\n",
      "FIITED: Fine-grained embedding dimension optimization during training for recommender systems\n",
      "['Large Language Model Compression', 'LLM Pruning']\n",
      "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation\n",
      "['Federated Learning', 'Model Heterogeneity', 'Model Pruning']\n",
      "One Training Fits All: Addressing Model-Heterogeneity Federated Learning via Architecture Probing\n",
      "['deep learning', 'pruning', 'Hessian', 'Grassmannians']\n",
      "What Apples Tell About Oranges: Connecting Pruning Masks and Hessian Eigenspaces\n",
      "['Spiking Neural Networks', 'Network Pruning']\n",
      "Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework\n",
      "['diffusion models', 'efficiency', 'network pruning']\n",
      "Denoising Diffusion Step-aware Models\n",
      "['Pruning', 'Winograd Convolution', 'GPU']\n",
      "Winograd Structured Pruning\n",
      "['Model Robustness', 'Training Efficiency', 'Data Pruning']\n",
      "FTFT: efficient and robust Fine-Tuning by transFerring Training dynamics\n",
      "['Generalization Bounds', 'Pruning', 'Sparsity']\n",
      "Generalization Bounds for Magnitude-Based Pruning via Sparse Matrix Sketching\n",
      "['Data pruning', 'vision-language datasets', 'synthetic captions']\n",
      "SIEVE: Multimodal Dataset Pruning using Image-Captioning Models\n",
      "['pruning', 'structured pruning', 'grouped kernel pruning', 'CNN', 'one-shot']\n",
      "LeanFlex-GKP: Advancing Hassle-Free Structured Pruning with Simple Flexible Group Count\n",
      "['Unstructured pruning', 'Sparse neural networks', 'Lottery ticket hypothesis', 'Weight averaging', 'Bayesian neural networks']\n",
      "Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning\n",
      "['deep learning', 'sparsity', 'disparate impact', 'constrained optimization', 'pruning', 'fairness']\n",
      "Balancing Act: Sparse Models with Constrained Disparate Impact\n",
      "['sparse training', 'sparsity', 'pruning', 'lottery tickets', 'scalable', 'efficient']\n",
      "Always-Sparse Training with Guided Stochastic Exploration\n",
      "['Network Pruning', 'Deep Neural Networks', 'Masked Autoencoder', 'Reconstruction']\n",
      "Pruning-as-Reconstruct: Masked Autoencoders are Efficient Importance Indicators\n",
      "['grokking', 'lottery ticket hypothesis', 'weight norm', 'pruning', 'generalization']\n",
      "Grokking Tickets: Lottery Tickets Accelerate Grokking\n",
      "['Neural architecture search', 'model compression', 'structural pruning', 'large language models']\n",
      "Structural Pruning of Large Language Models via Neural Architecture Search\n",
      "['Channel Prunning', 'Visual Prompt', 'Sparse Neural Network']\n",
      "($\\texttt{PASS}$) Visual Prompt Locates Good Structure Sparisty through a Recurent HyperNetwork\n",
      "['Deep Learning', 'Pruning', 'Fine Tuning', 'Subnetworks', 'Stochastic Annealing', 'Regularization']\n",
      "Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Subnetworks\n",
      "['Post-Training Pruning', 'Combinatorial Optimization', 'Large Language Models', 'Inference Acceleration']\n",
      "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models\n",
      "['Generative pre-trained language models', 'Unstructured pruning', 'Sparse regularization', 'Model compression']\n",
      "Enhancing One-Shot Pruned Generative Pre-training Language Models through Sparse-Dense-Sparse Mechanism\n",
      "['Network Pruning', 'Theoritical Explanation', 'Latent Space']\n",
      "Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space\n",
      "['imaging inverse problem', 'computational imaging', 'model-based deep learning', 'network pruning']\n",
      "A Structured Pruning Algorithm for Model-based Deep Learning\n",
      "['Intrinsic Dimension', 'vision and language', 'cross modal representation', 'pruning']\n",
      "Understanding Vision and Language Representations under the Lens of Intrinsic Dimension\n",
      "['sparsity', 'pruning', 'natural robustness', 'flatness', 'sharpness', 'generalization', 'deep learning', 'out of distribution']\n",
      "Adaptive Sharpness-Aware Pruning for Robust Sparse Networks\n",
      "['network pruning', 'sparsity', 'large language models', 'network architectures', 'outlier features']\n",
      "A Simple and Effective Pruning Approach for Large Language Models\n",
      "['strong lottery tickets', 'edge-pop', 'soft pruning']\n",
      "Soft iEP: On the Exploration Inefficacy of Gradient Based Strong Lottery Exploration\n",
      "['Invariance Learning', 'Neural Network Pruning', 'Auto ML', 'Contrastive Learning', 'Lazy Training', 'Representation Learning', 'Self-Supervised Learning', 'Computer Vision', 'Tabular Learning']\n",
      "Learning Invariances via Neural Network Pruning\n",
      "['transformers', 'pruning', 're-training', 'latency', 'throughput', 'token reduction', 'one-shot']\n",
      "The Need for Speed: Pruning Transformers with One Recipe\n",
      "['Network Pruning', 'Model Compression', 'Optimal Transport', 'Wasserstein Distance', 'Deep Learning']\n",
      "Robust Network Pruning With Sparse Entropic Wasserstein Regression\n",
      "['Model Compression', 'Structured pruning', 'Limited data']\n",
      "A Fast Framework for Post-training Structured Pruning Without Retraining\n",
      "['data pruning', 'dataset distillation', 'random sampling', 'corset selection', 'data-efficient learning']\n",
      "Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning\n",
      "['large language models', 'model prune', 'gradient', 'sparsity']\n",
      "Pushing Gradient towards Zero: A Novel Pruning Method for Large Language Models\n",
      "['Sparsity', 'Pruning', 'Federated Learning', 'Sparse Federated Learning', 'Communication efficiency', 'Efficient FL', 'Pruning at Initialization']\n",
      "From Random to Relevant: Harnessing Salient Masks in Non-IID Federated Learning\n",
      "['Sparse co-training', 'pruning', 'efficient and flexible NN inferencing']\n",
      "Sparse Cocktail: Every Sparse Pattern Every Sparse Ratio All At Once\n",
      "['Simplex', 'vertex hunting', 'successive projection', 'pseudo-points', 'pruning', 'hyper-spectral unmixing', 'archetypal analysis', 'network analysis.']\n",
      "Improved algorithm and bounds for successive projection\n",
      "['Vision Transformers', 'Token Pruning', 'Token Pooling', 'Model Acceleration']\n",
      "PPT: Token Pruning and Pooling for Efficient Vision Transformers\n",
      "['Model Compression', 'Vision Transformer', 'Model Pruning']\n",
      "LSP: Low-Power Semi-structured Pruning for Vision Transformers\n",
      "['partial label learning', 'label disambiguation', 'candidate label set pruning']\n",
      "Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning\n",
      "['Pruning', 'Statistical Dimension', 'High Dimension Geometry']\n",
      "How Sparse Can We Prune A Deep Network: A Geometric Viewpoint\n",
      "['Structured Pruning', 'Adapters', 'Transfer Learning', 'Computer Vision', 'Convolutional Neural Network', 'Transformer', 'Vision Transformer']\n",
      "Structured Pruning Adapters\n",
      "['large language model', 'LLM', 'compression', 'pruning', 'fisher']\n",
      "The LLM Surgeon\n",
      "['pruning', 'second-order pruning', 'neural-network pruning', 'optimization. fishleg optimization']\n",
      "Pruning neural networks using FishLeg estimation\n",
      "['pruning', 'large-scale', 'data curation', 'concept-based', 'LAION', 'DataComp']\n",
      "Effective pruning of web-scale datasets based on complexity of concept clusters\n",
      "['Vision Transformers', 'Model compression', 'Dynamic pruning']\n",
      "Synergistic Patch Pruning for Vision Transformer: Unifying Intra- & Inter-Layer Patch Importance\n",
      "['Dynamic Data Pruning; Training acceleration']\n",
      "InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning\n",
      "['fairness', 'pruning', 'model compression']\n",
      "Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization Perspective\n",
      "['Compression', 'Large Language Models', 'Pruning', 'Quantization']\n",
      "Compressing LLMs: The Truth is Rarely Pure and Never Simple\n",
      "['spiking neural networks', 'pruning', 'network structure', 'power consumption']\n",
      "Towards efficient deep spiking neural networks construction with spiking activity based pruning\n",
      "['Neural Network Pruning', 'Parameter Efficient Tuning']\n",
      "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning\n",
      "['federated learning', 'memory efficient training', 'nerual network pruning']\n",
      "FedMef: Towards Memory-efficient Federated Dynamic Pruning\n",
      "['Data subset selection', 'data pruning', 'data-efficient learning']\n",
      "BWS: Best Window Selection Based on Sample Scores for Data Pruning across Broad Ranges\n",
      "['network architecture optimization', 'network pruning', 'knowledge distillation']\n",
      "SEArch: A Self-Evolving Framework for Network Architecture Optimization\n",
      "['language models', 'pruning', 'machine unlearning', 'capability removal', 'intepretability', 'modularity']\n",
      "Dissecting Language Models: Machine Unlearning via Selective Pruning\n",
      "['backdoor defense', 'backdoor attack', 'neuron pruning']\n",
      "Directional Rank Reduction for Backdoor Defense\n",
      "['Filter Pruning; Model Compression; Vision Transformer']\n",
      "Data-independent Module-aware Pruning for Hierarchical Vision Transformers\n",
      "['Convolutional neural networks', 'Neural Network Compression', 'Low-Rank Tensors', 'Dynamical Low-Rank Approximation', 'Neural Network Training', 'Pruning']\n",
      "Rank-adaptive spectral pruning of convolutional layers during training\n",
      "['Large Language Models', 'Gradient-based Language Model Pruner', 'Sparsity-centric Pruning']\n",
      "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models\n",
      "['Neural Network Pruning', 'Linear Mode Connectivity', 'Dataset Distillation', 'Sparse Neural Networks']\n",
      "On Synthetic Data and Iterative Magnitude Pruning: a Linear Mode Connectivity Study\n",
      "['Sparse Training', 'Pruning', 'Orthogonal Initialization']\n",
      "Sparser, Better, Deeper, Stronger: Improving Sparse Training with Exact Orthogonal Initialization\n",
      "['Pruning', 'Sparsity', 'Deep Learning', 'Regularization', 'Model Compression']\n",
      "A Demon at Work: Leveraging Neuron Death for Efficient Neural Network Pruning\n",
      "['spiking neural network', 'SNN', 'network pruning', 'stability', 'neuromorphic', 'leaky integrate and fire', 'STDP', 'sparsification', 'task-agnostic pruning', 'timescale optimization']\n",
      "Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN\n",
      "['pruning', 'efficiency', 'large language models', 'pre-training']\n",
      "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for sb in submissions:\n",
    "    keywords=sb.content['keywords']['value']\n",
    "    if any(['prun' in key.lower() for key in keywords]):\n",
    "        if any(['3d' in key.lower() for key in keywords]) == 0:\n",
    "            i+=1\n",
    "            # print(i,end=' ')\n",
    "            # if i%20 == 0:\n",
    "                # print('\\n')\n",
    "            print(keywords)\n",
    "            print(sb.content['title']['value'])\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
